root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (GlavaÅ¡ G, Å najder J 2014):
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
We chose these particular kernels because their general forms have intuitive interpretations for event matching and can be easily adjusted to fit our needs for event centered text comparison. 
Furthermore,  considering that numerous textual sources provide information about the same real world events,  the need for aggregating and summarizing the most relevant information has become obvious. 
In (Genest PE, Lapalme G 2011):
Most INITs do not lead to full sentences,  and need to be combined into a sentence structure before being realized as text. 
The cause of this low score is mostly our method for text generation,  which still needs to be refined in several ways. 
Text generation patterns can be used,  based on some knowledge about the topic or the information needs of the user. 
In (Giannakopoulos et-al. 2008):
Therefore,  one needs a representation of text which will enable semantic similarity determination between two texts. 
Nevertheless,  it is quite probable that the words expressing the content will exist in the same context or that part of the words used will be identical,  for example,  if different in ections are used. 
Tackling the problem of what kind of information should be used to represent a peer and a model summary in the evaluation of a summary,  one should take into account that the surface appearance of two equivalent pieces of the same semantic content need not be identical as happens in the case of paraphrases Zhou et al   2006. 
In (Hearst 1997):
coding discourse and dialogue phenomena,  and especially coding segment boundaries,  may be inherently more difficult than many previous types of content analysis(  for instance,  dividing newspaper articles based on subject matter) "  and so implies that the levels of agreement needed to indicate good reliability for TextTiling may be justified in being lower. 
First,  Nomoto and Nit ta(  1994)  use too large an interval words because this is approximately the average size needed for their implementation of the blocks version of TextTiling. 
With the increase in importance of multimedia information,  especially in the context of Digital Library projects,  the need for segmentation and summarization of alternative media types is becoming increasingly important. 
In (Alguliev et-al. 2013):
According to Mani and May bury(  1999) ,  automatic text summarization takes a partially structured source text from multiple texts written about the same topic,  extracts information content from it,  and presents the most important content to the user in a manner sensitive to the user s needs. 
Thus,  now more than ever,  consumers need access to robust text summarization systems,  which can effectively condense information found in several documents into a short,  readable synopsis,  or summary(  Harabagiu and Lacatusu,  2010,  Yang and Wang,  2008). 
In general,  an abstract can be described as summary comprising concepts ideas taken from the source,  which are then reinterpreted and presented,  in a different form,  whilst an extract is a summary consisting of units of text taken from the source and presented verbatim(  Kutlu,  Cigir,  & Cicekli,  2010). 
In (Moawad IF, Aref M 2012):
The input graph contains the information needed to generate the final text. 
To achieve its task,  the phase accesses the domain ontology,  which contains the information needed in the same domain of RSG to generate the final texts. 
The exponential growth in data increases the need for intelligent filtering and knowledge based approaches to reduce the time needed to absorb the key facts in documents and to avoid drowning in it. 
In (Gupta et-al. 2011):
Thus,  there is a need for automatic text summarization for the languages in order to subdue this constantly increasing amount of electronically produced text. 
Precision(  the fraction of the text portions retrieved that are relevant to user s information need)  and recall(  the fraction of the text portions that are relevant to the topic that are successfully retrieved)  are the two performance measure used for determining the quality of the summary. 
This need is usually expressed by a natural language query or,  as in World Wide Web search engines,  by a list of keywords. 
In (Carlson et-al. 2003):
The procedural knowledge available at the EDU level is likely to need further refinement for higher level text spans along the lines of other work which posits a few macro level relations for text segments,  such as Ferrari(  1998)  or Meyer(  1985). 
Yet,  often an appropriate choice of relation for an unseen segment may not be obvious from the current(  rightmost)  unit that needs to be attached. 
The annotator segments the text one unit at a time,  then incrementally builds up the discourse tree by immediately attaching the current node to a previous node. 
In (Kulesza and Taskar 2012):
(  Perhaps we think students tend to spread out)  In this context each meeting of the class is a new sample from the empirical distribution over subsets of the(  xed)  seats,  so we merely need to collect enough samples and we should be able to t our model,  as desired For many problems,  however,  the notion of a single xed base set Y is inadequate. 
For instance,  consider extractive document summarization,  where the goal is to choose a subset of the sentences in a news article that together form a good summary of the entire article In this setting Y is the set of sentences in the news article being summarized,  thus Y is not xed in advance but instead depends on context. 
2-tVarious types of text Summarization
In (Giannakopoulos et-al. 2008):
There are various types of correlation measures,  called correlation coef cients,  depending on the context in which they can be applied. 
Although ROUGE takes into account contextual information,  it remains at the word level,  which means we either regard different types of the same word as different or we need to apply(  language dependent)  stemming or lemmatization to remove this effect. 
Trying to capture more than the simple co occurrence of words and in order to allow for different types of the same word,  our method uses character ngrams positioned within a context indicative graph. 
In (0079):
The use of rare words or technical terminology for example can make text difficult to read for certain audience types(  CollinsThompson and Call an,  2004; Sch warm Schwa rm and Ostendorf,  2005; Elhadad and Sutaria,  2007). 
Coherent text is characterized by various types of cohesive links that facilitate text comprehension(  Halliday and Has an,  1976). 
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non discourse features(  Miltsakaki and Kukich,  2000). 
In (Carlson et-al. 2003):
Thus,  our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth,  such as anaphoric relations(  Gar side et al,  1997)  or style types(  Leech et al,  1997)  ; analysis of a single text from multiple perspectives(  Mann and Thompson,  1992)  ; or illustrations of a theoretical model on a single representative text(  Brit ton Britt on and Black,  1985; Van Dijk and Kintsch,  1983). 
The annotators hired to build the corpus were all professional language analysts with prior experience in other types of data annotation. 
textual organization,  span,  and same unit(  used to link parts of units separated by embedded units or spans). 
In (Barzilay and Lapata 2005):
Each text can thus be viewed as a distribution defined over transition types. 
Note that considerable latitude is available when specifying the transition types to be included in a feature vector. 
there is often no single coherent rendering of a given text but many different possibilities that can be partially ordered. 
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (Zajic et-al. 2008):
a pure statistical approach based on HMMs. 
On the other hand,  we expect that the purely statistical HMMbased approach will be more robust to text from different genres. 
Specifically,  a sentence selector builds the final summary by choosing among the candidates,  based on features propagated from the sentence compression method,  features of the candidates themselves,  and features of the present summary state. 
It is currently a middle of the pack system based on recent DUC evaluations â€“ not significantly better or worse than most systems(  Madnani et al,  2007,  Zajic et al,  2006). 
In (Carenini et-al. 2008):
Second,  we use two graph based summarization approaches,  Generalized ClueWordSummarizer and PageRank,  to extract sentences as summaries. 
All these approaches used the email thread as a coarse representation of the underlying conversation structure. 
In our recent study(  Carenini et al,  2007) ,  we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer(  CWS)  based on the concept of clue words. 
3.2-tTopic based approaches
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Hennig and Labor(  2009)  proposed a multi document summarization method based on Probabilistic Latent Semantic Analysis(  PLSA) ,  which represents sentences and queries as probability distributions over latent topics. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
In (Kedzie et-al. 2015):
The second category,  predictive approaches,  includes ranking and classification based methods. 
In classification based methods,  model features are usually derived from human generated summaries,  and are non lexical in nature(  e g,  sentence starting position,  number of topic signatures,  number of unique words). 
Sentences have been ranked by the average word probability,  average TF*IDF score,  and the number of topically related words(  topic signatures in the summarization literature) (  Nenkova and Vanderwende,  2005; Hovy and Lin,  1998; Lin and Hovy,  2000). 
In (Yeh et-al. 2005):
Entity level approaches model text entities and their relationships co occurrence,  co reference,  etc,  and determine salient information based on the text entity model(  Azzam et al,  1999; McKeown & Radev,  1995). 
outperforms keyword based text summarization approaches. 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
In (Riedhammer et-al. 2010):
The former,  which does not require training data,  is represented by algorithms ported from the text community,  such as variants of MMR(  Murray et al,  a Riedhammer et al,  a) ,  graph based methods(  Garg et al,  2009,  and concept based methods(  Filatovaand Hatzivassiloglou,  2004; Riedhammer et al,  b Takamura and Okumura,  2009)  Supervised approaches rely on a classi er,  usually a support vector machine(  Burg es,  1998) ,  to predict a binary class label for each input sentence indicating whether it should be included in the summary or not. 
sentence based scoring of relevance and redundancy,  and sub sentence based scoring with implicit redundancy. 
 We compare two approaches for global modeling in summarization. 
In (Shen et-al. 2007):
In this paper,  we present a Conditional Random Fields(  CRF)  based framework to keep the merits of the above two kinds of approaches while avoiding their disadvantages. 
In the past,  extractive summarizers have been mostly based on scoring sentences in the source document based on a set of predefined features Mani and Bloedorn,  1998. 
and graphs based on the similarity of sentences Mihalcea,  2005. 
In (Li et-al. 2015a):
whether or not a sentence is in the summary,  or unsupervised methods such as graph based approaches to rank the sentences. 
Similar to the above method,  here we still focus on measuring the similarity between a bi gram big ram and the topic query,  but based on WordNet. 
Definitions of the extent of the Indian subcontinent differ but it usually includes the core lands of India,  Pakistan,  and Bangladesh the percentage of the correct bi grams big rams(  those in the human reference summaries)  by our proposed selection method and the original ICSI system which justICSI SystemSystemused document ICSI frequency based selection. 
In (Gupta et-al. 2011):
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 
Note that the value of threshold is predetermined based on the percentage of output summary required to be generated. 
3.3-tGraph based approaches
In (GlavaÅ¡ G, Å najder J 2014):
In this article,  we present event graphs,  a novel event based document representation model that filters and structures the information about events described in text. 
To construct the event graphs,  we combine machine learning and rule based models to extract sentence level event mentions and determine the temporal relations between them. 
To adequately capture the semantics of events,  we introduce event graphs,  a novel event centered document representation based on sentence level event mentions. 
In (Mihalcea and Tarau 2004):
In this paper,  we introduce TextRank â€“ a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications. 
Graph based ranking algorithms like Klein berg s HITS algorithm(  Klein berg,  1999)  or Google s PageRank(  Br in and Page,  1998)  have been successfully used in citation analysis,  social networks,  and the analysis of the link structure of the World Wide Web. 
In short,  agraphbased ranking algorithm is a way of deciding on the importance of a vertex within a graph,  by taking into account global information recursively computed from the entire graph,  rather than relying only on local vertexspeci c information. 
In (Zhao et-al. 2009):
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query. 
Define two sets S = Ï•,  S si i = 1,  2,  â€¦,  N,  and initialize the ranking score of each sentence to its graph based ranking score,  i e. 
Use a graph based ranking algorithm to rank all the sentences in the documents where the original query is used. 
3.4-tDiscourse based approaches
In (Gupta et-al. 2011):
Based on this classification,  automatic summarizers can be characterized as approaching the problem at the surface,  entity,  or discourse level. 
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 
There are several approaches to Text Summarization,  such as those proposed by(  Hovy and Mar cu 1998; Mani and May bury 1999; Tucker 1999; Radev 2000; May bury and Mani 2001; Mani 2001; Alonso and Castellon 2001). 
In (Yeh et-al. 2005):
Entity level approaches model text entities and their relationships co occurrence,  co reference,  etc,  and determine salient information based on the text entity model(  Azzam et al,  1999; McKeown & Radev,  1995). 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
outperforms keyword based text summarization approaches. 
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005):
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 
However,  most IR techniques that have been exploited in text summarization focus on symbolic level analysis,  and they do not take into account semantics such as synonymy,  polysemy,  and term dependency(  Hovy & Lin,  1997). 
Latent semantic analysis(  LSA)  is a mathematical technique for extracting and inferring relations of expected contextual usage of words in passages of discourse(  Deer wester et al,  1990; Landau er et al,  1998). 
In (0017):
In existing unsupervised methods,  Latent Semantic Analysis(  LSA)  is used for sentence selection sentences election. 
Unsupervised methods do not require training data such as human made summaries to train the summarizer(  Nomoto & Yuji,  2001; Buckley & Walz,  1999; Gong & Liu,  2001)  Recently,  many generic document summarization methods using Latent Semantic Analysis(  LSA)  have been proposed(  Gong & Liu,  2001; Li,  Li,  & Wu,  2006; Yeh et al,  2005; Zha,  2002). 
The second method uses a latent semantic analysis(  LSA)  to semantically identify important sentences for summary creations. 
4.2-tInformation extraction using sentence based abstraction technique
In (Chan 2006):
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 
Simulation experiments suggest that this technique is useful because it moves away from a purely keyword based method of textual information extraction and its associated limitations. 
At the syntactic stage,  information extraction involves tokenization,  partsofspeech tagging,  syntactic parsing,  phrasal pattern matching,  and the transformation of each sentence into an intermediate structure using various templates. 
In (Pang and Lee 2008):
Document level sentiment classification Sentence level sentiment analysisFeaturebased opinion mining and summarization Comparative sentence and relation extraction Summary. 
E g,  using a na ve Bayesian classifier with a set of data features attributes extracted from training sentences(  Wiebeet al   ACL99). 
Feature based opinion mining and summarization Comparative sentence and relation extraction Summary. 
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007):
Following our work in the Document Understanding Conference(  DUC)  2005 and 2006(  Ye et al,  2005,  Ye et al,  2006) ,  our summarization algorithm uses the DCL to select a globally optimum sentence set that represents as many concepts as possible with the fewest words. 
Text summarization Document concept lattice Concept Semantic. 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document(  s)  that can be preserved after summarization. 
In (Gupta and Lehal 2010):
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
Summarization tools may also search for headings and other markers of subtopics in order to identify the key points of a document. 
In multi document summarization,  rdq will be 1 exactly when d is in the document set corresponding to query q. 
In (Alguliev et-al. 2013):
In order to implement extractive summarization,  some sentence extraction techniques are utilized to identify the most important sentences,  which can express the overall understanding of a given document. 
Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some textual units of the documents and extracting those with highest scores. 
This paper proposes an optimization based model for generic document summarization. 
In (Heu et-al. 2015):
In the literature,  the development of multi document summarization has been largely promoted by the Document Understanding Conferences(  DUC)  2 and Text Analysis Conferences(  TAC)  .3. 
We used the ROUGE(  Lin & Hovy,  2003)  toolkit for evaluations,  which has been widely adopted by the Document Understanding Conference(  DUC)  for automatic summarization evaluation. 
Figure   2 shows the framework of our multi document summarization system FoDoSu. 
In (Barrera and Verma 2012):
In 200102 the Document Understanding Conference(  DUC)  proposed the task of creating word summaries of individual news articles but soon after dropped single document summarization competitions to move on to multi document extraction and update summarization. 
These results have implications not only for extractive and abs tractive single document summarization,  but could also be leveraged in multi document summarization. 
In this work we revisit the important problem of single document summarization and reconsider the performance of the baseline in a different context,  viz,  scientific magazine articles. 
In (Mihalcea and Tarau 2004):
We evaluate the TextRank sentence extraction algorithm on a single document summarization task,  using 567 news articles provided during the Document Understanding Evaluations 2002(  DUC,  2002). 
Fo reach For each article,  TextRank generates an words summary the task undertaken by other systems participating in this single document summarization task For evaluation,  we are using the ROUGE evaluation toolkit,  which is a method based on Ngramstatistics,  found to be highly correlated with human evaluations(  Lin and Hovy,  2003). 
Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents,  results in a graph based ranking model that can be applied to a variety of natural language processing applications,  where knowledge drawn from an entire text is used in making local ranking selection decisions. 
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Giannakopoulos et-al. 2008):
The AutoSummENG method is based on the concept of using statistically ex tr acted tract ed textual information from summaries integrated into a rich representa tional equivalent of a text to measure similarity between generated summaries and a set of model summaries. 
evaluation frameworks uses statistical measures of similarity based on ngrams of words,  4 although it supports different kinds of analysis,  ranging from ngram to semantic Hovy et al   b. 
A number of different intermediate representations of summaries information have been introduced in existing summarization evaluation literature,  ranging from automatically extracted snippets to human decided sub sentential portions of text. 
In (Gupta and Lehal 2010):
Text summarization based on fuzzy logic system architecture. 
Automatic text summarization based on fuzzy logic. 
Extractive summaries are formulated by extracting key text segments(  sentences or passages)  from the text,  based on statistical analysis of individual or mixed surface level features such as word phrase frequency,  location or cue words to locate the sentences to be extracted. 
In (Rush et-al. 2015):
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 
To test the effectiveness of this approach we run extensive comparisons with multiple abs tractive and extractive baselines,  including traditional syntax based systems,  integer linear program constrained systems,  information retrieval style approaches,  as well as statistical phrase based machine translation. 
Finally,  we use a phrase based statistical machine translation system trained on Gigaword to produce summaries,  MOSES+(  Koehn et al,  2007). 
In (Goldstein et-al. 2000):
Our approach addresses these issues by using domain independent techniques based mainly on fast,  statistical processing,  a metric for reducing redundancy and maximizing diversity in the selected passages,  and a modular framework to allow easy parameterization for different genres,  corpora characteristics and user requirements. 
This paper presented a statistical method of generating extraction based multi document summaries. 
This paper discusses a text extraction approach to multi document summarization that builds on single document summarization methods by using additional,  available information about the document set as a whole and the relationships between the documents. 
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008):
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 
We integrate our best cohesion measure together with the subjective opinions. 
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones. 
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 
Subjective opinions are often critical in many conversations. 
In (Carenini et-al. 2007):
Since we are studying multiple email summarization in a conversational context,  we required that each conversation contained at least 4 emails. 
This clearly shows that the hidden emails do carry crucial information and have to be considered by the email summarization system. 
Not only does this study provide a gold standard to evaluate CWS and other summarization methods,  but it also sheds light on the importance of clue words and hidden emails to human summarizers. 
4.6-tSummarization of text through complex network approach
In (Antiqueira et-al. 2009):
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features. 
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
Automatic summarization Complex networks Network measurements Sentence extraction Summary informativeness. 
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary. 
In (Baralis et-al. 2013):
to tackle the summarization problem by combining complex network analysis. 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Ferreira et-al. 2014):
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
The 2002 conference was the last one that proposed the contest to create generic multi document summaries. 
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
In (Liu et-al. 2009):
The experiments,  both on the task 1 of DUC02 which focuses on generic single document summarizing,  and on the main task of DUC07 whose goal is to produce a query based summary for a set of related documents,  illustrate that our proposed approaches achieve a stateoftheart performance Particularly noteworthy about our approach are the following points. 
The proposed approach is described in Section 3,  followed by evaluations both on a generic single document setting,  and on a query based multidocumentsetting in Section 4. 
Based on this proposed word information qu anti cation measure,  a sentence ssigni cance value for a generic summarization is computed using(  5)  where m is the number of words in sentence s,  I(  w)  is set to 1 when w is a keyword,  otherwise,  info(  w)  is equivalent to info(  w)  in(  2)  when w is a keyword,  otherwise When the documents to be processed are news articles,  a feature about sentence. 
In (Alguliev et-al. 2013):
This paper proposes an optimization based model for generic document summarization. 
â–º The paper presents a generic document summarization as an optimization problem. 
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009):
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 
Section 2 presents the different text feature parameters,  Section 3 is about the proposed automatic summarization model,  Section 4 shows the experimental results and finally Section 5 presents conclusions and future work. 
In (Almeida and Martins 2013):
Automatic text summarization is a seminal problem in information retrieval and natural language processing(  Luhn,  1958; Baxendale,  1958; Edmund son,  1969). 
In Stan Szpakowicz MarieFrancine Mo ens,  editor,  Text Summarization Branches Out. 
We present a dual decomposition framework for multi document summarization,  using a model that jointly extracts and compresses sentences. 
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Kulesza and Taskar 2012):
â€¢ An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;â€¢ A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;â€¢ A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
â€¢ The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
The DPP models outperform the baselines in most cases furthermore,  there is a signi cant boost in performance due to the use of DPP maximum likelihood training in place of logistic regression. 
In (Gupta and Lehal 2010):
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
Query based extractive text summarization. 
In query based text summarization. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
In (Otterbacher et-al. 2009):
However,  in our approach,  answers are extracted from a set of multiple documents rather than on a documentbydocument basis. 
A key challenge for passage retrieval for QA is that,  when attempting to retrieve answers to questions from a set of documents published by multiple sources over time(  e g. 
Our method is a query based extension of the LexRanksummarization method introduced in(  Erk an and Radev,  2004). 
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011):
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 
Filatova and Hatzivassiloglou(  2004)  modeled extractive document summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
In (Alguliev et-al. 2013):
A good summary,  as whole,  is expected to be the one with extensive coverage of the focuses presented in documents,  minimum redundancy,  and smooth connection among sentences. 
Filatova and Hatzivassiloglou(  2004)  represented each sentence with a set of conceptual units and formalized the extractive summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. 
Takamura and Okumura(  a)  represented text summarization as maximum coverage problem with knapsack constraint(  MCKP). 
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Ferreira et-al. 2013):
Extractive summarization Sentence scoring methods Summarization evaluation. 
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization. 
Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. 
Essentially,  text summarization techniques are classified as Extractive and Abs tractive. 
These techniques are mainly used into abs tractive summarization,  but they may be adapted for extractive ones. 
In (Antiqueira et-al. 2009):
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
Although extractive summarization can produce texts that have cohesion and coherence problems,  many systems have been proven to yield summaries whose informative level is satisfactory. 
A graph,  or network,  is a representation that may capture text structure in various ways,  being therefore suitable for extractive summarization. 
In (Ferreira et-al. 2014):
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
Multi document summarization Extractive summarization Sentence clustering. 
The following sections detail the sentence scoring methods and the sentence clustering algorithm used here. 
In (Riedhammer et-al. 2010):
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain. 
The selected meeting extracts can then either be juxtaposed to form a short text summarizing a meeting or used as a starting point to enhance browsing experience Extractive summarization algorithms often rely on the measurement of two important aspects. 
We conclude on the be ne ts and drawbacks of the presented models and give an outlook on future aspects to improve extractive meeting summarization 2010 Else vier B.V. 
In (Gupta and Lehal 2010):
Text Summarization methods can be classified into extractive and abs tractive summarization. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
This survey paper is concentrating on extractive summarization methods. 
In (Azmi AM, Al-Thanyyan S 2012):
A comprehensive evaluation in Uz da et al  (  2008)  has concluded that automatic text summarization methods which are based on RST are better than extractive summarizers and those with hybrid methods produce worse summaries. 
In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. 
The Optimized Dual Classification System(  Sobh et al,  2006)  is an Arabic extractive text summarization system. 
In (Otterbacher et-al. 2009):
In the current paper,  we have also demonstrated the e ectiveness of our method as applied to two classical IR problems,  extractive text summarization and passage retrieval for question answering. 
An extractive summarization method that is almost equivalent to LexRank with cosine links was independently proposed in(  Mihalceaand Tarau,  2004). 
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013):
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
In fact,  on the one hand,  since they do not consider the underlying correlations among multiple terms,  some relevant facets of the analyzed data could be disregarded. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
In (Kulesza and Taskar 2012):
In the rst,  we subtract a multiple of one of the vectors in V from all of the other vectors so that they are zero in the ithcomponent,  leaving us with a set of vectors spanning the subspace of V orthogonal to ei. 
We assume that the conditional DPP kernel L(  X ; Î¸)  is parameterized in terms ofa generic Î¸,  and let denote the conditional probability of an output Y given input X under parameter Î¸. 
Our input will be a still image depicting multiple people,  and our goal is to simultaneously identify the poses the positions of the torsos,  heads,  and left and right arms of all the people in the image. 
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Alguliev et-al. 2013):
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
DE is a simple yet efficient evolutionary algorithm,  which has been widely applied to solve continuous optimization problems(  Price,  Storn,  & Lampinen,  2005). 
(  1)  content coverage,  summary should contain salient sentences that cover the main content of the documents(  2)  diversity,  summaries should not contain multiple sentences that convey(  carry)  the same information and(  3)  length,  summary should be bounded in length. 
In (Baralis et-al. 2013):
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
Unlike all of the above mentioned approaches,  our summarizer discovers association rules from the analyzed document to also represent the correlations among multiple terms in the graph based model. 
A parallel effort has been devoted to using clustering algorithms for summarizing documents. 
In (Song et-al. 2011):
In this section,  we propose a fuzzy evolutionary optimization model to solve the problem of data clustering. 
Extractive summarization Topics estimation Unsupervised categorization Fuzzy evolutionary optimization Normalized Google distance. 
This paper proposes a fuzzy evolutionary optimization modeling(  FEOM)  and its applications to unsupervised categorization and extractive summarization. 
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Yang et-al. 2014):
Recall that our goal is to obtain more accurate sentence clusters and generate good summaries in theme based summarization. 
Thus,  good sentence clusters are the guarantee of good summaries in theme based summarization. 
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters. 
Ranking based clustering Sentence clustering Theme based summarization. 
Ranking based clustering framework shows the best performance,  it further presents ranking distribution of documents and terms can help generate more accurate sentence clusters. 
In (Harabagiu S, Lacatusu F 2005):
Second,  we believe that our results represent a comprehensive and replicable study,  which demonstrates the effectiveness of a structured,  theme based approach to multi document summarization. 
We first discuss a total of six different sentence extraction methods(  EM1â€“ EM6)  which correspond to the four baseline topic representation techniques(  TR1â€“TR4) ,  introduced in Section 2,  plus the two topic theme based representations based on TR5 introduced in Section 3.16 For ease of exposition,  we will refer to the graph based theme representation as TH1 and the linked list based theme representation as TH2. 
We believe that these results prove the benefits of using the theme based topic representation for multi document summarization. 
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010):
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
All features are computed over primitives,  syntactic,  linguistic,  or knowledge based information units extracted from the sentences. 
In (Fung P, Ngai G 2006):
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
A nal meta summarizer is used to summarize multiple documents using the HMSM state labels. 
In (Harabagiu S, Lacatusu F 2005):
We expect that theme based representations can be used to organize topic relevant information from multiple sources,  extracted from either, (  1)  a single sentence, (  2)  a cluster of sentences, (  3)  a discourse fragment,  or even(  4)  a cluster of documents. 
In Section 2,  we considered five different baseline topic representations that use terms(  TR1) ,  relations(  TR2) ,  document segments(  TR3) ,  sentence clusters(  TR4) ,  or semantic frames generated from multiple different documents(  TR5). 
We separately evaluated the quality of compression when different topic representations were available to the summarization system. 
In (Radev et-al. 2001):
Summarization relies on the principles of text redundancy and unevenly distributed information content to extract highly informative snippets(  often keywords or sentences)  from a document which can be read by the user in lieu of the original document(  s)  In WebInEssence,  we have adopted an approach to summarization of multiple documents called centroid based summarization(  Radev et al,  2000). 
In this paper,  we present our recent work on the development of a scalable personalized webbasedmultidocument summarization and recommendation system. 
Both single document summarization for a single URL and multiple document summarization for a cluster of URLs are supported in our system More related work can be found on the Extractor web site. 
In (Khan et-al. 2015):
exploits template based method to produce abs tractive summary from multiple news wire newspaper documents depending on the output of the information extraction(  IE)  system. 
Linguistic(  Syntactic)  based approach and Semantic based approach. 
Moreover,  this method could not handle or capture the information about similarities and differences across multiple documents. 
In (Owczarzak K 2009):
Evaluation is a crucial component in the area of automatic summarization it is used both to rank multiple participant systems in shared summarization tasks,  such as the Summarization track at TextAnalysis Conference(  TAC)  2008 and its Document Understanding Conference(  DUC)  predecessors,  and to provide feedback to developers whose goal is to improve their summarization systems However,  manual evaluation of a large number of documents necessary for a relatively unbiased view is often unfeasible,  especially in the contexts where repeated evaluations are needed. 
In TAC 2008 Summarization track,  all submitted runs were scored with the ROUGE(  Lin,  2004)  and Basic Elements(  BE)  metrics(  Hovy et al,  2005)  .ROUGE is a collection of stringcomparisontechniques,  based on matching ngrams betweena candidate string and a reference string. 
Table 1 presents system level Pearson s correlations between the scores provided by ourdependencybased metric DEPEVAL(  summ) ,  as well as the automatic metrics ROUGE2,  ROUGESU4,  and BEHM used in the TACevaluation,  and the manual Pyramid scores,  which measured the content quality of the systems It also includes correlations with the manual Overall Responsiveness score,  which reflected both content and linguistic quality. 
In (Li et-al. 2015a):
They ensure that selecting a sentence leads to the selection of all the concepts it contains,  and selecting a concept only happens when it is present in at least one of the selected sentences In such ILPbased summarization methods,  how to determine the concepts and measure their weight sis weights is the key factor impacting the system performance Intuitively,  if we can successfully identify the important key bi grams big rams to use in the ILP system,  or assign large weights to those important bi grams big rams,  the system generated summary sentences will contain as many important bi grams big rams as possible. 
Some stateoftheart summarization systems use integer linear programming(  ILP)  based methods that aim to maximize the important concepts covered in the summary. 
Second,  to estimate the importance of the oft he bi grams big rams,  in addition to the internal features based on the test documents(  e g,  document frequency,  bi gram big ram positions) ,  we propose to extract features by leveraging multiple external resources(  such as word embedding from additional corpus,  Wikipedia,  Dbpedia,  WordNet,  SentiWordNet). 
In (Ferreira et-al. 2014):
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
The proposed system uses a graph model based on statistic similarities and linguistic treatment to represent the collection of input documents(  differently from Canhasi and Kononenko,  2014,  Chen et al,  2014). 
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015):
Many extraction based summarization methods have been proposed in the past years. 
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 
With the construction of feature groups,  a latent variable is utilized to control the selection of them. 
For image summarization,  we choose an information theoretic measure which based on Jensenâ€“Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
In (CR132):
The extrinsic evaluation,  also called task based evaluation,  has received more attention recently at the DARPASummarization Evaluation Conference Mani et al,  1998. 
,  uses modified TF*IDF to produce clusters of news articles on the same event We developed a new technique for multidocumentsummarization(  or MDS) ,  called centroidbasedsummarization(  CBS)  which uses as input the centroids of the clusters produced by CIDR to identify which sentences are central to the topic of the oft he cluster,  rather than the individual articles. 
Finally,  we describe two user studies that test our models of multidocumentsummarization. 
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
In (Baralis et-al. 2013):
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
In (CR132):
the development of a centroid based multidocumentsummarizer,  the use of cluster based sentence utility(  CBSU)  and cross sentence informational subsumption(  CSIS)  for evaluation of single andmultidocument summaries,  two user studies that support our findings,  and an evaluation of MEAD. 
The authors mention that their preliminary results indicate that multiple document son documents on the same topic also contain redundancy but they fall short of using MMR for multidocumentsummarization. 
,  uses modified TF*IDF to produce clusters of news articles on the same event We developed a new technique for multidocumentsummarization(  or MDS) ,  called centroidbasedsummarization(  CBS)  which uses as input the centroids of the clusters produced by CIDR to identify which sentences are central to the topic of the oft he cluster,  rather than the individual articles. 
In (Radev et-al. 2001):
Summarization relies on the principles of text redundancy and unevenly distributed information content to extract highly informative snippets(  often keywords or sentences)  from a document which can be read by the user in lieu of the original document(  s)  In WebInEssence,  we have adopted an approach to summarization of multiple documents called centroid based summarization(  Radev et al,  2000). 
WebInEssence is designed to help end users e ectively search for useful information and automatically summarize selected documents based on the users personal pro les. 
The clustering component groups these URLsinto clusters of lexically related documents(  Radevet al,  1999). 
In (Li et-al. 2015a):
Second,  to estimate the importance of the oft he bi grams big rams,  in addition to the internal features based on the test documents(  e g,  document frequency,  bi gram big ram positions) ,  we propose to extract features by leveraging multiple external resources(  such as word embedding from additional corpus,  Wikipedia,  Dbpedia,  WordNet,  SentiWordNet). 
Second,  to estimate the bi grams big rams weights,  in addition to using information from the test documents,  such as document frequency,  syntactic role in a sentence,  etc,  we utilize a variety of external resources,  includinga corpus of news articles with human generated summaries,  Wiki documents,  description of name entities from DBpedia,  WordNet,  and SentiWordNet.Discriminative features are computed based on these external resources with the goal to better represent the importance of a bi gram big ram and its semantic similarity with the given query. 
identifying important summary sentences from one or multiple documents. 
In (Banerjee et-al. 2015):
Abs tractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. 
(  1)  Aligning similar sentences from multiple documents and(  2)  Generating the most informative and linguistically well formed sentence from each cluster,  and then appending them together. 
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Khan et-al. 2015):
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling. 
First,  it employs semantic role labeling for semantic representation of text. 
Our work,  in contrast,  aims to treat this limitation by using semantic role labeling(  SRL)  technique to build semantic representation from the document text automatically. 
To the best of our knowledge,  semantic role labeling(  SRL)  technique,  which exploits semantic role parser,  has not been employed for the semantic representation oftextin multi document abs tractive summarization. 
In (Kaljahi et-al. 2014):
This paper describes a series of French semantic role la belling label ling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment. 
Moreover,  there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 
Semantic role la belling label ling(  SRL) (  Gild ea and Jurafsky,  2002)  is the task of identifying the predicates in a sentence,  their semantic arguments and the roles these arguments take. 
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015):
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 
In (Baralis et-al. 2013):
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
Frequent item sets,  which represent correlations among terms,  are extracted from a transactional representation of the document collection and combined in a graph based model. 
In (Yang et-al. 2013):
We have leveraged the dictionary learning for sparse representation model to construct the summary and to represent the image. 
We have discovered that the image collection summarization problem can be interpreted straightforwardly with the dictionary learning for sparse representation model under the SIFT BoW framework. 
We have explicitly reformulated the problem of automatic image summarization by using an sparse representation model and the simulated annealing algorithm is adopted to solve the optimization function more effectively. 
In (Li et-al. 2015a):
Given the recent success of the continuous representation for words,  we propose to use an unsupervised method to induce dense real valued low dimensional word embedding,  and then use the inner product as ameasure of semantic similarity between two strings In the word embedding model,  every word can be represented by a vector w We define the similarity between two sequences S = x 1,  x 2,  x k and sequence S = y 1,  y 2,  y l as the average pairwise similarity between any two words in them. 
Second,  to estimate the importance of the oft he bi grams big rams,  in addition to the internal features based on the test documents(  e g,  document frequency,  bi gram big ram positions) ,  we propose to extract features by leveraging multiple external resources(  such as word embedding from additional corpus,  Wikipedia,  Dbpedia,  WordNet,  SentiWordNet). 
identifying important summary sentences from one or multiple documents. 
In (Alguliev et-al. 2013):
A novel multi document summarization model based on the budgeted median problem proposed in Takamura and Okumura(  b). 
One of the advantages of this representation is that MCKP can directly model whether each concept in the given documents is covered by the summary or not,  and can dispense with rather counterintuitive approaches such as giving penalty to each pair of two similar sentences. 
The facility location model adds features to the pmedian model,  and the linear representation model jumps out of the idea of clustering,  and modify the representation method. 
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (Fung P, Ngai G 2006):
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
A nal meta summarizer is used to summarize multiple documents using the HMSM state labels. 
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
For multi document summa rization,  the state labels of individual documents are used to group multiple summaries together into a single meta summary,  greatly simplifying the pro cess. 
In (CR92):
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS)  Speci cally,  a set of reader comments associated with the news reports are also collected. 
We name such aparadigm of extension as reader aware multi document summarization(  RAMDS)  We give a real example taken from a data set collected by us to illustrate the importance of RAMDS. 
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS). 
In (Heu et-al. 2015):
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
In (Radev et-al. 2001):
We will introduce an e ective search engine to summarize clusters of related web pages which provide more contextual and summary information to help users explore the retrieval result more e ciently We describe in this paper a system,  WebInEssence,  which blends the traditional information retrieval technology with advanced document clustering,  document recommendation,  and multi document summarization technology in an integrated framework Text summarization is the process of selecting the most salient information in one or more textual documents. 
Both single document summarization for a single URL and multiple document summarization for a cluster of URLs are supported in our system More related work can be found on the Extractor web site. 
Summarization relies on the principles of text redundancy and unevenly distributed information content to extract highly informative snippets(  often keywords or sentences)  from a document which can be read by the user in lieu of the original document(  s)  In WebInEssence,  we have adopted an approach to summarization of multiple documents called centroid based summarization(  Radev et al,  2000). 
In (Goldstein et-al. 2000):
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
This is the focus of our work reported here,  including the necessity to eliminate redundancy among the information content of multiple related documents. 
More recently,  single document summarization systems provide an automated generic abstract or a query relevant summary(  TIPSTER,  a). 
In (Harabagiu S, Lacatusu F 2005):
We expect that theme based representations can be used to organize topic relevant information from multiple sources,  extracted from either, (  1)  a single sentence, (  2)  a cluster of sentences, (  3)  a discourse fragment,  or even(  4)  a cluster of documents. 
In Section 2,  we considered five different baseline topic representations that use terms(  TR1) ,  relations(  TR2) ,  document segments(  TR3) ,  sentence clusters(  TR4) ,  or semantic frames generated from multiple different documents(  TR5). 
In our work,  we have considered both, (  1)  component based evaluations,  which evaluated each phase in the creation of a multi document summary separately,  and(  2)  intrinsic evaluations,  which evaluate the quality of each individual multi document summary generated by a summarization system. 
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a):
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 
We develop a Ranking framework upon Recursive Neural Networks(  R2N2)  to rank sentences for multi document summarization. 
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 
In (Gupta and Lehal 2010):
Text summarization with neural networks. 
This method involves training the neural networks to learn the types of sentences that should be included in the summary. 
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
Multi document extractive summarization deals with extraction of summarized information from multiple texts written about the same topic. 
In (Cao et-al. 2015c):
R2N2 applies recursive neural networks to learn feature combination. 
Since the Convolution al Neural Networks(  CNNs)  have shown promising progress in latent feature representation(  Yih et al,  2014; Sh en et al,  2014; Zeng et al,  2014) ,  PriorSum applies CNNs with multiple filters to capture a comprehensive set of document independent features derived from length variable phrases. 
Different from previous work using manually compiled document independent features,  we develop a novel summary system called PriorSum,  which applies the enhanced convolution al neural networks to capture the summary prior features derived from length variable phrases. 
In (Baralis et-al. 2013):
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015):
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 
Therefore,  in our work,  computing importance,  non redundancy and coherence is tightly integrated and taken care of simultaneously Our graph based summarization technique has the further advantage of being completely unsupervised We apply our graph based summarization technique to scienti c articles from the journal PLOS Medicine,  a high impact open access journal from the medical domain. 
Hence,  we optimize importance,  non redundancy and coherence simultaneously. 
In (Antiqueira et-al. 2009):
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
A graph,  or network,  is a representation that may capture text structure in various ways,  being therefore suitable for extractive summarization. 
In (CR100):
However,  preliminary experiments carried out prove that the combination of extractive and abs tractive information is a more suitable strategy to adopt towards the generation of abstracts.KeywordsHuman Language Technologies,  automated retrieval and mining,  automated content summarization,  abs tractive techniques,  graph based algorithms. 
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed(  i e,  COMPENDIUM) ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted. 
attempt to transform an extractive summarization into an abs tractive one in the context of meeting summarization by performing sentence compression. 
In (GlavaÅ¡ G, Å najder J 2014):
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
The extractive multi document summarization model selects sentences based on the relevance of the individual event mentions and the temporal structure of events. 
In this work,  we present an extractive multi document summarization model based on the extraction of sentence level event mentions and the temporal structure of documents. 
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015):
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 
Our approach is based on sub modular maximization and mixture learning,  which has been successfully used in applications such as document summarization(  Lin,  2012)  and image summarization(  Tschiatschek et al,  2014) ,  but has never been applied to topic identification tasks or,  more generally,  DAG summarization. 
Given a collection of articles spanning different topics,  but with similar titles,  automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy as a topic DAG. 
In (Wang and Li 2012):
Given a collection of documents,  a variety of summarization methods based on different strategies have been proposed to extract the most important sentences from the original documents. 
As a good ensemble requires the diversity of the individual members,  in this paper,  we first study the most widely used multi document summarization systems based on a variety of strategies(  e g,  the centroid based method,  the graph based method,  LSA,  and NMF) ,  and evaluate different baseline combination methods(  e g,  average score,  average rank,  Borda count,  median aggregation,  round robin scheme,  correlation based weighting method,  and graph based combination)  for obtaining a consensus summarizer to improve the summarization performance. 
Multi document summarization is a fundamental tool for document understanding and has received much attention recently. 
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015):
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates. 
Our system predicts sentence salience in the context of a large scale event,  such as a disaster,  and integrates these predictions into a clustering based multi document summarization system. 
The AP+SALIENCE model is better able to find salient updates earlier on for the disaster domain,  this is an especially important quality of the model. 
In (Kulesza and Taskar 2012):
For instance,  in Figure a weak pose detector favors large clusters of poses that are nearly identical,  but lteringthrough a DPP ensures that the nal predictions are well separated Throughout this survey we demonstrate applications for DPPs in a variety of settings,  including. 
â€¢ The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
The goal of learning is to choose appropriate Î¸ based on the training sample so that we can make accurate predictions on unseen inputs While there are a variety of ob jective functions commonly used for learning,  here we will focus on maximum likelihood learning(  or maximum likelihood estimation,  often abbreviatedMLE) ,  where the goal is to choose Î¸ to maximize the conditional log likelihood of the Optimizing L is consistent under mild assumptions that is,  if the training data are actually drawn from a conditional DPP with parameter Î¸âˆ—,  then the learned Î¸ â†’ Î¸âˆ— as T â†’ Of course real data are unlikely to exactly follow any particular model,  but in any case reasonable probability estimates,  since maximizing L can be seen as minimizing the loglossthe maximum likelihood approach has the advantage of calibrating the DPP to produce on the training data To optimize the log likelihood,  we will use standard algorithms such as gradient ascent or LBFGS Nocedal,  1980. 
4.23.11-tSummarizing multiple documents through system combination
In (Goldstein et-al. 2000):
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
This is the focus of our work reported here,  including the necessity to eliminate redundancy among the information content of multiple related documents. 
Specifically,  we are constructing sets of 10 documents,  which either contain a snapshot of an event from multiple sources or the unfoldment of an event over time. 
In (Heu et-al. 2015):
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015):
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
In total,  therefore,  when employing ROUGE for the evaluation of summarization systems,  there are 192(  8 x 2 x 2 x 3 x 2)  possible system level variants to choose from. 
Automatic metrics of summarization evaluation have their origins in machine translation(  MT) ,  with ROUGE(  Lin and Hovy,  2003) ,  the first and still most widely used automatic summarization metric,  comprising an adaption of the BLEU score(  Papineni et al,  2002). 
In (Lin 2004):
In this paper,  we introduced ROUGE,  an automatic evaluation package for summarization,  and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data. 
We found that(  1)  ROUGE2,  ROUGEL,  ROUGEW,  and ROUGES worked well in single document summarization tasks, (  2)  ROUGE1,  ROUGEL,  ROUGEW,  ROUGESU4,  and ROUGESU9 performed great in evalua ting very short summaries(  or headline like summaries) , (  3)  correlation of high 90% was hard to achieve for multi document summarization tasks but ROUGE1,  ROUGE2,  ROUGES4,  ROUGES9,  ROUGESU4,  and ROUGESU9 worked reasonably well when stop words were exc luded from match ing, (  4)  exclusion of stop words usually improved correlation,  and(  5)  correlations to human judgments were increased by using multiple references. 
We found that correlations were not affected by stemming or removal of stop words in this data set,  ROUGE2 performed be tter among the ROUGEN variants,  ROUGEL,  ROUGEW,  and ROUGES were all performing well,  and using mu ltiple references improved pe rformance though not much. 
In (Kulesza and Taskar 2012):
As described in Section 4.2.1,  ROUGE is an automatic evaluation metric for text summarization based on ngram overlap statistics Lin,  We report three standard variants. 
Figure 12 depicts a sample cluster from the test set To measure performance on this task we follow the original evaluation and use ROUGE,  an automatic evaluation metric for summarization Lin,  2004. 
We demonstrate learning for the conditional DPP quality model on an extractive multi document summarization task using news text. 
In (Dunlavy et-al. 2007):
We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences(  DUC)  as measured by the best known automatic metric for summarization system evaluation,  ROUGE. 
The benefit of using QCS over such methods is that it is a fully automatic system for document retrieval,  organization,  and summarization. 
Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001â€“2006 Document Understanding Conferences The focus of these conferences is the evaluation and discussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. 
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (CR100):
In light of this,  Text Summarization(  TS)  is of great help since its main aim is to produce a condensed new text containing a significant portion of the information in the original text(  s)  The process of summarization can be divided into three stages. 
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed(  i e,  COMPENDIUM) ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted. 
This paper focuses on abs tractive text summarization. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
7-tMultilingual approaches for text summarization
In (Baralis et-al. 2013):
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
To effectively address the summarization problem in a multilingual context in. 
Multi document summarization Text mining Association rule mining Graph ranking. 
The multi document summarization task entails generating a summary of a collection of textual documents. 
The raw textual content is commonly unsuitable for use in item set items et and association rule mining. 
GraphSum performs better than many stateoftheart approaches,  including those that heavily rely on advanced semantics based models(  e g,  ontologies)  or complex linguistic processing steps. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
In (GlavaÅ¡ G, Å najder J 2014):
Furthermore,  the results suggest that similar event based approaches could be used to address closely related NLP tasks such as text simplification. 
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
We evaluate the performance using ROUGE1 and ROUGE2 evaluation metrics(  Lin,  2004) ,  the most commonly used automated evaluation metrics for text summarization. 
In (Genest PE, Lapalme G 2011):
Most systems develop ped for the main international conference on text summarization,  the Text Analysis Conference(  TAC) (  Owczarzak and Dang,  2010) ,  predominantly use sentence extraction,  including all the top ranked systems,  which make only minor post editing of extracted sentences(  Conroy et al,  2010) (  Gil lick et al,  2009) (  Ge nest Gen est Gene st et al,  2008) (  Chen et al,  2008). 
This low linguistic score is understandable,  because this was our first try at text generation and abs tractive summarization,  whereas the other systems that year used sentence extraction,  with at most minor modifications made to the extracted sentences. 
The kind of texttotext generation involved in our work is related to approaches in paraphrasing(  Androutsopoulos and Malakasiotis,  2010). 
8-tSummary evaluation
8.1-tInformativeness evaluation
In (Patel et-al. 2007):
So,  evaluation in terms of informativeness is usually preferred. 
The first is Quality evaluation and the second is an informativeness evaluation. 
One of the measures in informativeness methodology for extractive summary evaluation is content based evaluation i e. 
Thus,  after preprocessing we have made the text suitable for extracting important sentences. 
So,  the next step is to carry out sentence analysis and to determine sentence weight. 
In (Banerjee et-al. 2015):
Further,  manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness. 
In manual evaluation,  our approach also achieves promising results on informativeness and readability. 
Hence,  we introduce two factors â€“ Informativeness(  I(  p Cj i) )  and Linguistic quality(  LQ(  p Cj i) ). 
for automatic evaluation of summaries(  compared against human written model summaries)  as it has been proven effective in measuring qualities of summaries and correlates well to human judgments. 
In (Antiqueira et-al. 2009):
We have described the use of complex networks concepts for extractive summarization,  as well as its evaluation through standard informativeness scores and significance tests. 
This type of analysis depends only on the corpus employed,  not on a fixed compression rate or specific informativeness score. 
We conclude the evaluation of CNSumm with an analysis of correlation between its different strategies. 
In (GlavaÅ¡ G, Å najder J 2014):
we first compute the participants importance score(  line 10)  and the event informativeness score(  line 11). 
The de fac to standard is the data sets created within the Document Understanding Conference(  DUC)  shared evaluation tasks organized between 2001 and 2007. 
(  1)  the importance of the event s participants, (  2)  the informativeness of the event,  and(  3)  the temporal relations among the events. 
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (Baralis et-al. 2012):
toolkit,  which has been adopted as official DUCâ€™04 tool for performance evaluation. 
toolkit. 
Others considered previously selected patterns in item set items et evaluation to reduce model redundancy. 
Experimental results,  performed on the DUCâ€™04 document collection by means of ROUGE toolkit,  show that the proposed approach achieves better performance than a large set of competitors. 
Sentence evaluation and selection steps consider(  i)  a sentence relevance score that combines the tfidf statistics. 
trjk(  Ii)  =(  1 if Ii âŠ† trjk,  0 otherwise(  2)  The coverage of a sentence sjk with respect to the pattern based model is defined as the number of ones that occur in the corresponding coverage vector SCjk. 
In (Shen et-al. 2007):
A second evaluation method is by the ROUGE toolkit,  which is based on Ngram statistics Lin and Hovy,  2003. 
Many methods,  including supervised and unsupervised algorithms,  have been developed for extractive document summarization. 
A similar method is to select the lead sentence in each paragraph. 
In (He et-al. 2012):
We use the ROUGE(  RecallOriented Understudy for Gi sting Evaluation)  toolkit(  Lin 2004)  which has been widely adopted by DUC for automatic summarization evaluation. 
We choose two automatic evaluation methods ROUGEN and ROUGEL in our experiment. 
ROUGE toolkit reports separate scores for 1,  2,  3 and gram,  and also for the longest common subsequence. 
In (Baralis et-al. 2013):
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
To perform stop word elimination,  we adopted the Natural Language Toolkit(  NTLK)  stop word corpus. 
8.4-tText summarization evaluation programs
In (Neto et-al. 2000):
This paper describes a text mining tool that performs two tasks,  namely document clustering and text summarization. 
Our text summarization algorithm is based on computing the value of aTFISF(  term frequency â€“ inverse sentence frequency)  measure for each word,  which is an adaptation of the conventional TFIDF(  term frequency â€“ inverse document frequency)  measure of information retrieval. 
However,  the textual,  unstructured nature of documents makes these two text mining tasks considerably more difficult than their data mining counterparts. 
In (Dunlavy et-al. 2007):
We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences(  DUC)  as measured by the best known automatic metric for summarization system evaluation,  ROUGE. 
Information retrieval Latent semantic indexingClusteringSummarizationText processing Sentence trimming. 
In many of the DUC evaluations,  similar lead sentence summaries have been used as baselines,  representing a summarization approach requiring minimal text and or natural language processing. 
In (Hovy et-al. 2006):
The text summarization community has also searched for automatic summary evaluation methods that produce reliable scores that correlate well with human scoring. 
A good automatic summarization evaluation procedure should be able to differentiate good systems from bad ones. 
32 automatic summarization systems participated to create question focused summaries by answering a list of complicated questions from sets of 2550 texts. 
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193â€“197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764â€“7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675â€“1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514â€“14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196â€“206
0074 Amigo et-al. 2005 AmigÃ³ E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL â€™05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280â€“289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584â€“599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260â€“273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553â€“563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208â€“1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SACâ€™12), pp 782â€“786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96â€“109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366â€“377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL â€™05), pp 141â€“148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107â€“117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829â€“833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335â€“336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91â€“100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353â€“361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85â€“112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759â€“777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, Oâ€™Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588â€“1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613â€“1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126â€“144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755â€“5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780â€“5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1â€“16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340â€“348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64â€“73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1â€“39
0037 GlavaÅ¡ G, Å najder J 2014 GlavaÅ¡ G, Å najder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904â€“6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40â€“48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128â€“137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203â€“225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717â€“727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258â€“268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620â€“1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSPâ€™06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIRâ€™ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202â€“209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33â€“64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212â€“225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515â€“1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107â€“117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81â€“94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382â€“386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591â€“594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319â€“322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608â€“1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737â€“747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61â€“66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255â€“262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695â€“1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366â€“1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788â€“791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462â€“471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887â€“902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490â€“500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778â€“787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74â€“81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912â€“920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168â€“178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61â€“66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159â€“165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404â€“411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132â€“138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41â€“55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319â€“324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42â€“54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227â€“237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190â€“198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1â€“135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210â€“218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298â€“1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123â€“132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79â€“88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1â€“4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919â€“938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801â€“815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419â€“430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206â€“213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177â€“184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862â€“2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1â€“8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224â€“233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112â€“9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341â€“359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513â€“523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948â€“961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37â€“50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643â€“1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75â€“95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600â€“1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35â€“41
