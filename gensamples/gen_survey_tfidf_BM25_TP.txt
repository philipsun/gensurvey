root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (Leite and Rino 2006):
Need of a Corpus. 
 Compression Rate = 30%(  extract length / source text length). 
Two Main Approaches for Automatic Summarization. 
John,  G. 
3364. 
Computational Linguistics,  23(  1) ,  pp. 
Segmenting Text into MultiParagraph Subtopic Passages,. 
TextTiling. 
(  1997). 
Hearst,  M. 
Morgan Kaufmann Publishers. 
359366,  San Francisco,  CA. 
In Proceedings of the International Conference on Machine Learning,  pp. 
Correlation based feature selection of discrete and numeric class machine learning. 
In (Neto et-al. 2000):
This paper describes a text mining tool that performs two tasks,  namely document clustering and text summarization. 
document clustering and text summarization. 
By contrast,  text mining deals with unstructured or semi structured data,  namely the text found in articles,  documents,  etc In addition to the availability of little(  if any)  structure in the text,  there are other reasons why text mining is so difficult. 
2-tVarious types of text Summarization
In (Gupta V 2013):
Various features used for summarizing multilingual HindiPunjabi text are given below. 
Various features used in this algorithm for summarizing multilingual Hindi and Punjabi text are. 
Various automatic text summarization systems are commercially or non commercially available for most of the commonly used natural languages. 
Various HindiPunjabi noun words which are in bold,  italics or underlined font or having high scores for TF(  TermFrequency). 
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (CR100):
Statistical differences according to a ttest are indicated with a star As it can be seen from the results obtained,  we can confirm that for generic summaries,  it is better to generate the new sentences from the first words of each original sentence. 
In contrast,  abs tractive approaches require a more elaborate process,  involving sentence compression,  information fusion,  and or language generation. 
Section III describes the word graph based method for compressing and merging sentences. 
In (Ko et-al. 2003):
Statistical features of this test set are as the following Table 1. 
Although all the techniques presented above are easily computed,  these approaches depend very much on the particular format and style of writing. 
DOCUSUM is our summarization system based on new topic keyword identification method. 
3.2-tTopic based approaches
In (Bairi et-al. 2015):
Topic Specificity,  Topic Clarity,  and Topic Relevance. 
Topic Coherence. 
Topic Relevance. 
Topic Specificity. 
Topic Clarity. 
We show from our experiments that this approach performs better than all other approaches and baselines. 
Penalty based diversity. 
Feature based Functions. 
Topic clarity is the fraction of descendant topics that cover one or more documents. 
3.3-tGraph based approaches
In (Moawad IF, Aref M 2012):
There are many rules can be derived based on many factors. 
The Rich Semantic Graph Reduction Phase. 
The Rich Semantic Graph Creation Phase. 
the Rich Semantic Graph Creation Phase,  the Rich Semantic Graph Reduction Phase,  and the Summarized Text Generation Phase. 
3)  The Rich Semantic Graph Generation module. 
The ranking method is based on deriving the average weight of each concept(  word sense)  and the average weight of the whole sentence concepts based on(  1)  and(  2)  respectively. 
3.4-tDiscourse based approaches
In (Moawad IF, Aref M 2012):
x Discourse Structuring Process. 
The exponential growth in data increases the need for intelligent filtering and knowledge based approaches to reduce the time needed to absorb the key facts in documents and to avoid drowning in it. 
Lexicalization,  Discourse Structuring,  Aggregation,  and Referring Expression processes. 
The module uses the PDTB(  Pann Discourse Tree Bank Model)  relations. 
Many approaches addressed this problem by building systems depending on the type of the required summary. 
In (Ferreira et-al. 2014):
Discourse relations. 
Discourse relations(  Wolf & Gibson,  2005). 
The aforementioned approaches present valid contributions to the field and display good performance,  in general. 
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005):
We conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 
Hence,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Thus,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
one used relevance measure to rank sentence relevance,  and the other used latent semantic analysis to identify semantically important sentences. 
In (Aliguliyev 2009):
Summarization of text based documents with a determination of latent topical sections and information rich sentences. 
Global optimization in the summarization of text documents. 
The query based summarizer used only query term information,  and the hybrid summarizer used some discourse information along with query term information. 
4.2-tInformation extraction using sentence based abstraction technique
In (Chan 2006):
In this article,  a sentence based abstraction technique for information extraction is presented. 
Information extraction Automatic summary Shallow text processingConnectionist model. 
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 
We have described a discourse network of sentence abstraction based on textual continuity that arises from a connectionist model. 
In (Bing et-al. 2015):
extraction based approaches,  compression based approaches,  and abstraction based approaches. 
extraction based,  compression based and abstraction based. 
On the other hand,  abstraction based approaches can generate new sentences based on the facts from different source sentences. 
Some works,  albeit less popular,  have studied abstraction based approach that can construct a sentence whose fragments come from different source sentences. 
In (Pang and Lee 2008):
Feature based opinion mining and summarization Comparative sentence and relation extraction Summary. 
Comparative sentence and relation extraction Summary. 
Sentence level sentiment analysisFeaturebased opinion mining and summarization Comparative sentence and relation extraction Summary. 
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007):
Text summarization Document concept lattice Concept Semantic. 
Answers appear frequently in the document set might be considered to be more crucial in understanding the source texts. 
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 
Text summarization is the process of distilling the most important information from sources to produce an abridged version for a particular users and tasks(  Mani & May bury,  1999). 
In (Gupta and Lehal 2010):
Text summarization with neural networks. 
Text summarization using regression for estimating feature weights. 
Text summarization based on fuzzy logic system architecture. 
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Rush et-al. 2015):
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 
To test the effectiveness of this approach we run extensive comparisons with multiple abs tractive and extractive baselines,  including traditional syntax based systems,  integer linear program constrained systems,  information retrieval style approaches,  as well as statistical phrase based machine translation. 
A similar issue in machine translation inspired Bahdanau et al  (  2014)  to instead utilize an attention based contextual encoder that constructs a representation based on the generation context. 
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008):
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 
Table 3 shows the result of using subjective opinions described in Section 5. 
Moreover,  we study how to include subjective opinions to help identify important sentences for summarization. 
We integrate our best cohesion measure together with the subjective opinions. 
In (Zajic et-al. 2008):
Summarization technology might be especially attractive for display of email on mobile devices with limited screen area. 
These two alternatives make opposite hypotheses about the conversational structure of email threads. 
(  Probably a mixture of both)  How should the conversational nature of email threads be conveyed. 
4.6-tSummarization of text through complex network approach
In (Kulkarni and Prasad 2010):
The configurations of the network used for our approach is shown in the Figure   5. 
Authors describe here,  the connectionist model used in the proposed approach for automatic text summarization. 
Summarization algorithm module. 
The occurrence summarization system based on statistical approach count of all the individual words in the sentence is using fuzzy logic over some significant text features. 
The computed feature score is applied to the trained network that returns the final score of every sentence presented in the input text document. 
In (Antiqueira et-al. 2009):
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
They play an important role in complex network theory,  being the hallmark of scale free networks. 
A simple network representation of texts was defined,  which requires only shallow text preprocessing. 
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Ferreira et-al. 2014):
the creation of summaries with 200 and 400 words. 
It combines single document summaries using sentence clustering techniques to generate multi document summaries. 
Edge creation select. 
As already mentioned,  multi document summarization can be classified into generic and query based summarization. 
The 2002 conference was the last one that proposed the contest to create generic multi document summaries. 
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009):
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 
Automatic summarization Genetic algorithm Mathematical regression Feed forward neural network Probabilistic neural network Gaussian mixture model. 
After that we used all models for the summarization task using the maximum likelihood of each category as follows. 
In (Khan et-al. 2015):
Different semantic representations of text used in the literature are ontology based and template based representation. 
Experiment of this study is carried out using DUC2002,  a standard corpus for text summarization. 
Linguistic(  Syntactic)  based approach employs syntactic parser to analyze and represent the text syntactically. 
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Cao et-al. 2015a):
Graph based models play a leading role in the summarization area. 
As for multiple references,  we choose the maximal value. 
The documents are all from the news domain and are grouped into various thematic clusters. 
Briefly,  RNN processes structured inputs(  usually a binary tree)  by repeatedly applying the same neural network at each node. 
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011):
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 
The diversity is very important evidence serving to control the redundancy in the summarized text and produce more appropriate summary. 
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Ganesan et-al. 2010):
Figure 2 shows ROUGE scores of these scoring methods at varying levels of gap. 
This is because extractive methods that just select sentences tend to be much longer resulting in higher recall. 
Due to the subtle variations of redundant opinions,  typical extractive methods are often inadequate for summarizing such opinions. 
Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. 
Evaluation results on a set of review documents show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method. 
In (Ferreira et-al. 2013):
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization. 
Extractive summarization Sentence scoring methods Summarization evaluation. 
• In general,  sentence scoring methods are faster. 
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013):
The correlations among multiple terms are extracted using an established data mining technique,  i e,  association rule mining. 
To represent the most significant correlations among multiple terms a graph based model,  named correlation graph,  is generated. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Kulkarni and Prasad 2010):
Summarization algorithm module. 
Evolutionary Programming(  EP)  model. 
A separate algorithm is developed to determine which modifiers apply to a nucleus. 
Precision recall schemes,  as well as summary accuracy measures which incorporate weightings based on multiple human decisions,  are suggested as particularly suitable in evaluating generic summaries. 
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Almeida and Martins 2013):
Improving summarization performance by sentence compressiona pilot study. 
Improving joint parsing and named entity recognition with non jointly labeled data. 
For the multitask experiments,  we also used the data set of BergKirkpatrick et al  (  2011) ,  but we augmented the training data with extractive summarization and sentence compression data sets,  to help train the compressive summarizer. 
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010):
Text summarization based on fuzzy logic system architecture. 
Statistical selection of signature words. 
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
Automatic text summarization based on fuzzy logic. 
is a multi document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
In (Ko et-al. 2003):
DOCUSUM is our summarization system based on new topic keyword identification method. 
DOCUSUM is a text summarization system based on IR techniques using semantic and statistical methods. 
However,  the summarization of documents with multiple topics is also handled importantly,  because a document with a large topic can consist of a number of smaller topics. 
In (Ferreira et-al. 2014):
Statistical similarity. 
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
The proposed system uses a graph model based on statistic similarities and linguistic treatment to represent the collection of input documents(  differently from Canhasi and Kononenko,  2014,  Chen et al,  2014). 
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015):
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 
Many extraction based summarization methods have been proposed in the past years. 
Thus,  we totally define 13 feature groups for the text summarization task. 
abstract based and extraction based. 
Experiment results on DUC2003 and DUC2004 data sets for text summarization and NUSWide data set for image summarization demonstrate that the introduction of group selection can indeed improve summary performance. 
In (Rino and Modolo 2004):
Three diverse summarization heuristics may be applied to select sentences to include in an extract based on lexical chaining. 
in sentence co selection cos election,  the preprocessing mode is not relevant in content based similarity,  gramming yielded better results for BP. 
Only co selection cos election measures. 
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 
Folksonomy systems are classification systems derived from social tags assigned to multimedia content by everyday users. 
Folksonomies became popular on Web 2.0 as part of social tagging applications,  such as social bookmarking and image annotations uploaded by users. 
In (Glavaš G, Šnajder J 2014):
There are a number of approaches to multi document summarization that first semantically annotate the sentences and then assign relevance scores to sentences based on these annotations,  e g. 
With the amount of documents describing real world events growing rapidly(  e g,  news stories,  intelligence reports,  social media posts) ,  it has become increasingly important to efficiently retrieve and concisely present event oriented information. 
Baralis et al  (  2013)  semantically annotate sentences with concepts from the popular ontological knowledge base Ya go and then score the sentences based on the concepts they contain. 
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Shen et-al. 2007):
in this paper,  which is a stateoftheart sequence labeling method. 
,  the authors observed that hidden topics can be discovered in a document as well as the projection of each sentence on each topic through Latent Semantic Analysis Deer wester et al,  1990. 
The methods based on the last two features require less extra resources and efforts while still achieve better performances compared to other methods,  as shown in Gong and Liu,  2001. 
In this paper,  we use CRF as a tool to model this sequence labeling problem. 
In (Khan et-al. 2015):
First,  it employs semantic role labeling for semantic representation of text. 
After applying semantic role labeling to sentence S,  the corresponding two predicate argument structures are obtained as follows. 
Semantic Similarity Matrix Output. 
The corresponding simple predicate argument structures P and P are obtained after applying semantic role labeling to sentences S and S. 
In (Kaljahi et-al. 2014):
Semantic role la belling label ling(  SRL) (  Gild ea and Jurafsky,  2002)  is the task of identifying the predicates in a sentence,  their semantic arguments and the roles these arguments take. 
Apart from English,  only a few languages have SRL resources and these resources tend to be of limited size compared to the English data sets. 
This can be especially useful for languages which lack such resources and require techniques such as cross lingual transfer to replace them. 
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015):
In fact,  our model incorporate a two level sparse representation model. 
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 
Then atwolevel sparse representation model is devised to extract all the salient sentences. 
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 
In (CR92):
Each topic has 25 news documents and 4 model summaries. 
MDSSparse Liu et al,  proposed a two level sparse representation model,  considering coverage,  sparsity,  and diversity. 
(  1)  Our sparse coding model directly assigns coef cient values. 
After the above preparation steps,  we will introduce our summarization model in Section 2.5. 
Currently,  there are only a few works employing sparse coding for the summarization task. 
In (Yang et-al. 2013):
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Finally,  our system generated a final summary of the given documents based on the top n scored sentences in the Score Table. 
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a):
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 
In this parsing tree,  recursive neural networks measure the importance of all these non terminal nodes. 
It transforms the ranking task into a hierarchical regression process which is modeled by recursive neural networks. 
This process is modeled by recursive neural networks(  RNN). 
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 
In (Cao et-al. 2015c):
R2N2 applies recursive neural networks to learn feature combination. 
ClusterCMRW incorporates the cluster level information into the graph based ranking algorithm. 
REGSUM is a word regression approach based on some advanced features such as word polarities(  Wiebe et al,  2005)  and categories(  Tausczik and Penne baker,  2010). 
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015):
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 
importance,  coherence value and redundancy Variables. 
Hence,  we optimize importance,  non redundancy and coherence simultaneously. 
apply their local coherence model,  the entity grid,  to summary coherence evaluation However,  to our knowledge,  the entity grid has not been used directly in extractive summarization to ensure summary coherence. 
In (Hong and Nenkova 2014):
We now show that the performance of extractive summarization can be improved by better estimation of word weights. 
keyword identification and extractive summarization. 
We show that our better estimation of word importance leads to better extractive summaries. 
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015):
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 
Our problem is different from traditional summarization tasks since we have an underlying DAG as a topic hierarchy that we wish to summarize in response to a subset of documents. 
Figure 1 describes the topic summarization process for creation of the disambiguation page for Apple. 
In this paper,  we investigate structured prediction methods for learning weighted mixtures of sub modular functions to summarize topics for a collection of objects using DAGstructured topic hierarchies. 
In (Sanderson M, Croft WB 1999):
· terms for the hierarchy were to be extracted from the documents and had to best reflect the topics covered within them. 
clusters where membership is based on only one feature. 
Document clustering is based on finding document wide similarities to form clusters. 
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015):
However,  we need to reconcile these sentences with updates from the previous hour to ensure that the most salient and least redundant updates are selected. 
Automatic summarization could deliver relevant and salient information at regular intervals,  even when human volunteers are unable to. 
The AP+SALIENCE model is better able to find salient updates earlier on for the disaster domain,  this is an especially important quality of the model. 
A principal concern in extractive multi document summarization is the selection of salient sentences for inclusion in summary output(  Nenkova and McKeown,  2012). 
In (Yang et-al. 2013):
The Affinity Propagation algorithm updates the availability function and the responsibility function in turns as below. 
,  which iteratively updates the code book and the codewords are updated as the centroids from a nearest neighbor clustering result. 
In our problem for automatic image summarization,  the summary(  which is learned in this manner)  is inclined to be composed by the salient visual components of the original image set. 
4.23.11-tSummarizing multiple documents through system combination
In (Carenini et-al. 2008):
Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. 
Summarizing email conversations is challenging due to the characteristics of emails,  especially the conversational nature. 
This is different from other documents such as newspaper articles and formal reports. 
In this paper,  we use two metrics to measure the accuracy of a system generated summary. 
In (Goldstein et-al. 2000):
,  a linear combination of both criteria is optimized. 
When these 200 documents were added to a set of 4 other topics of 200 documents,  yielding a document set documents et with 1000 documents,  the query relevant multi document summarization system produced exactly the same re suits. 
Consider the situation where the user issues a search query,  for instance on a news topic,  and the retrieval system finds hundreds of closely ranked documents in response. 
In (Wang and Li 2012):
For simple average combination schemes(  such as Ave_Score,  Ave_Rank,  Med_Rank,  RR,  BC,  and CW) ,  they treat each individual summarization system equally. 
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015):
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
Our evaluation reveals for the first time which metric variants significantly outperform others,  optimal metric variants distinct from current recommended best variants,  as well as machine translation metric BLEU to have performance on par with ROUGE for the purpose of evaluation of summarization systems. 
Automatic metrics of summarization evaluation have their origins in machine translation(  MT) ,  with ROUGE(  Lin and Hovy,  2003) ,  the first and still most widely used automatic summarization metric,  comprising an adaption of the BLEU score(  Papineni et al,  2002). 
In (Boudin F, Morin E 2013):
We also investigated the correlation between manual and automatic evaluation metrics and found that ROUGE and BLEU have a medium correlation with manual ratings. 
To assess the effectiveness of automatic evaluation metrics,  we compute the Pearson s correlation coefficient between ROUGE and BLEU scores and averaged manual ratings. 
ROUGE measures are computed using stop word removal and French stemming 5. 
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (Yeh et-al. 2005):
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
This paper proposes two approaches to address text summarization. 
outperforms keyword based text summarization approaches. 
7-tMultilingual approaches for text summarization
In (Gupta and Lehal 2010):
Multilingual text summarization is to summarize the source text in different language to the target language final summary. 
Multilingual Extractive Text summarization. 
Automatic text summarization system. 
Extractive text summarization process. 
Automatic text summarization based on fuzzy logic. 
This paper focuses on extractive text summarization methods. 
8-tSummary evaluation
8.1-tInformativeness evaluation
In (0104):
Informativeness is rendered by the methods of selecting the information from documents to incorporate it into the summary. 
ROUGE includes four automatic evaluation methods that measure the similarity between summaries. 
Following the recent adoption of automatic evaluation techniques(  such as BLEU/NIST)  by the machine translation community,  a similar set of evaluation metrics – known as ROUGE 10 – were introduced for both single and multi document summarization. 
Recently,  a series of government sponsored evaluation efforts in text summarization have taken place in both the United States and Japan. 
In (Graham 2015):
Since MT evaluation in general has its own imperfections,  we do not attempt to indiscriminately impose all MT evaluation methodologies on summarization,  but specifically revisit evaluation methodologies applied to one particular area of summarization,  evaluation of metrics. 
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
We outline an evaluation methodology that overcomes all such challenges,  providing the first method of significance testing suitable for evaluation of summarization metrics. 
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (Li et-al. 2013):
We train the CRF model with the Pocket CRF toolkit 5 using the guided compression corpus collected in Section 3. 
We employ thePenn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree,  and use these dependency features. 
In future,  we would like to further explore the reinforcement relationship between keywords and summaries(  Wan et al,  2007) ,  improve the readability of the sentences generated from the guided compression system,  and report results using multiple evaluation metrics(  Nenkova et al,  2007; Louis and Nenkova,  2012)  as well as performing human evaluations.AcknowledgmentsPart of this work was done during the first author s internship in Bosch Research and Technology Center. 
In (He et-al. 2012):
We choose two automatic evaluation methods ROUGEN and ROUGEL in our experiment. 
Due to limited space,  more information can be referred to the toolkit package. 
Table 1 and Table 2 show the ROUGE evaluation results on DUC 2006 and DUC 2007 data sets respectively. 
8.4-tText summarization evaluation programs
In (Neto et-al. 2000):
“ Text summarization is still an emerging field,  and serious questions remain concerning the appropriate methods and types of evaluation. 
Here the topic is provided as an input to the summarization system,  and the evaluation seeks to determine whether the ... 
In order to make our experiments more relevant,  we have used the evaluation thee valuation framework of a large scale project for evaluating text summarization algorithms. 
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193–197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764–7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675–1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514–14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196–206
0074 Amigo et-al. 2005 Amigó E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL ’05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280–289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584–599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260–273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553–563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208–1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SAC’12), pp 782–786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96–109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366–377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL ’05), pp 141–148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107–117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829–833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335–336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91–100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353–361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85–112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759–777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, O’Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588–1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613–1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755–5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780–5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1–16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340–348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64–73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1–39
0037 Glavaš G, Šnajder J 2014 Glavaš G, Šnajder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904–6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40–48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128–137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203–225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717–727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258–268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620–1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSP’06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33–64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212–225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515–1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107–117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81–94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382–386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591–594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319–322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608–1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737–747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61–66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255–262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695–1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366–1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788–791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462–471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887–902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490–500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778–787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74–81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912–920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168–178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61–66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159–165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404–411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132–138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41–55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319–324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42–54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227–237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190–198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210–218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298–1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123–132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79–88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1–4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919–938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801–815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419–430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206–213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177–184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862–2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1–8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224–233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112–9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341–359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513–523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948–961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37–50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643–1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75–95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600–1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35–41
