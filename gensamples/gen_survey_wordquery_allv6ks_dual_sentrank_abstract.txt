root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
TextTiling is a technique for subdividing texts into multi paragraph multipara graph units that represent passages,  or subtopics
The discourse cues for identifying major subtopic shifts are patterns of lexical co occurrence and distribution
The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts
Multi paragraph Multipara graph subtopic segmentation should be useful for many text analysis tasks,  including information retrieval and summarization
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents
This abstract representation relies on the concept of Information Items   INIT  ,  which we define as the smallest element of coherent information in a text or a sentence
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text
2-tVarious types of text Summarization
We combine lexical,  syntactic,  and discourse features to produce a highly predictive model of human readers judgments of text readabil ity
This is the first study to take into ac count such a variety of linguistic factors and the first to empirically demonstrate that dis course relations are strongly associated with the perceived quality of text
But we know little about the rel ative importance of each factor and how they com bine in determining perceived text quality
We show that various surface metrics generally expected to be related to readability are not very good pre dictors of readability judgments in our Wall Street Journal corpus
We also establish that readability predictors behave differently de pending on the task predicting text readabil ity or ranking the readability
Our ex peri men ts indicate that discourse relations are the one class of features that exhibits robustness across these two tasks
Still,  we do not have unified computational mod els that capture the interplay between various as pe cts pec ts of readability
Most studies focus on a sin gle factor contributing to readability for a given in tended audience
The use of rare words or technical terminology for example can make text difficult to read for certain audience types   CollinsThompson and Call an,  2004; Sch warm Schwa rm and Ostendorf,  2005; Elhadad and Sutaria,  2007
Syntactic complexity is associated with delayed processing time in un der standing   Gibson,  1998   and is another factor that can decrease readability
Text organization   dis course structure  ,  topic development   entity co her ence   and the form of referring expressions also de ter mine readability
This article presents a novel automatic method   AutoSummENG   for the evaluation of summa rization systems,  based on comparing the character ngram graphs representation of the extracted summaries and a number of model summaries
The presented approach is language neutral,  due to its statistical nature,  and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods
Within this study,  we measure the effectiveness of different representation methods,  namely,  word and character ngram graph and histogram,  dif ferent ngram neighborhood indication methods as well as different comparison methods between the supplied representations
A theory for the a priori determination of the methods parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process
Categories and Subject Descriptors
I Ar ti cial Intelligence
Natural Language Process ing Text analysis,  language models
General Terms
Algorithms,  Languages,  Measurement,  Performance
Additional Key Words and Phrases
Automatic summarization,  summarization evaluation,  ngram graph
In this paper,  we introduce TextRank â€“ a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications
In particular,  we propose two innovative unsupervised methods for keyword and sentence extraction,  and show that the results obtained compare favorably with previously published results on established benchmarks
We describe our experience in developing a discourse annotated corpus for community wide use
Working in the framework of Rhetorical Structure Theory,  we were able to create a large annotated resource with very high consistency,  using a well defined methodology and protocol
This resource is made publicly available through the Linguistic Data Consortium to enable researchers to develop empirically grounded,  discourse specific applications
This paper considers the problem of automatic assessment of local coherence
We present a novel entity based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function
Our experiments demonstrate that the induced model achieves significantly higher accuracy than a stateoftheart coherence model
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build
In this work,  we propose a fully data driven approach to abs tractive sentence summarization
Our method utilizes a local attention based model that generates each word of the summary conditioned on the input sentence
While the model is structurally simple,  it can easily be trained endtoend and scales to a large amount of training data
The model shows significant performance gains on the DUC2004 shared task compared with several strong baselines
Determinant al point processes   DPPs   are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory
In contrast to traditional structured models like Markov random elds,  which become intractable and hard to approximate in the presence of negative correlations,  DPPs o er e cient and exact algorithms for sampling,  marginalization,  conditioning,  and other inference tasks
We provide a gentle introduction to DPPs,  focusing on the intuitions,  algorithms,  and extensions that are most relevant to the machine learning community,  and show howDPPs can be applied to real world applications like nding diverse sets of highqualitysearch results,  building informative summaries by selecting diverse sentences from documents,  modeling non overlapping human poses in images or video,  and automatically building timelines of important news stories
This paper concerns relationships among focus of attention,  choice of referring expression,  and perceived coherence of utterances within a discourse segment
It presents a framework and initial theory of centering intended to model the local component of attentional state
The paper examines interactions between local coherence and choice of referring expressions it argues that differences in coherence correspond in part to the inference demands made by different types of referring expressions,  given a particular attentional state
It demonstrates that the attentional state properties modeled by centering can account for these differences
TextTiling is a technique for subdividing texts into multi paragraph multipara graph units that represent passages,  or subtopics
The discourse cues for identifying major subtopic shifts are patterns of lexical co occurrence and distribution
The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts
Multi paragraph Multipara graph subtopic segmentation should be useful for many text analysis tasks,  including information retrieval and summarization
In this paper,  we present our recent work on the development of a scalable personalized webbasedmultidocument summarization and recommendation system
WebInEssence
WebInEssence is designed to help end users e ectively search for useful information and automatically summarize selected documents based on the users personal pro les
We address some of the design issues to improve the scalability and readability of our multi document summarizer included in WebInEssence
Some evaluation results with di erent con gurations are also presented
Information retrieval systems consist of many complicated components
Research and development of such systems is often hampered by the difficulty in evaluating how each particular component would behave across multiple systems
We present a novel integrated information retrieval system the Query,  Cluster,  Summarize   QCS   system which is portable,  modular,  and permits experimentation with different instantiations of each of the constituent text analysis components
Most importantly,  the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build
In this work,  we propose a fully data driven approach to abs tractive sentence summarization
Our method utilizes a local attention based model that generates each word of the summary conditioned on the input sentence
While the model is structurally simple,  it can easily be trained endtoend and scales to a large amount of training data
The model shows significant performance gains on the DUC2004 shared task compared with several strong baselines
This article presents a novel automatic method   AutoSummENG   for the evaluation of summa rization systems,  based on comparing the character ngram graphs representation of the extracted summaries and a number of model summaries
The presented approach is language neutral,  due to its statistical nature,  and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods
Within this study,  we measure the effectiveness of different representation methods,  namely,  word and character ngram graph and histogram,  dif ferent ngram neighborhood indication methods as well as different comparison methods between the supplied representations
A theory for the a priori determination of the methods parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process
Categories and Subject Descriptors
I Ar ti cial Intelligence
Natural Language Process ing Text analysis,  language models
General Terms
Algorithms,  Languages,  Measurement,  Performance
Additional Key Words and Phrases
Automatic summarization,  summarization evaluation,  ngram graph
In this paper,  we consider document summarization as a multi objective optimization problem involving four objective functions,  namely information coverage,  significance,  redundancy and text coherence
These functions measure the possible summaries based on the identified core terms and main topics   i e a cluster of semantically or statistically related core terms
We choose the DUC 2005 and 2006 query oriented summarization tasks to exam the proposed model
The encouraging results indicate that the multi objective optimization based framework for document summarization is truly a promising research direction
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
3.2-tTopic based approaches
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic
However,  these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an outofdate dictionary or thesaurus
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents
We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm
Then,  by exploiting tag clusters,  we analyze the semantic relationships between words in the word frequency table
3.3-tGraph based approaches
In this paper,  we introduce TextRank â€“ a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications
In particular,  we propose two innovative unsupervised methods for keyword and sentence extraction,  and show that the results obtained compare favorably with previously published results on established benchmarks
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query
Our approach makes use of both the sentencetosentence relations and the sentencetoword relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result
3.4-tDiscourse based approaches
We present an approach of identifying the most prominent text sentences using various shallow linguistic features,  taking degree of connective ness among the text units into consideration so as to minimize the poorly linked sentences in the resulting summary
As per the limitations of the current summarizing systems,  the summary generated by those systems contains poorly linked sentences and are not topically salient
Thus,  the paper aims at highlighting the effect of lexical chain scoring after the nouns and compound nouns are chained by searching for lexical cohesive relationships between words in the text using WordNet and using lexicographical relationships such as synonymy and hyponyms
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity   measuring chain score,  word score and finally sentence score   for ranking the text units
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
This paper proposes two approaches to address text summarization modified corpus based approach   MCBA   and LSAbased T.R.M approach   LSA + T.R.M
The first is a trainable summarizer,  which takes into account several features,  including position,  positive keyword,  negative keyword,  centrality,  and the resemblance to the title,  to generate summaries
Two new ideas are exploited
1   sentence positions are ranked to emphasize the significances of different sentence positions,  and   2   the score function is trained by the genetic algorithm   GA   to obtain a suitable combination of feature weights
The second uses latent semantic analysis   LSA   to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map
We evaluate LSA + T.R.M both with single documents and at the corpus level to investigate the competence of LSA in text summarization
The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles
When the compression rate was 30%,  an average fmeasure of 49% for MCBA,  52% for MCBA + GA,  44% and 40% for LSA + T.R.M in single document and corpus level were achieved respectively
In existing unsupervised methods,  Latent Semantic Analysis   LSA   is used for sentence selection sentences election
However,  the obtained results are less meaningful,  because singular vectors are used as the bases for sentence selection from given documents,  and singular vector components can have negative values
4.2-tInformation extraction using sentence based abstraction technique
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents
This abstract representation relies on the concept of Information Items   INIT  ,  which we define as the smallest element of coherent information in a text or a sentence
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text
We present a first attempt made at developing a system from this framework,  along with evaluation results for it from TAC 2010
We also present related work,  both from within and outside of the automatic summarization domain
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines
In this paper,  we provide a novel quantitative model for the creation of a summary by extracting a set of sentences that represent the most salient content of a text
The model is based on a shallow linguistic extraction technique
What distinguishes it from previous research is that it does not work on the detection of specific keywords or cue phrases to evaluate the relevance of the sentence concerned
Instead,  the attention is focused on the identification of the main factors in the textual continuity
Simulation experiments suggest that this technique is useful because it moves away from a purely keyword based method of textual information extraction and its associated limitations
ning,  surface realization,  etc,  and the true abs tractive summarization remains a researcher s dream   Radev et al,  2002   Joint compression and summarization has been used recently to generate high quality summaries
4.3-tText understanding and summarization through document concept lattice
We argue that the quality of a summary can be evaluated based on how many concepts in the original document   s   that can be preserved after summarization
Here,  a concept refers to an abstract or concrete entity or its action often expressed by diverse terms in text
Summary generation can thus be considered as an optimization problem of selecting a set of sentences with minimal answer loss
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics
The local topics will specify the promising subspaces related to the selected concepts and sentences
Based on this lattice,  the summary is an optimized selection of a set of distinct and salient local topics that lead to maximal coverage of concepts with the given number of sentences
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning
It is very difficult for human beings to manually summarize large documents of text
Text Summarization methods can be classified into extractive and abs tractive summarization
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form
4.4-tSentence extraction through contextual information and statistical based summarization of text
This article presents a novel automatic method   AutoSummENG   for the evaluation of summa rization systems,  based on comparing the character ngram graphs representation of the extracted summaries and a number of model summaries
The presented approach is language neutral,  due to its statistical nature,  and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods
Within this study,  we measure the effectiveness of different representation methods,  namely,  word and character ngram graph and histogram,  dif ferent ngram neighborhood indication methods as well as different comparison methods between the supplied representations
A theory for the a priori determination of the methods parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process
Categories and Subject Descriptors
I Ar ti cial Intelligence
Natural Language Process ing Text analysis,  language models
General Terms
Algorithms,  Languages,  Measurement,  Performance
Additional Key Words and Phrases
Automatic summarization,  summarization evaluation,  ngram graph
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning
It is very difficult for human beings to manually summarize large documents of text
Text Summarization methods can be classified into extractive and abs tractive summarization
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In this paper,  we study the problem of summarizing email conversations
We first build a sentence quotation graph that captures the conversation structure among emails
We adopt three cohesion measures clue words,  semantic similarity and cosine similarity as the weight of the edges
Second,  we use two graph based summarization approaches,  Generalized ClueWordSummarizer and PageRank,  to extract sentences as summaries
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones
The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures
Moreover,  subjective words can significantly improve accuracy
We present two approaches to email thread summarization collective message summarization   CMS   applies a multi document summarization approach,  while individual message summarization   IMS   treats the problem as a sequence of single document summarization tasks
Both approaches are implemented in our general framework driven by sentence compression
Instead of a purely extractive approach,  we employ linguistic and statistical methods to generate multiple compressions,  and then select from those candidates to produce a final summary
We demonstrate these ideas on the Enron email collection â€“ a very challenging corpus because of the highly technical language
Experimental results point to two findings that CMS represents a better approach to email thread summarization,  and that current sentence compression techniques do not improve summarization performance in this genre
We describe a sentence level opinion detection system
We first define what an opinion means in our research and introduce an effective method for obtaining opinion bearing and nonopinionbearing words
Then we describe recognizing opinion bearing sentences using these words We test the system on 3 different test sets
MPQA data,  an internal corpus,  and the TREC- 2003 Novelty track data
We show that our automatic method for obtaining opinion bearing words can be used effectively to identify opinion bearing sentences
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document
4.6-tSummarization of text through complex network approach
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary
The graph or network representing one piece of text consists of nodes corresponding to sentences,  while edges connect sentences that share common meaningful nouns
Because various metrics could be used,  we developed a set of 14 summarizers,  generically referred to as CNSumm,  employing network concepts such as node degree,  length of shortest paths,  drings and kc ores
An additional summarizer was created which selects the highest ranked sentences in the 14 systems,  as in a voting system
When applied to a corpus of Brazilian Portuguese texts,  some CNSumm versions performed better than summarizers that do not employ deep linguistic knowledge,  with results comparable to stateoftheart summarizers based on expensive linguistic resources
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines
In this paper,  we provide a novel quantitative model for the creation of a summary by extracting a set of sentences that represent the most salient content of a text
The model is based on a shallow linguistic extraction technique
What distinguishes it from previous research is that it does not work on the detection of specific keywords or cue phrases to evaluate the relevance of the sentence concerned
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
The massive quantity of data available today in the Internet has reached such a huge volume that it has become humanly unfeasible to efficiently sieve useful information from it
One solution to this problem is offered by using text summarization techniques
Text summarization,  the process of automatically creating a shorter version of one or more text documents,  is an important way of finding relevant information in large text libraries or in the Internet
This paper presents a multi document summarization system that concisely extracts the main aspects of a set of documents,  trying to avoid the typical problems of this type of summarization information redundancy and diversity
Such a purpose is achieved through a new sentence clustering algorithm based on a graph model that makes use of statistic similarities and linguistic treatment
The DUC 2002 data set was used to assess the performance of the proposed system,  surpassing DUC competitors by a 50% margin of fmeasure,  in the best case
This paper proposes an optimization based model for generic document summarization
The model generates a summary by extracting salient sentences from documents
This approach uses the sentencetodocument collection,  the summarytodocument collection and the sentencetosentence relations to select salient sentences from given document collection and reduce redundancy in the summary
To solve the optimization problem has been created an improved differential evolution algorithm
The algorithm can adjust crossover rate adaptively according to the fitness of individuals
We implemented the proposed model on multi document summarization task
Experiments have been performed on DUC2002 and DUC2004 data sets
The experimental results provide strong evidence that the proposed optimization based approach is a viable method for document summarization
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
First,  we investigate the effect of each sentence feature on the summarization task
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
Moreover,  we use all feature parameters to train feed forward neural network   FFNN  ,  probabilistic neural network   PNN   and Gaussian mixture model   GMM   in order to construct a text summarizer for each model
Furthermore,  we use trained models by one language to test summarization performance in the other language
The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles
The results of the proposed approach are promising,  especially the GMM approach
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain
Based on utterance comparison,  we introduce an optimal formulation for the widely used greedy maximum marginal relevance   MMR   algorithm
Following the idea that information is spread over the utterances in form of concepts,  we describe a system which nds an optimal selection of utterances covering as many unique important concepts as possible
Both optimization problems are formulated as an integer linear program   ILP   and solved using public domain software
We analyze and discuss the performance of both approaches using various evaluation setups on two well studied meeting corpora
4.9-tQuery-based summarization of multiple documents by applying regression models
Determinant al point processes   DPPs   are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory
In contrast to traditional structured models like Markov random elds,  which become intractable and hard to approximate in the presence of negative correlations,  DPPs o er e cient and exact algorithms for sampling,  marginalization,  conditioning,  and other inference tasks
We provide a gentle introduction to DPPs,  focusing on the intuitions,  algorithms,  and extensions that are most relevant to the machine learning community,  and show howDPPs can be applied to real world applications like nding diverse sets of highqualitysearch results,  building informative summaries by selecting diverse sentences from documents,  modeling non overlapping human poses in images or video,  and automatically building timelines of important news stories
For managing a vast hoard of online or offline information,  summarization can be the useful means because the users can decide about the relevance of an individual document or a document cluster using just summary information
Multi document summaries can enable the users to identify the main theme   central idea   of a cluster of texts very rapidly
This paper presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system
For our experiments,  we develop   1   a primary summarization system,  which extracts sentences to form a draft summary and   2   a trimming component,  which accepts a draft summary for revision
4.10-tMaximum coverage and minimum redundancy in summarization of text
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document   s
In particular,  we model text summarization as an integer linear programming problem
One of the advantages of this model is that it can directly discover key sentences in the given document   s   and cover the main content of the original document   s
This model also guarantees that in the summary can not be multiple sentences that convey the same information
The proposed model is quite general and can also be used for single and multi document summarization
We implemented our model on multi document summarization task
Experimental results on DUC2005 and DUC2007 data sets showed that our proposed approach outperforms the baseline systems
This paper proposes an optimization based model for generic document summarization
The model generates a summary by extracting salient sentences from documents
This approach uses the sentencetodocument collection,  the summarytodocument collection and the sentencetosentence relations to select salient sentences from given document collection and reduce redundancy in the summary
To solve the optimization problem has been created an improved differential evolution algorithm
The algorithm can adjust crossover rate adaptively according to the fitness of individuals
We implemented the proposed model on multi document summarization task
Experiments have been performed on DUC2002 and DUC2004 data sets
The experimental results provide strong evidence that the proposed optimization based approach is a viable method for document summarization
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization
Three requisite properties is proposed to form an ideal re constructable reconstruct able sum
mary
Coverage,  Sparsity and Diversity
We then for
malize the task of multi document summarization as an optimization problem according to the above properties,  and use simulated annealing algorithm to solve it
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain
Based on utterance comparison,  we introduce an optimal formulation for the widely used greedy maximum marginal relevance   MMR   algorithm
Following the idea that information is spread over the utterances in form of concepts,  we describe a system which nds an optimal selection of utterances covering as many unique important concepts as possible
Both optimization problems are formulated as an integer linear program   ILP   and solved using public domain software
We analyze and discuss the performance of both approaches using various evaluation setups on two well studied meeting corpora
We conclude on the be ne ts and drawbacks of the presented models and give an outlook on future aspects to improve extractive meeting summarization 2010 Else vier B.V
All rights reserved
Keywords
Multiparty meetings speech Summarization Key phrases Global optimization
Automatic text summarization is an essential tool in this era of information overloading
In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary
It is a direct system where no machine learning is involved
We use a two pass algorithm where in pass one,  we produce a primary summary using Rhetorical Structure Theory   RST   ; this is followed by the second pass where we assign a score to each of the sentences in the primary summary
These scores will help us in generating the final summary
For the final output,  sentences are selected with an objective of maximizing the overall score of the summary whose size should not exceed the user selected limit
We used Rouge to evaluate our system generated summaries of various lengths against those done by a   human   news editorial professional
Experiments on sample texts show our system to outperform some of the existing Arabic summarization systems including those that require machine learning
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum   Graph based Summarizer
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches
The graph nodes,  which represent combinations of two or more terms,  are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations
Then,  the produced node ranking is used to drive the sentence selection process
The experiments performed on benchmark and real life documents demonstrate the effectiveness of the proposed approach compared to many stateoftheart summarizers
Determinant al point processes   DPPs   are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory
In contrast to traditional structured models like Markov random elds,  which become intractable and hard to approximate in the presence of negative correlations,  DPPs o er e cient and exact algorithms for sampling,  marginalization,  conditioning,  and other inference tasks
We provide a gentle introduction to DPPs,  focusing on the intuitions,  algorithms,  and extensions that are most relevant to the machine learning community,  and show howDPPs can be applied to real world applications like nding diverse sets of highqualitysearch results,  building informative summaries by selecting diverse sentences from documents,  modeling non overlapping human poses in images or video,  and automatically building timelines of important news stories
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum   Graph based Summarizer
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches
The graph nodes,  which represent combinations of two or more terms,  are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations
Then,  the produced node ranking is used to drive the sentence selection process
The experiments performed on benchmark and real life documents demonstrate the effectiveness of the proposed approach compared to many stateoftheart summarizers
This paper proposes an optimization based model for generic document summarization
The model generates a summary by extracting salient sentences from documents
This approach uses the sentencetodocument collection,  the summarytodocument collection and the sentencetosentence relations to select salient sentences from given document collection and reduce redundancy in the summary
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information
As the length of sentences is short and the content it contains is limited,  the bagofwords cosine similarity traditionally used for document clustering is no longer reasonably suitable
Special treatment for measuring sentence similarity is necessary
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters
The effectiveness of the proposed framework is demonstrated by both the cluster quality analysis and the summarization evaluation conducted on the DUC 2004 and DUC2007 data sets
The problem of using topic representations for multi document summarization   MDS   has received considerable attention recently
Several topic representations have been employed for producing informative and coherent summaries
In this article,  we describe five previously known topic representations and introduce two novel representations of topics based on topic themes
We present eight different methods of generating multi document summaries and evaluate each of these methods on a large set of topics used in past DUC workshops
Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations
With the number of documents describing real world events and event oriented information needs rapidly growing on a daily basis,  the need for efficient retrieval and concise presentation of event related information is becoming apparent
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events
4.18-tStatistical and linguistic based summarization system for multiple documents
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning
It is very difficult for human beings to manually summarize large documents of text
Text Summarization methods can be classified into extractive and abs tractive summarization
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form
The importance of sentences is decided based on statistical and linguistic features of sentences
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words
It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document
In this paper,  a Survey of Text Summarization Extractive techniques has been presented
The problem of using topic representations for multi document summarization   MDS   has received considerable attention recently
Several topic representations have been employed for producing informative and coherent summaries
In this article,  we describe five previously known topic representations and introduce two novel representations of topics based on topic themes
We present eight different methods of generating multi document summaries and evaluate each of these methods on a large set of topics used in past DUC workshops
Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations
We propose a framework for abs tractive summarization of multi documents,  which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization   abbreviated as TAOS
The underlying intuition of the proposed TAOS is that different topics can prefer different aspects and the different aspects can be represented by different preference of features   e g,  technical topic may prefer proper noun than sports topic
In order to materialize the intuition of TAOS,  we first extract several groups of features according to topic factors,  and then a group norm penalty   i e,  norm   and latent variables are utilized to select overlapping groups of features
We compare our proposed approach with some stateoftheart methods on DUC2003,  DUC2004 data sets for text summarization and NUSWide data set for image summarization
The results show our method can generate meaningful summarization in terms of ROUGE and Jensenâ€“Shannon Divergence metrics
The technology of automatic document summarization is maturing and may provide a solution to the information overload problem
Nowadays,  document summarization plays an important role in information retrieval
2005
Global optimization in the summarization of text documents
Automatic Control and Computer Sciences 39,  42â€“47 ,  Aliguliyev Aliguliyev,  R
M
2006
A novel partitioning based clustering method and generic document summarization
In Proceedings of the 2006 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology   WIâ€“IAT 2006 Workshops     WIâ€“IATWâ€™06  ,  18â€“22 December   pp
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic
However,  these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an outofdate dictionary or thesaurus
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents
We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm
Then,  by exploiting tag clusters,  we analyze the semantic relationships between words in the word frequency table
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others
Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems
For managing a vast hoard of online or offline information,  summarization can be the useful means because the users can decide about the relevance of an individual document or a document cluster using just summary information
Multi document summaries can enable the users to identify the main theme   central idea   of a cluster of texts very rapidly
This paper presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
We propose a framework for abs tractive summarization of multi documents,  which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling
Content selection for summary is made by ranking the predicate argument structures based on optimized features,  and using language generation for generating sentences from predicate argument structures
Our proposed framework differs from other abs tractive summarization approaches in a few aspects
First,  it employs semantic role labeling for semantic representation of text
Secondly,  it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text and finally it ranks the predicate argument structures based on features weighted by genetic algorithm   GA
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization
Three requisite properties is proposed to form an ideal re constructable reconstruct able sum
mary
Coverage,  Sparsity and Diversity
We then for
malize the task of multi document summarization as an optimization problem according to the above properties,  and use simulated annealing algorithm to solve it
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
We propose a new MDS paradigm called reader aware multi document summarization   RAMDS   Speci cally,  a set of reader comments associated with the news reports are also collected
The generated summaries from the reports for the event should be salient according to not only the reports but also the reader comments
To tackle this RAMDS problem,  we propose a sparsecodingbasedmethod that is able to calculate the salience of the text units by jointly considering news reports and reader comments
Another reader aware characteristic of our framework is to improve linguistic quality via entity rewriting
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
We develop a Ranking framework upon Recursive Neural Networks   R2N2   to rank sentences for multi document summarization
It formulates the sentence ranking task as a hierarchical regression process,  which simultaneously measures the salience of a sentence and its constituents   e g,  phrases   in the parsing tree
This enables us to draw on word level to sentence level supervisions derived from reference summaries
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs
Hierarchical regressions are then conducted with learned features concatenating raw features
Ranking scores of sentences and words are utilized to effectively select informative and non redundant sentences to generate summaries
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously
We represent input documents by means of a bipartite graph consisting of sentence and entity nodes
We rank sentences on the basis of importance by applying a graph based ranking algorithm to this graph and ensure nonredundancyand local coherence of the summary by means of an optimization step
Our graph based method is applied to scienti c articles from the journal PLOSMedicine
We use human judgements to evaluate the coherence of our summaries
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
We study the problem of summarizing DAGstructured topic hierarchies over a given set of documents
Example applications include automatically generating Wikipedia disambiguation pages for a set of articles,  and generating candidate multi labels for preparing machine learning data sets   e g,  for text classification,  functional genomics,  and image classification
Unlike previous work,  which focuses on clustering the set of documents using the topic hierarchy as features,  we directly pose the problem as a sub modular optimization problem on a topic hierarchy using the documents as features
Desirable properties of the chosen topics include document coverage,  specificity,  topic diversity,  and topic homogeneity,  each of which,  we show,  is naturally modeled by a sub modular function
4.23.10-tDisaster summarization through prediction of salient updates
During crises such as natural disasters or other human tragedies,  information needs of both civilians and responders often require urgent,  specialized treatment
Monitoring and summarizing a text stream during such an event remains a difficult problem
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates
We use novel,  disaster specific features for salience prediction,  including geolocations and language models representing the language of disaster
4.23.11-tSummarizing multiple documents through system combination
This paper discusses a text extraction approach to multi document summarization that builds on single document summarization methods by using additional,  available information about the document set as a whole and the relationships between the documents
Multi document summarization differs from single in that the issues of compression,  speed,  redundancy and passage selection are critical in the formation of useful summaries
Our approach addresses these issues by using domain independent techniques based mainly on fast,  statistical processing,  a metric for reducing redundancy and maximizing diversity in the selected passages,  and a modular framework to allow easy parameterization for different genres,  corpora characteristics and user requirements
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern
1   movement away from evaluation by correlation with human assessment   2   omission of important components of human assessment from evaluations,  in addition to large numbers of metric variants   3   absence of methods of significance testing improvements over a baseline
We outline an evaluation methodology that overcomes all such challenges,  providing the first method of significance testing suitable for evaluation of summarization metrics
Our evaluation reveals for the first time which metric variants significantly outperform others,  optimal metric variants distinct from current recommended best variants,  as well as machine translation metric BLEU to have performance on par with ROUGE for the purpose of evaluation of summarization systems
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
This paper focuses on abs tractive text summarization
Our aim is to explore to what extent new sentences generated employing a word graph based method   which either compress or merge information   are suitable for producing abstracts
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed   i e,  COMPENDIUM  ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted
As shown by the results obtained,  this task is very challenging
However,  preliminary experiments carried out prove that the combination of extractive and abs tractive information is a more suitable strategy to adopt towards the generation of abstracts.KeywordsHuman Language Technologies,  automated retrieval and mining,  automated content summarization,  abs tractive techniques,  graph based algorithms
This paper proposes two approaches to address text summarization modified corpus based approach   MCBA   and LSAbased T.R.M approach   LSA + T.R.M
The first is a trainable summarizer,  which takes into account several features,  including position,  positive keyword,  negative keyword,  centrality,  and the resemblance to the title,  to generate summaries
Two new ideas are exploited
1   sentence positions are ranked to emphasize the significances of different sentence positions,  and   2   the score function is trained by the genetic algorithm   GA   to obtain a suitable combination of feature weights
The second uses latent semantic analysis   LSA   to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map
We evaluate LSA + T.R.M both with single documents and at the corpus level to investigate the competence of LSA in text summarization
The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles
When the compression rate was 30%,  an average fmeasure of 49% for MCBA,  52% for MCBA + GA,  44% and 40% for LSA + T.R.M in single document and corpus level were achieved respectively
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query
Our approach makes use of both the sentencetosentence relations and the sentencetoword relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result
Compared to previous query expansion approaches,  our approach can capture more relevant information with less noise
We performed experiments on the data of document understanding conference   DUC   2005 and DUC 2006,  and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the stateoftheart systems
The Internet provides many sources of different opinions,  expressed through user reviews of products,  blogs,  and forum discussions
Systems which could automatically summarize these opinions would be immensely useful for those who wish to use this information to make decisions
The previous work in automatic summarization has completely focused on extractive summarization,  in which key sentences are identified from the source text and extracted to form the output
An alternative solution is abs tractive summarization in which the information from the source text is first extracted into the form of abstract data which is then post processed to infer the most important message from the original text
This work is built upon past work of extractive summarization methods to create abs tractive summaries by creating new sentences in it
This paper conveys the methodology for the abs tractive summarization process and its evaluation considering Telugu,  a south Indian regional language,  as the language of study
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document   s
In particular,  we model text summarization as an integer linear programming problem
One of the advantages of this model is that it can directly discover key sentences in the given document   s   and cover the main content of the original document   s
This model also guarantees that in the summary can not be multiple sentences that convey the same information
The proposed model is quite general and can also be used for single and multi document summarization
We implemented our model on multi document summarization task
Experimental results on DUC2005 and DUC2007 data sets showed that our proposed approach outperforms the baseline systems
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents
This abstract representation relies on the concept of Information Items   INIT  ,  which we define as the smallest element of coherent information in a text or a sentence
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text
We present a first attempt made at developing a system from this framework,  along with evaluation results for it from TAC 2010
We also present related work,  both from within and outside of the automatic summarization domain
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization
Three requisite properties is proposed to form an ideal re constructable reconstruct able sum
mary
Coverage,  Sparsity and Diversity
We then for
malize the task of multi document summarization as an optimization problem according to the above properties,  and use simulated annealing algorithm to solve it
Extensive experiments on summarization benchmark data sets DUC2006 and DUC2007 show that our proposed model is effective and outperforms the stateoftheartalgorithms
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
First,  we investigate the effect of each sentence feature on the summarization task
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
Moreover,  we use all feature parameters to train feed forward neural network   FFNN  ,  probabilistic neural network   PNN   and Gaussian mixture model   GMM   in order to construct a text summarizer for each model
Furthermore,  we use trained models by one language to test summarization performance in the other language
The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles
The results of the proposed approach are promising,  especially the GMM approach
This paper presents SuPor,  an environment for extractive Automatic Summarization of texts written in Brazilian Portuguese,  which can be explored by a specialist on AS to select promising strategic features for extraction
By combining any number of features,  SuPor actually entitles one to investigate the performance of distinct AS systems and identify which groups of features are more adequate for Brazilian Portuguese
One of its systems has outperformed six other extractive summarizers,  signaling a significant grouping of features,  as shown in this paper
We present an approach of identifying the most prominent text sentences using various shallow linguistic features,  taking degree of connective ness among the text units into consideration so as to minimize the poorly linked sentences in the resulting summary
7-tMultilingual approaches for text summarization
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum   Graph based Summarizer
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches
The graph nodes,  which represent combinations of two or more terms,  are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations
Then,  the produced node ranking is used to drive the sentence selection process
The experiments performed on benchmark and real life documents demonstrate the effectiveness of the proposed approach compared to many stateoftheart summarizers
This paper proposes two approaches to address text summarization modified corpus based approach   MCBA   and LSAbased T.R.M approach   LSA + T.R.M
The first is a trainable summarizer,  which takes into account several features,  including position,  positive keyword,  negative keyword,  centrality,  and the resemblance to the title,  to generate summaries
Two new ideas are exploited
1   sentence positions are ranked to emphasize the significances of different sentence positions,  and   2   the score function is trained by the genetic algorithm   GA   to obtain a suitable combination of feature weights
The second uses latent semantic analysis   LSA   to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map
We evaluate LSA + T.R.M both with single documents and at the corpus level to investigate the competence of LSA in text summarization
The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles
When the compression rate was 30%,  an average fmeasure of 49% for MCBA,  52% for MCBA + GA,  44% and 40% for LSA + T.R.M in single document and corpus level were achieved respectively
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query
Our approach makes use of both the sentencetosentence relations and the sentencetoword relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result
Compared to previous query expansion approaches,  our approach can capture more relevant information with less noise
We performed experiments on the data of document understanding conference   DUC   2005 and DUC 2006,  and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the stateoftheart systems
The Internet provides many sources of different opinions,  expressed through user reviews of products,  blogs,  and forum discussions
Systems which could automatically summarize these opinions would be immensely useful for those who wish to use this information to make decisions
The previous work in automatic summarization has completely focused on extractive summarization,  in which key sentences are identified from the source text and extracted to form the output
An alternative solution is abs tractive summarization in which the information from the source text is first extracted into the form of abstract data which is then post processed to infer the most important message from the original text
This work is built upon past work of extractive summarization methods to create abs tractive summaries by creating new sentences in it
This paper conveys the methodology for the abs tractive summarization process and its evaluation considering Telugu,  a south Indian regional language,  as the language of study
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document   s
In particular,  we model text summarization as an integer linear programming problem
One of the advantages of this model is that it can directly discover key sentences in the given document   s   and cover the main content of the original document   s
This model also guarantees that in the summary can not be multiple sentences that convey the same information
The proposed model is quite general and can also be used for single and multi document summarization
We implemented our model on multi document summarization task
Experimental results on DUC2005 and DUC2007 data sets showed that our proposed approach outperforms the baseline systems
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents
This abstract representation relies on the concept of Information Items   INIT  ,  which we define as the smallest element of coherent information in a text or a sentence
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text
We present a first attempt made at developing a system from this framework,  along with evaluation results for it from TAC 2010
We also present related work,  both from within and outside of the automatic summarization domain
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization
Three requisite properties is proposed to form an ideal re constructable reconstruct able sum
mary
Coverage,  Sparsity and Diversity
We then for
malize the task of multi document summarization as an optimization problem according to the above properties,  and use simulated annealing algorithm to solve it
Extensive experiments on summarization benchmark data sets DUC2006 and DUC2007 show that our proposed model is effective and outperforms the stateoftheartalgorithms
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
8-tSummary evaluation
8.1-tInformativeness evaluation
This paper describes an efficient algorithm for language independent generic extractive summarization for single document
The algorithm is based on structural and statistical   rather than semantic   factors
Through evaluations performed on a single document summarization for English,  Hindi,  Gujarati and Urdu documents,  we show that the method performs equally well regardless of the language
The algorithm has been applied on DUC data for English documents and various newspaper articles for other languages with corresponding stop words list and modified st emmer
The results of summarization have been compared with DUC 2002 data using degree of representativeness
For other languages,  the degree of representativeness we get is highly encouraging
Key Words
Content richness,  Theme features,  Sentence reference index,  Partitioning,  Fuzzy summary,  Coherence,  Degree of representative ness
Abs tractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries
In this work,  we aim at developing an abs tractive summarizer
First,  our proposed approach identifies the most important document in the multi document set
The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences
Second,  we generate Kshortest paths from the sentences in each cluster using a word graph structure
Finally,  we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming   ILP   model with the objective of maximizing information content and readability of the final summary
Our ILP model represents the shortest paths as binary variables and considers the length of the path,  information score and linguistic quality score in the objective function
Experimental results on the DUC 2004 and 2005 multi document summarization data sets show that our proposed approach outperforms all the baselines and stateoftheart extractive summarizers as measured by the ROUGE scores
Our method also outperforms a recent abs tractive summarization technique
In manual evaluation,  our approach also achieves promising results on informativeness and readability
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary
The graph or network representing one piece of text consists of nodes corresponding to sentences,  while edges connect sentences that share common meaningful nouns
Because various metrics could be used,  we developed a set of 14 summarizers,  generically referred to as CNSumm,  employing network concepts such as node degree,  length of shortest paths,  drings and kc ores
An additional summarizer was created which selects the highest ranked sentences in the 14 systems,  as in a voting system
When applied to a corpus of Brazilian Portuguese texts,  some CNSumm versions performed better than summarizers that do not employ deep linguistic knowledge,  with results comparable to stateoftheart summarizers based on expensive linguistic resources
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features
With the number of documents describing real world events and event oriented information needs rapidly growing on a daily basis,  the need for efficient retrieval and concise presentation of event related information is becoming apparent
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events
In this article,  we present event graphs,  a novel event based document representation model that filters and structures the information about events described in text
To construct the event graphs,  we combine machine learning and rule based models to extract sentence level event mentions and determine the temporal relations between them
Building on event graphs,  we present novel models for information retrieval and multi document summarization
The information retrieval model measures the similarity between queries and documents by computing graph kernels over event graphs
The extractive multi document summarization model selects sentences based on the relevance of the individual event mentions and the temporal structure of events
Experimental evaluation shows that our retrieval model significantly outperforms well established retrieval models on event oriented test collections,  while the summarization model outperforms competitive models from shared multi document summarization tasks
The problem of using topic representations for multi document summarization   MDS   has received considerable attention recently
Several topic representations have been employed for producing informative and coherent summaries
In this article,  we describe five previously known topic representations and introduce two novel representations of topics based on topic themes
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
Document summarization is of great value to many real world applications,  such as snippets generation for search results and news headlines generation
Traditionally,  document summarization is implemented by extracting sentences that cover the main topics of a document with a minimum redundancy
In this paper,  we take a different perspective from data reconstruction and propose a novel framework named Document Summarization based on Data Reconstruction   DSDR
Specifically,  our approach generates a summary which consist of those sentences that can best reconstruct the original document
To model the relationship among sentences,  we introduce two objective functions
1   linear reconstruction,  which approximates the document by linear combinations of the selected sentences   2   nonnegative linear reconstruction,  which allows only additive,  not subtractive,  linear combinations
In this framework,  the reconstruction error becomes a natural criterion for measuring the quality of the summary
For each objective function,  we develop an efficient algorithm to solve the corresponding optimization problem
8.4-tText summarization evaluation programs
This paper describes a text mining tool that performs two tasks,  namely document clustering and text summarization
These tasks have,  of course,  their corresponding counterpart in â€œ conventional data mining
However,  the textual,  unstructured nature of documents makes these two text mining tasks considerably more difficult than their data mining counterparts
In our system document clustering is performed by using the Autoclassdata mining algorithm
Our text summarization algorithm is based on computing the value of aTFISF   term frequency â€“ inverse sentence frequency   measure for each word,  which is an adaptation of the conventional TFIDF   term frequency â€“ inverse document frequency   measure of information retrieval
Sentences with high values of TFISF are selected to producea summary of the source text
The system has been evaluated on real world documents,  and the results are satisfactory
As part of evaluating a summary automatically,  it is usual to determine how much of the contents of one or more human produced ideal summaries it contains
Past automated methods such as ROUGE compare using fixed word ngrams,  which are not ideal for a variety of reasons
In this paper we describe a framework in which summary evaluation measures can be instantiated and compared,  and we implement a specific evaluation method using very small units of content,  called Basic Elements,  that address some of the shortcomings of ngrams
This method is tested on DUC 2003,  2004,  and 2005 systems and produces very good correlations with human judgments
one or more model   reference   summaries
The method is similar in nature to Basic Elements   Hovy et al,  2005  ,  in that it extends beyonda simple string comparison of word sequences,  reaching instead to a deeper linguistic analysis of the text
We examine an umber of variations of the method,  including the addition of WordNet,  partial matching,  or removing relation labels from the dependencies
In a teston TAC 2008 and DUC 2007 data,  DEPEVAL   summ   achieves comparable or higher correlations with human judgments than the popular evaluation metricsROUGE and Basic Elements   BE
Both methods use handwritten extraction rules to derive dependencies from constituent parses produced by widely available PennII Tree bank parsers
The difference betweenDEPEVAL   summ   and BE is that in DEPEVAL   summ   the dependency extraction is accomplished through an LFG annotation of Ca hill et al     2004   applied to the output of the rerankingparser of Charniak and Johnson   2005  ,  whereas in BE   in the version presented here   dependencies are generated by the Mini par parser   Lin,  1995
Despite relying on a the same concept,  our approach outperforms BE in most comparisons,  and it often achieves higher correlations with human judgments than the string matching metricROUGE   Lin,  2004   A more detailed description of BE and ROUGEis presented in Section 2,  which also gives an account of manual evaluation methods employed atTAC 2008
Section 3 gives a short introduction to the tot he LFG annotation
Section 4 describes in more detail DEPEVAL   summ   and its variants
Section 5 presents the experiment in which we compared the perfomance of all three metrics on theTAC 2008 data   consisting of 5,  952 words summaries   and on the DUC 2007 data   1,  word summaries   and discusses the correlations these metrics achieve
Finally,  Section presents conclusions and some directions for future work This paper presents DEPEVAL   summ  ,  a dependency based metric for automatic evaluation of summaries
Using a re ranking reran king parser and a LexicalFunctional Grammar   LFG   annotation,  we produce as et of dependency triples for each summary The dependency set for each candidate summary is then automatically compared against dependencies generated from model summaries
This paper describes the functionality of MEAD,  a comprehensive,  public domain,  open source,  multi document multilingual summarization environment that has been thus far downloaded by more than 500 organizations
MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query
Our approach makes use of both the sentencetosentence relations and the sentencetoword relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result
Compared to previous query expansion approaches,  our approach can capture more relevant information with less noise
We performed experiments on the data of document understanding conference   DUC   2005 and DUC 2006,  and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the stateoftheart systems
Recent studies on extractive text summarization formulate it as a combinatorial optimization problem such as a Knapsack Problem,  a Maximum Coverage Problem or a Budgeted Median Problem
These methods successfully improved summarization quality,  but they did not consider the rhetorical relations between the textual units of a source document
Thus,  summaries generated by these methods may lack logical coherence
This paper proposes a single document summarization method based on the trimming of a discourse tree
This is a twofold process
First,  we propose rules for transforming a rhetorical structure theory based discourse tree into a dependency based discourse tree,  which allows us to take a tree trimming approach to summarization
Second,  we formulate the problem of trimming a dependency based discourse tree as a Tree Knapsack Problem,  then solve it with integer linear programming   ILP
Evaluation results showed that our method improved ROUGE scores
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193â€“197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764â€“7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675â€“1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514â€“14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196â€“206
0074 Amigo et-al. 2005 AmigÃ³ E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL â€™05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280â€“289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584â€“599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260â€“273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553â€“563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208â€“1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SACâ€™12), pp 782â€“786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96â€“109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366â€“377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL â€™05), pp 141â€“148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107â€“117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829â€“833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335â€“336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91â€“100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353â€“361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85â€“112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759â€“777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, Oâ€™Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588â€“1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613â€“1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126â€“144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755â€“5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780â€“5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1â€“16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340â€“348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64â€“73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1â€“39
0037 GlavaÅ¡ G, Å najder J 2014 GlavaÅ¡ G, Å najder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904â€“6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40â€“48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128â€“137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203â€“225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717â€“727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258â€“268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620â€“1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSPâ€™06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIRâ€™ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202â€“209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33â€“64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212â€“225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515â€“1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107â€“117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81â€“94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382â€“386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591â€“594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319â€“322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608â€“1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737â€“747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61â€“66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255â€“262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695â€“1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366â€“1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788â€“791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462â€“471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887â€“902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490â€“500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778â€“787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74â€“81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912â€“920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168â€“178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61â€“66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159â€“165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404â€“411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132â€“138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41â€“55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319â€“324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42â€“54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227â€“237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190â€“198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1â€“135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210â€“218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298â€“1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123â€“132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79â€“88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1â€“4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919â€“938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801â€“815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419â€“430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206â€“213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177â€“184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862â€“2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1â€“8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224â€“233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112â€“9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341â€“359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513â€“523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948â€“961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37â€“50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643â€“1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75â€“95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600â€“1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35â€“41
