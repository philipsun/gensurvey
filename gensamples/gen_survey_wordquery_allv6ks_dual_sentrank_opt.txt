root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (Moawad IF, Aref M 2012), the authors present following contributions
The input graph contains the information needed to generate the final text. 
To achieve its task,  the phase accesses the domain ontology,  which contains the information needed in the same domain of RSG to generate the final texts. 
The exponential growth in data increases the need for intelligent filtering and knowledge based approaches to reduce the time needed to absorb the key facts in documents and to avoid drowning in it. 
Keywords Text Summarization Abs tractive Summary Semantic Representation Rich Semantic Graph Semantic Graph. 
To achieve its task,  the phase accesses the domain ontology,  which contains the information needed in the same domain of RSG. 
This phase accepts a semantic representation in the form of RSG and generates the summarized text. 
One of the important Natural Language Processing applications is Text Summarization,  which helps users to manage the vast amount of information available,  by condensing documents content and extracting the most relevant facts or topics included. 
Extractive summary is the procedure of identifying important sections of the text and producing them verbatim while abs tractive summary aims to produce important material in a new generalized form. 
Text Summarization can be classified according to the type of summary. 
Besides,  a simulated case study is presented to show how the original text was minimized to fifty percent. 
In this process,  for each verb noun object,  its synonyms are selected by accessing the WordNet ontology to generate the target content. 
For example,  in rule 1,  both main verbs(  MV1 and MV2)  are merged and both sentence objects(  ON1 and ON2)  are merged if the two sentence subjects are instances of the same noun(  N) ,  both sentence verbs are similar,  and finally both sentence objects are similar. 
The Summarized Text Generation Phase. 
This phase aims to generate the abs tractive summary from the reduced Rich Semantic Graph(  RSG). 
18. 
Besides,  the WordNet ontology is accessed to generate multiple texts according to the word synonyms. 
The generated multiple texts are evaluated and ranked,  where the most ranked text is considered. 
The texts evaluation is achieved according two criteria. 
the most frequently used words and the discourse sentence relations. 
Figure   4 shows the main modules composing the Summarized Text Generation phase,  where there are four modules namely the Text planning,  the Sentence Planning,  the Surface Realization,  and the Evaluation modules. 
Firstly,  the text planning module aims to select the appropriate content material to be expressed in the final text. 
Secondly,  the sentence planning module specifies the sentence boundaries,  and generates and orders an intermediate paragraphs. 
Thirdly,  the sentence realization module generates grammatically corrected paragraphs. 
Finally,  because of generating multiple texts,  the module of text evaluation evaluates the final multiple texts based on the most frequently used words using the WordNet ontology and the relations between sentences. 
It decides what information should be included in the generated text. 
This weight is calculated using(  3) ,  where E is the existence probability of the synonym in the input rich semantic graph,  NR represents the synonym WordNet rank,  RT represents the total value of all synonym ranks,  NGS represents the WordNet group by similarity for synonym,  and TG represents the total number of groups by similarity for all synonyms. 
Subject Noun(  SN)  node,  Main Verb(  MV)  node,  and Object Noun(  ON)  node. 
In our approach,  to preserve all semantic information embedded in the input semantic representation(  Rich Semantic Graph) ,  all graph objects(  noun and verb objects)  are considered to be passed to the sentence planning module. 
2)  The Sentence Planning module. 
It improves the fluency or understandability of the text. 
To achieve this objective,  the words of the text should be related to each other,  the clauses should exhibit no unintentional redundancy,  and the different sentences with the same subject should be aggregated. 
The sentence planning module receives noun and verb objects and generates semi paragraphs. 
The sentence planning consists of four main processes. 
Lexicalization,  Discourse Structuring,  Aggregation,  and Referring Expression processes. 
x Lexicalization Process. 
To select the most appropriate synonyms,  a weight W is assigned for every synonym. 
1)  The Text planning module. 
Each sentence is composed of three nodes. 
SN1,  MV1,  ON1. 
and Sen2=. 
For example,  the"  student"  noun has two concepts. 
the first concept is a kind of enrollee or pupil,  and the second is equivalent to scholar. 
Therefore,  the phrase"  graduate student"  is valid only with the first concept. 
x nsubj(  student,  Sara)  x cop(  student,  is)  x det(  student,  a)  x am od(  student,  graduate). 
Figure 3. 
Example of syntactic and morphological tags,  and typed dependency relations. 
x Sentences Ranking process. 
It aims to rank and to threshold the highest ranked rich semantic sub graphs for each sentence. 
To generate single rich semantic graph and to keep the semantic consistency for the whole sentence,  the process considers the first ranked rich semantic sub graph only. 
In (Hearst 1997), the authors present following contributions
With the increase in importance of multimedia information,  especially in the context of Digital Library projects,  the need for segmentation and summarization of alternative media types is becoming increasingly important. 
Salton et al  (  1996)  have recognized the need for multi paragraph multipara graph units in the automatic creation of hypertext links as well as theme generation(  this work is discussed in Section 5). 
First,  Nomoto and Nit ta(  1994)  use too large an interval words because this is approximately the average size needed for their implementation of the blocks version of TextTiling. 
coding discourse and dialogue phenomena,  and especially coding segment boundaries,  may be inherently more difficult than many previous types of content analysis(  for instance,  dividing newspaper articles based on subject matter) "  and so implies that the levels of agreement needed to indicate good reliability for TextTiling may be justified in being lower. 
The next section argues for the need for algorithms that can detect multi paragraph multipara graph subtopic structure(  referred to here interchangeably as passages and subtopic segments) ,  and discusses application areas that should benefit from such structure. 
This section concentrates on two application areas for which the need for multi paragraph multipara graph units has been recognized. 
Pa ice(  1990)  recognizes the need for taking topical structure into account but does not suggest a method for determining such structure. 
We should expect to see,  in grouping together paragraph sized units instead of utterances,  a decrease in the complexity of the feature set and algorithm needed. 
Note that this evaluation does away with the need for LC and HC cutoff levels. 
It suggests a need for recognizing and accommodating very short digressions more effectively. 
Most classification work focuses on identifying main topic(  s) ,  as opposed to TextTiling'  s method of finding both globally distributed main topics and locally occurring subtopics nevertheless,  variations on some existing algorithms should be applicable to subtopic classification. 
This phenomenon occurs in some of the other texts as well,  but to a much lesser extent. 
In (Genest PE, Lapalme G 2011), the authors present following contributions
The cause of this low score is mostly our method for text generation,  which still needs to be refined in several ways. 
Text generation patterns can be used,  based on some knowledge about the topic or the information needs of the user. 
2-tVarious types of text Summarization
In (0079), the authors present following contributions
Coherent text is characterized by various types of cohesive links that facilitate text comprehension(  Halliday and Has an,  1976). 
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non discourse features(  Miltsakaki and Kukich,  2000). 
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (Pardo et-al. 2003b), the authors present following contributions
Deep based approaches,  e g,  those based on the rhetorical structuring of source texts,  address nuclearity to select relevant information. 
There are other surface based approaches that address gist preservation in both mono and multi document summarization. 
3.2-tTopic based approaches
In (Heu et-al. 2015), the authors present following contributions
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
3.3-tGraph based approaches
In (Mihalcea and Tarau 2004), the authors present following contributions
In this paper,  we introduce TextRank – a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications. 
The basic idea implemented by a graph based ranking model is that of voting or recommendation. 
In this paper,  we introduce the TextRank graph based ranking model for graphs extracted from natural language texts. 
Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents,  results in a graph based ranking model that can be applied to a variety of natural language processing applications,  where knowledge drawn from an entire text is used in making local ranking selection decisions. 
Graph based ranking algorithms are essentially away of deciding the importance of a vertex withina graph,  based on global information recursively drawn from the entire graph. 
Graph based ranking algorithms like Klein berg s HITS algorithm(  Klein berg,  1999)  or Google s PageRank(  Br in and Page,  1998)  have been successfully used in citation analysis,  social networks,  and the analysis of the link structure of the World Wide Web. 
In short,  agraphbased ranking algorithm is a way of deciding on the importance of a vertex within a graph,  by taking into account global information recursively computed from the entire graph,  rather than relying only on local vertexspeci c information. 
In the context of Web sur ng,  this graph based ranking algorithm implements the random surfer model,  where a user clicks on links at random with a probability L,  and jumps to a completely new page with probability MONL. 
Notice that the nal values obtained after TextRank runs to completion are not affected by the choice of the initial value,  only the number of iterations to convergence may be different It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google s PageRank(  Br in and Page,  1998) ,  other graph based ranking algorithms such as e g. 
Consequently,  we introduce a new formula forgraphbased ranking that takes into account edge weights when computing the score associated with a vertex in the graph. 
In this section,  we report on our experiments in keyword extraction using TextRank,  and show that the graph based ranking model outperforms the best published results in this problem. 
Iterate the graph based ranking algorithm until convergence. 
One can for instance consider only nouns and verbs for addition to the graph,  and consequently draw potential edges based only on relations that can be established between nouns and verbs. 
the number of keywords based on the size of the text For the data used in our experiments,  which consists of relatively short abstracts,  is set to a third of the number of vertices in the graph. 
The text is therefore represented as a weighted graph,  and consequently we are using the weighted graph based ranking formula introduced in Section After the ranking algorithm is run on the graph,  sentences are sorted in reversed order of their score,  and the top ranked sentences are selected for inclusion in the summary Figure 3 shows a text sample,  and the associated weighted graph constructed for this text. 
2.2 Weighted Graphs In the context of Web sur ng,  it is unusual for a page to include multiple or partial links to another page,  and hence the original PageRank de nit ion forgraphbased ranking is assuming unweighted graphs. 
In this paper,  we introduced TextRank a graph based ranking model for text processing,  and show how it can be successfully used for natural language applications In particular,  we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction,  and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposedstateoftheart algorithms. 
Hence,  the score associated with a vertex is determined based on the votes that are cast for it,  and the score of the vertices casting these votes. 
Formally,  let be a directed graph with the set of vertices  and set of edges,  where is a subset of E. 
Through its iterative mechanism,  TextRank goes beyond simple graph connectivity,  and it is able to score text units based also on the importance of other text units they link to. 
For instance,  sentence 15 in the int he example provided in Figure 3 would not be identi ed as important based on the number of connections it has with other vertices in the graph,  but it isidenti ed as important by TextRank(  and by humans – see the reference summaries displayed in the same gure)  Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries(  headlines consisting of one. 
Intuitively,  TextRank works well because it does not only rely on the local context of a text unit(  vertex) ,  but rather it takes into account information recursively drawn from the entire text(  graph)  Through the graphs it builds on texts,  TextRankidentiﬁesconnections between various entities in at ext,  and implements the concept of recommendation. 
However,  in our model the graphs are build from natural language texts,  and may include multiple or partial links between the units(  vertices)  that are extracted from text. 
TextRank turns out to be well suited for this type of applications,  since it allows ital lows for a ranking over text units that is recursively computed based on information drawn from the entire text. 
where Lis a damping factor that can be set between 0 and 1,  which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph. 
TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself. 
Sort vertices based on their nal score. 
Similar weights are computed for each edge in the graph,  but are not displayed due to space restrictions ROUG E is available at http. 
Identify relations that connect such text units,  and use these relations to draw edges between vertices in the graph. 
A text unit recommends other related text units,  and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation. 
The stateoftheart in this area is currently represented by supervised learning methods,  where a system is trained to recognize keywords in a text,  based on lexical and syntactic features. 
Fo reach For each article,  TextRank generates an words summary the task undertaken by other systems participating in this single document summarization task For evaluation,  we are using the ROUGE evaluation toolkit,  which is a method based on Ngramstatistics,  found to be highly correlated with human evaluations(  Lin and Hovy,  2003). 
After running the algorithm,  a score is associated with each vertex,  which represents the importance of the vertex within the graph. 
Other sentence similarity measures,  such as string kernels,  cosine similarity,  longest common subsequence,  etc   are also possible,  and we are currently evaluating their impact on the summarization performance The resulting graph is highly connected,  with aw eight associated with each edge,  indicating the strength of the connections established between various sentence pairs in the text. 
Starting from arbitrary values assigned to each node in the graph,  the computation iterates until convergence below a given threshold is achieved 1. 
Convergence is achieved when the error rate for any vertex in the graph falls below a given threshold. 
Figure 1 plots the convergence curves for the same sample graph from section 2.1,  with random weights in the interval 0–10 added to the edges. 
While the nal vertex scores(  and therefore rankings)  differ signi cantly as compared to their unweighted alternatives,  the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs. 
Identify text units that best de ne the task at hand,  and add them as vertices in the graph. 
Sunday. 
In (Zhao et-al. 2009), the authors present following contributions
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query. 
Query focused summarization Query expansion Graph based ranking. 
3.4-tDiscourse based approaches
In (Pardo et-al. 2003b), the authors present following contributions
Deep based approaches,  e g,  those based on the rhetorical structuring of source texts,  address nuclearity to select relevant information. 
There are other surface based approaches that address gist preservation in both mono and multi document summarization. 
Indeed,  if we discourse analyze the sample text based on the RST Theory. 
Similarly to the other deep based work,  our system can also produce various summaries for the same source text,  for distinct strategies may be corresponding to varied choices of information units or discourse relations. 
In a similar deep approach based on the Rhetorical Structure Theory. 
Although GistSumm also determines the gist of a source text,  its approach is purely statistical,  avoiding the inherent complexity of those deep based ones. 
approaches. 
Those works aim at guiding the selection of discourse segments that are related to the most salient nucleus and organizing them to produce the final summary structure. 
In doing so,  the source rhetorical structure is pruned,  but its discourse backbone is kept unaltered,  implying that nuclearity referring to the main idea is preserved in the final summary. 
The novelty of our gist based method,  embedded in the so called GistSumm(  GIST SUMMarizer)  system,  consists of both the way gist is identified and used to produce the extract. 
Since a variety of discourse segments can be chosen,  distinct rhetorical structures can be produced,  which result in distinct summaries after surface realization. 
This paper presents a method for text summarization based on the gist of a source text that differs from the related ones in Computational Linguistics. 
A quite considerable limitation of our proposal refers to gist being corresponding to just one sentence,  since very often it is embedded in the thread of discourse and,  thus,  it may be diffuse in the text. 
the gist identification method based on the keywords distribution performs better than that based on the inverse distribution of sentences in the source text,  for the test corpus adopted so far. 
English is the dominant language in the writing and publishing of scientific research in the form of scientific articles However,  many nonnatives users of English suffer the interference of their mother tongues when writing scientific papers in English These users face problems concerning rules of grammar and style,  and or feel unable to generate standard expressions and clauses,  and the longer linguistic compositions which are conventional in this genre In order to ease these users'  problems,  we developed a learning environment for scientific writing named AMADEUS(  Amiable Article Development for User Support)  .. 
AMADEUS consists of several interrelated tools - reference,  support,  critic and tutoring tools - and provides the context in which this dissertation is inserted The main goal of this research is to implement AMADEUS as an agent based architecture with collaborative agents communicating with a special agent embodying a dynamic user model In order to do that we introduce the concept of adaptivity in computer systems and describe several user model shells We also provide details about intelligent agents which were used to implement the user model for the AMADEUS environment. 
scientific texts,  namely,  the Theses Corpus. 
The key point in GistSumm is,  thus,  to identify those sentences that better correlate to gist. 
By means of the gist sentence,  it is possible to build coherent extracts,  which will convey the gist sentence itself and those extra sentences that may complement it,  and,  thus,  make extracts more informative. 
II. 
19. 
2 It also depends on the intended level of detail,  but this is not measurable in GistSumm. 
,  the gist sentence usually appears near the end of the text. 
This is explained below,  along with the description of its processes. 
23. 
To overcome it,  we should extend GistSumm to signal multiple segments,  instead of just one sentence. 
This certainly will compromise compression rates. 
This has also been corroborated by Aretoulaki. 
by observing the words co occurrence,  the extracts are more likely to be coherent by including gist,  they are more likely to convey well the main idea of their source texts. 
,  GistSumm is novel because of both the way gist is determined and used as a guide for extraction(  through lexical cohesion). 
24. 
23. 
Although many authors have stressed the need to convey the main idea and to warrant the textuality of the results in automatic summarization. 
However,  further investigations should explore more deeply such a difference,  for other text genres and domains and for more significant corpora. 
Two conclusions can be withdrawn from this. 
Moreover,  they show that gist conveys better the content of the corresponding source text when it is computed through GistKey,  instead of GistTFISF. 
The experiments described here made evident that the correct determination of the gist sentence usually influences the quality of the related extracts. 
In order to avoid it,  thresholds or other means to correlate sentences to gist must be explored(  e g,. 
20. 
So,  it could not be the gist sentence. 
In (Gupta et-al. 2011), the authors present following contributions
Based on this classification,  automatic summarizers can be characterized as approaching the problem at the surface,  entity,  or discourse level. 
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005), the authors present following contributions
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 
The second uses latent semantic analysis(  LSA)  to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. 
4.2-tInformation extraction using sentence based abstraction technique
In (Genest PE, Lapalme G 2011), the authors present following contributions
(  Vickrey and Koller,  2008)  applies similar techniques,  using a sequence of rule based simp li fications of sentences,  to preprocess documents for Semantic Role Labeling. 
They applied this technique to extractive summarization in(  Rusu et al,  2009)  by building what the authors call semantic graphs,  derived from triplets,  and then using said graphs to identify the most interesting sentences for the summary. 
The work of(  Beigman Klebanov et al,  2004)  simplifies sentences by using MINIPAR parses as a starting point,  in a process similar to ours,  for the purpose of helping information seeking applications in their own task. 
Thus,  we want to move away from the current way we generate sentences,  which is too similar to rule based sentence compression. 
We compute a score based on the frequencies of the terms in the sentences generated from the INITs and select sentences that way. 
Sentence fusion first identifies themes(  clusters of similar sentences)  from the source documents and selects which themes are important for the summary(  a process similar to the sentence selection of centroid based extractive summarization methods(  Radev et al,  2004) )  and then generates a representative sentence for each theme by sentence fusion. 
Generating sentences that do not all sound similar and generic is an additional challenge that we have for now circumvented by reusing the original sentence structure to a large extent,  which is a type of texttotext generation. 
The work of(  Barzilay and McKeown,  2005)  on sentence fusion shows an example of reusing the same syntactical structure of a source sentence to create a new one with a slightly different meaning. 
An INIT is defined as a dated and located subject verb object triple,  relying mostly on syntactical analyses from the MINIPAR parser(  Lin,  1998)  and linguistic annotations from the GATE information extraction engine(  Cunningham et al,  2002). 
Figure 2 shows two examples of sentences that were generated from a source document sentence using the simplified abs tractive summarization framework. 
This is accomplished using the original parse tree of the sentence from which the INIT is taken,  and the NLG realizer SimpleNLG(  Gatt and Reiter,  2009)  to generate an actual sentence. 
Frequency based models,  such as those used for extractive summarization,  could be applied to INIT selection instead of sentence selection. 
The generated sentences are ranked based on their average DF(  the sum of the DF of all the unique lemmas in the sentence,  divided by the total number of words in the sentence). 
According to a recent study(  Ge nest Gen est Gene st et al,  b) ,  there is an empirical limit intrinsic to pure extraction,  as compared to abstraction. 
At the core of moving toward full abstraction,  we need to redefine INITs so that they can be manipulated(  compared,  grouped,  realized as sentences,  etc)  more effectively. 
Recent abs tractive approaches,  such as sentence compression(  Knight and Mar cu,  2000) (  Cohn and Lapata,  2009)  and sentence fusion(  Barzilay and McKeown,  2005)  or revision(  Tan aka Tana ka et al,  2009)  have focused on rewriting techniques,  without consideration for a complete model which would include a transition to an abstract representation for content selection. 
This purpose is not the same as ours,  and triplet extraction was conducted quite superficially(  and thus included a lot of noise) ,  whereas we used several rules to clean up the SVOs that would serve as INITs. 
An Information Item is the smallest element of coherent information in a text or a sentence. 
We intend to use tools and techniques that will enable us to find words and phrases of similar meanings,  and to allow the generation of a sentence that is an aggregate of information found in several source sentences. 
This intentionally vague definition leaves the implementation details to be decided based on resources available. 
Text generation patterns can be used,  based on some knowledge about the topic or the information needs of the user. 
NP Generation Noun phrase generation is based on the sub tree of its head word in the dependency parse tree. 
One could use heuristic rules with different priority levels or pregenerated summary scenarios,  to help decide how to structure sentences and order the summary. 
Generate a Noun Phrase(  NP)  to represent the subject if present Generate a NP to represent the object if present Generate a NP to represent the indirect object if present Generate a complement for the verb if one is present and only if there was no object Generate the Verb Phrase(  VP)  and link all the components together,  ignoring anything else present in the original sentence. 
Paraphrase generation produces sentences with similar meanings,  but paraphrase extraction from texts requires a certain level of analysis. 
Most systems develop ped for the main international conference on text summarization,  the Text Analysis Conference(  TAC) (  Owczarzak and Dang,  2010) ,  predominantly use sentence extraction,  including all the top ranked systems,  which make only minor post editing of extracted sentences(  Conroy et al,  2010) (  Gil lick et al,  2009) (  Ge nest Gen est Gene st et al,  2008) (  Chen et al,  2008). 
This abstract representation relies on the concept of Information Items(  INIT) ,  which we define as the smallest element of coherent information in a text or a sentence. 
The head in the sub tree becomes the head of the NP and children in its parse sub tree are added based on manual rules that determine which children are realized and how. 
We propose the concept of Information Items(  INIT)  to help define the abstract representation. 
The first verb modifier that follows it in the sentence order is used,  including for example prepositional phrases and infinitive clauses. 
Section 3 describes and analyses our first attempt at using this framework,  for the TAC 2010 multi document news summarization task,  followed by the competition s results in section 4. 
However,  during development,  no such naive implementation of selecting INITs provided satisfactory results,  because of the low frequency of those constructs,  and the difficulty to compare them semantically in our current level of abstraction. 
it is difficult for a machine to properly extract information from sentences at an abstract level,  and text generated from noisy data will often be flawed. 
Previous work by(  Rusu et al,  2007)  deals specifically with what the authors call triplet extraction,  which is the same as SVO extraction. 
This low linguistic score is understandable,  because this was our first try at text generation and abs tractive summarization,  whereas the other systems that year used sentence extraction,  with at most minor modifications made to the extracted sentences. 
Subject–Verb–Object(  SVO)  extraction is not new. 
We believe this to be representative of the great difference of our approach compared to extraction. 
We propose a new,  ambitious framework for abs tractive summarization,  which aims at selecting the content of a summary not from sentences,  but from an abstract representation of the source documents. 
Generating sentences should rely less on the original sentence structure and more on the information meant to be transmitted. 
An INIT is the smallest element of coherent information in a text or a sentence. 
In (Chan 2006), the authors present following contributions
In this article,  a sentence based abstraction technique for information extraction is presented. 
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007), the authors present following contributions
We argue that the quality of a summary can be evaluated based on how many concepts in the original document(  s)  that can be preserved after summarization. 
Text summarization Document concept lattice Concept Semantic. 
This task of document understanding(  Dang,  2005)  is a core issue in text summarization. 
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests. 
Following our work in the Document Understanding Conference(  DUC)  2005 and 2006(  Ye et al,  2005,  Ye et al,  2006) ,  our summarization algorithm uses the DCL to select a globally optimum sentence set that represents as many concepts as possible with the fewest words. 
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 
• Motivated by our evaluation metric on answer loss,  we propose a novel document model,  the document concept lattice,  which indexes sentences with respect to their coverage of overlapping concepts. 
Once words that represent unified concepts in the documents are linked,  we represent the sources as a document concept lattice(  DCL). 
Following our work in DUC 2005 and 2006,  we proposed a document concept lattice(  DCL)  model and the corresponding algorithm for summarization. 
As such,  introduce a new data structure,  the document concept lattice,  that compactly represents such hanging structures. 
Such summarization methods simplify the problem of summarization into the problem of selecting a representative subset of the sentences in the original documents. 
In both extractive and abs tractive summarization,  a key consideration is how to properly represent the knowledge contained in the input document. 
In this paper,  we review and detail our approach to automatic,  multi document extractive summarization. 
Based on this lattice,  the summary is an optimized selection of a set of distinct and salient local topics that lead to maximal coverage of concepts with the given number of sentences. 
While an RST approach to summarization is well motivated,  it is difficult to build such trees for single documents without explicit textual cues,  and even more problematic to build a tree for a set of multiple source documents. 
To illustrate this phenomena,  we construct an example document with seven sentences containing a set of artificial answers(  without loss of generality,  these answers could be words,  BEs,  or semantic concepts,  depending on the system) ,  shown in Figure   2. 
For ease of reference,  we denote the set of concepts in the source documents using letters as shown in Table 1. 
Our innovation is in pairing this simple representation with the DCL,  which hierarchically organizes these semantic concepts into a partial ordering lattice structure. 
Answers appear frequently in the document set might be considered to be more crucial in understanding the source texts. 
In practice in multi document summarization,  we have found that sentences that have the same inventory of concepts rarely express different semantics between concepts. 
Considering the various contributions of different concepts,  we further employ inverse document frequency(  IDF3)  from a large text collection to better weight the frequency counts to obtain the representative power(  RP)  of sentence S as follows. 
Our semantic concepts can be thought of as extending the argument for using smaller units to represent the source documents. 
This is because texts often exhibit such repetition on purpose,  in order to aid a reader s understanding of the text. 
(  3)  In view of underlying philosophy,  concepts are designed to grasp the existing entities and actions in the document cluster rather than naive assumption in VSM. 
where N is the number of documents in the corpus,  is the frequency of concept Ci in the document cluster,  and is the number of documents containing concept Ci. 
At the base level of the lattice,  sentence nodes store the inventory of concepts in each sentence,  along with its word count. 
Constructing a DCL for a set of source document enumerates and indexes all concepts into a DAG of internal nodes and pointers to sentences in the leaf nodes. 
(  1)  In a derived node,  the local topic,  represented by the a set of existing concepts,  defines a subspace in the concept space of the document cluster. 
As a document concept model,  DCL has the following properties. 
Besides concepts,  we could also employ other text representations,  such as the original words and open class terms,  to build a lattice to generate local topics and index sentences. 
We now describe how to build the lattice from source documents. 
This is in contrast to abs tractive summarization,  which may compose novel sentences,  unseen in the original sources. 
Text summarization is the process of distilling the most important information from sources to produce an abridged version for a particular users and tasks(  Mani & May bury,  1999). 
we make a strong assumption that relationships between words are stable across input documents. 
Here,  a concept refers to an abstract or concrete entity or its action often expressed by diverse terms in text. 
In our approach,  we model such information using a simple,  shallow lexical representation,  which we term concepts. 
When the original words,  open class terms and concepts are used to build DCLs,  we notice that the ROUGE scores of DCL(  BWS = 1)  increases gradually,  while the average dimensions of original words,  open class terms and concepts existing in each document cluster decreases gradually(  see Table 2). 
• We motivate our approach by considering an automatic summarization evaluation framework where summaries are judged on answer loss. 
A DCL for the sample artificial document is shown in Figure   4. 
The input to OnModer comprises a cluster of relevant documents and an optional topic. 
A problem is that this greedy process may lead the summarization system to pick a local optimum instead of a global one. 
Here,  we focus on examining the efficiency of our proposed DCL model for summarization. 
For instance,  our representation of concepts does not distinguish between John killed Smith and Smith killed John as they both generate the same unordered set of concepts,  namely Smith,  John,  killed. 
From our perspective,  concepts model key facts and answers to important questions that the reader asks. 
All derived nodes render overlapping local topics with various granularity,  which will form a lattice under partial ordering relation as a whole. 
A formal description and construction algorithm of a lattice can be found in Wang,  Dubitzky,  D ntsch,  and Bell(  1999)  and Nguifo,  Duquenne,  and Liquiere(  2003). 
However,  the number of derived nodes and the average frequency per derived node in DCL built by concepts are the largest. 
(  4)  Finally,  a new node containing all concepts is introduced at the base. 
The local topics will specify the promising subspaces related to the selected concepts and sentences. 
Furthermore,  the concept sets with diverse frequencies will overlap each other and form a hierarchy. 
In (Gupta and Lehal 2010), the authors present following contributions
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
attempts to develop an understanding of the main concepts in a document and then express those concepts in clear natural language. 
It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. 
It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. 
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form. 
Summarization tools may also search for headings and other markers of subtopics in order to identify the key points of a document. 
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form. 
Such an approach thus avoids any efforts on deep text understanding. 
The goal of content selection is to identify important concepts mentioned in a document collection. 
Given a set of training document and their extractive summaries,  the summarization process is modeled as a classification problem. 
In a key step for locating important sentences,  NeATS computes the likelihood ratio to identify key concepts in uni grams,  bi grams big rams,  and trigrams,  using the on topic document collection as the relevant set and the off topic document collection as the irrelevant set. 
In (Alguliev et-al. 2013), the authors present following contributions
In order to implement extractive summarization,  some sentence extraction techniques are utilized to identify the most important sentences,  which can express the overall understanding of a given document. 
This paper proposes an optimization based model for generic document summarization. 
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Giannakopoulos et-al. 2008), the authors present following contributions
The AutoSummENG method is based on the concept of using statistically ex tr acted tract ed textual information from summaries integrated into a rich representa tional equivalent of a text to measure similarity between generated summaries and a set of model summaries. 
the type of statistical information extracted,  the representation chosen for the extracted information,  the method of similarity calculation. 
The AutoSummENG method for summarization evaluation is a promising method based on language neutral analysis of texts and comparison to gold standard summaries. 
The method presented herein matches and even exceeds the correlation of the aforementioned methodologies on the newer,  DUC 20065 data in a language neutral,  statistical manner,  while taking into account contextual information. 
The over information common ground of recent information retrieval efforts has created a serious motive for the design and implementation of summarization systems,  which are either based on existing information retrieval practices or provide a new pointofview on the retrieval process. 
These categories of ngrams are based on statistical criteria and are used to describe how noise can deteriorate the performance of our method as a function of the methodology parameters. 
A number of different intermediate representations of summaries information have been introduced in existing summarization evaluation literature,  ranging from automatically extracted snippets to human decided sub sentential portions of text. 
evaluation frameworks uses statistical measures of similarity based on ngrams of words,  4 although it supports different kinds of analysis,  ranging from ngram to semantic Hovy et al   b. 
Although ROUGE takes into account contextual information,  it remains at the word level,  which means we either regard different types of the same word as different or we need to apply(  language dependent)  stemming or lemmatization to remove this effect. 
Therefore,  it would be interesting to hold only useful sub graphs based on a statistical measure of usefulness or nd an algorithmic alternative to our own. 
Our method accounts for these assumptions while retaining language neutrality by using only statistical methods and language independent assumptions for the extraction of information from texts and for the computation of textual similarity. 
Answering the questions posed in Section 1.1,  statistical information related to co occurrence of character ngrams seems to provide important information concerning the evaluation process of summary systems. 
In the domain of automatic summarization,  graphs have been used as a means to determine salient parts of text Mihalcea 2004; Erk an and Radev a,  b or determine query related sentences(  see Otterbacher et al. 
In multi document summarization,  graphs have also been used to detect differences and similarities between source texts Mani and Bloedorn 1997. 
We begin by indicating how the performance of evaluation methods is measured along with the required statistical measures,  then describe existing approaches for the evaluation of summaries and summarization systems. 
On the other hand,  the evaluation of summarization systems seems to be nontrivial in itself. 
The presented approach is language neutral,  due to its statistical nature,  and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods. 
This article presents a novel automatic method(  AutoSummENG)  for the evaluation of summa rization systems,  based on comparing the character ngram graphs representation of the extracted summaries and a number of model summaries. 
In various applications of information extraction and retrieval as well as natural language processing,  there have been a number of uses for the ngram aspect of texts. 
Automatic summarization,  summarization evaluation,  ngram graph. 
Basic elements(  BE) ,  on the other hand,  use extraction techniques based on the use of structured representation in the form headmodi er relation err elation,  where head is a major syntactic constituent and relation is the relation holding between mo di er and head. 
At this point,  we will review underlying theory used in evaluation of summarization systems as well as existing methods of such evaluation. 
Then we provide background related to basic concepts of our methodology,  such as ngrams and graphs,  also presenting how comparisons between graphs are performed and the use of graphs in the domain of automatic summarization. 
Given this,  we have evaluated different representation types based on both the type of represented data(  character or word ngrams)  as well as the use or luck of use of connectivity information between the data(  graph or histogram). 
The study was based on the fact that there are relations between meaningful ngrams that we call symbols and non meaningful ones,  which we call non symbols. 
The assumption made is that if two English phrases are translated into the same foreign phrase with high probability(  shown in the alignment results from a statistically trained alignment algorithm) ,  then the two English phrases are paraphrases of each other Zhou et al   2006. 
We also nd the application of graphs to be a useful representation for multi document summarization,  for example,  in Mihalcea. 
Our method,  named AutoSummENG(  AUTOmatic SUMMary Evaluation based on Ngram Graphs) ,  attempts to have all these qualities,  while bearing results with a high correlation to the responsiveness measure,  which indicates a correlation to human judgement. 
,  uses a method based on machine translation practices where a paraphrase table is created based on parallel aligned corpora in two different languages. 
However,  several systems have managed to extract summaries that are rather informative even though they seem to suffer in terms of the legibility of the sum mary text Dang 2005. 
The results of our experiments indicated that our method outperforms current stateoftheart systems in this sort of correla ti on,  while remaining strictly statistical,  automated,  and context sensitive due to the nature of the representation used,  namely,  the ngram graph(  more on this in Section 3). 
The dif culty in the automation of the summarization process is that summarization,  especially from multiple documents,  proves to be an abs tractive mental process Dang 2005. 
Nevertheless,  it is quite probable that the words expressing the content will exist in the same context or that part of the words used will be identical,  for example,  if different in ections are used. 
Statistical Evidence In order to statistically support whether different approaches indeed rendered different results and,  thus,  conclude which approach is better,  we tested whether the distribution of evaluation performance values for each method was normal(  Gaussian). 
Concerning character ngrams,  in Table I,  we can see that the most promising representation is that of the graph value,  based on the ranking of average performance and robustness(  i e,  least standard deviation). 
//www.ontosum.org/static/AutomaticSummarization. 
They rely on the statistical analysis of co occurring word ngrams between the peer and reference summary. 
,  and summarization and summary evaluation Banko and Vanderwende 2004; Lin and Hovy 2003; Co peck Cope ck and Szpakowicz 2004. 
During its evaluation,  the system was found to perform differently based on its parameters. 
Natural Language Process ing Text analysis,  language models. 
In the automatic evaluation of summarization systems,  we require automatic grades to correlate to human grades. 
This composition occurs in characters,  words,  sentences,  paragraphs,  and so forth and has been founded both by generative as well as statistical language processing research(  e g. 
Another important note is that,  even though there is a difference between the performance of systems,  statistical analysis indicates through con dence intervals that the difference in performance may be due to randomness(  see also Dang. 
Salience has also been determined by the use of graphs,  based on the fact that documents can be represented as small world topology graphs Matsuo et al   2001. 
Tackling the problem of what kind of information should be used to represent a peer and a model summary in the evaluation of a summary,  one should take into account that the surface appearance of two equivalent pieces of the same semantic content need not be identical as happens in the case of paraphrases Zhou et al   2006. 
The information extracted from source texts is a set of indicators of neigh bor hood between ngrams contained within the source text. 
Ngram fuzzy matching detects similar portions of text even if other words appear between the ngram words in the text Lin 2004. 
where different iterations over graph representations of texts determine the salient terms over a set of source texts. 
The method is based on ngram graphs even though it provides support for other histogram based approaches. 
Especially in the automatic summarization domain,  ngrams appear as word ngrams,  either used in the evaluation or the summarization process itself(  e g,  in the ROUGE/BE family of evaluator methods Hovy et al   b,  Lin 2004. 
As an overview of the major evaluation systems performance over the data of DUC 2005 to 2007,  Table XI has been provided,  based partly on Conroy and Dang. 
These problems are found mainly in the domain of multi document summariza ti on where the synthesis of summaries appears to be more than mere extraction of text snippets van Halter en and Teufel 2003; Nenkova 2006. 
To clarify how this happens,  consider the case where an automatic evaluation method is applied on a set of summarization systems,  providing a quantitative estimation of the latter s performance by means of a grade. 
There are various types of correlation measures,  called correlation coef cients,  depending on the context in which they can be applied. 
In this investigation,  the word ngram dimension should be reexamined because it may provide more noise free information considering the fact that whole words usually follow our de nit ion of symbols by being meaningful. 
In (Gupta and Lehal 2010), the authors present following contributions
The importance of sentences is decided based on statistical and linguistic features of sentences. 
Extractive summaries are formulated by extracting key text segments(  sentences or passages)  from the text,  based on statistical analysis of individual or mixed surface level features such as word phrase frequency,  location or cue words to locate the sentences to be extracted. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
Text summarization based on fuzzy logic system architecture. 
Automatic text summarization based on fuzzy logic. 
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
Worse yet,  stitching together de contextualized extracts may lead to a misleading interpretation of anaphors(  resulting in an inaccurate representation of source information,  i e,  low fidelity). 
Text summarization. 
This approach is less expensive and more robust than a summarization technique based entirely on a single method. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
is a very important aspect for text summarization. 
It is an extraction based multi document summarization system. 
Automatic text summarization system. 
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
In query based text summarization. 
Query based extractive text summarization. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
Interest in automatic text summarization,  arose as early as the fifties. 
In (Pardo et-al. 2003b), the authors present following contributions
This paper presents a method for text summarization based on the gist of a source text that differs from the related ones in Computational Linguistics. 
By making GistSumm gist based,  we assume it emulates human summarization in that,  when a person summarizes a text,  s he first tries to identify the gist and,  then,  adds information drawn from the text to complement it. 
Those are added to the extract provided that they satisfy summarization requirements,  namely,  gist preservation,  textuality,  relevance,  and compression constraints. 
Although GistSumm also determines the gist of a source text,  its approach is purely statistical,  avoiding the inherent complexity of those deep based ones. 
,  we also address text summarization by using a central proposition of a text. 
There are other surface based approaches that address gist preservation in both mono and multi document summarization. 
In (Rush et-al. 2015), the authors present following contributions
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 
To test the effectiveness of this approach we run extensive comparisons with multiple abs tractive and extractive baselines,  including traditional syntax based systems,  integer linear program constrained systems,  information retrieval style approaches,  as well as statistical phrase based machine translation. 
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008), the authors present following contributions
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 
4.6-tSummarization of text through complex network approach
In (Antiqueira et-al. 2009), the authors present following contributions
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features. 
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Liu et-al. 2009), the authors present following contributions
The generic single document summarizer based on(  6)  is compared with the one in Wan et al. 
The experiments,  both on ageneric single document summarization evaluation,  and on a querybasedmultidocument evaluation,  verify the e ectiveness of the proposed measures and show that the proposed approach achieves a stateoftheartperformance. 
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009), the authors present following contributions
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 
With the huge amount of information available electronically,  there is an increasing demand for automatic text summarization systems. 
Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user(  Ye et al,  2007,  Steinberger et al,  2007,  Dorr and Gaasterland,  2007,  Diaz and Gerv s,  2007). 
Moreover,  we use all feature parameters to train feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  in order to construct a text summarizer for each model. 
In this paper,  we have investigated the use of genetic algorithm(  GA) ,  mathematical regression(  MR) ,  feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  for automatic text summarization task. 
Section 2 presents the different text feature parameters,  Section 3 is about the proposed automatic summarization model,  Section 4 shows the experimental results and finally Section 5 presents conclusions and future work. 
Text summarization addresses both the problem of selecting the most important portions of text and the problem of generating coherent summaries. 
The different attempts in this field have shown that human quality text summarization was very complex since it encompasses discourse understanding,  abstraction,  and language generation(  Spar ck,  1993). 
Some were about evaluation of summarization using relevance prediction(  Hob son Hobs on et al,  2007) ,  ROUGEeval package(  Sj bergh,  2007) ,  SUMMAC,  NTCIR,  and DUC(  Over et al,  2007)  and voted regression model(  Hirao et al,  2007). 
The process of text summarization can be decomposed into three phases. 
The PNN approach performance evaluation based on precision(  English case). 
Unlike LexRank feature,  Bushy path is a simple and an effective text feature for single and multi document summarization task. 
The PNN approach performance evaluation based on precision(  Arabic case). 
Sequential learning systems such as Hidden Markov Models have been exploited for the text summarization task,  but they cannot fully exploit the rich linguistic features since they have to assume independence among the features for tractability(  Conroy and O’Leary,  2001,  Jing,  2002). 
It is clear from Table 3,  Table 4 that the most important text feature for summarization is f(  Bushy path)  since it gives the best results. 
Simpler approaches were then explored that consist of extracting representative text spans texts pans,  using statistical techniques and or techniques based on surface domain independent linguistic analyses. 
Automatic summarization Genetic algorithm Mathematical regression Feed forward neural network Probabilistic neural network Gaussian mixture model. 
Within this context,  summarization can be defined as the selection of a subset of the document sentences which is representative of its content. 
Our approaches have been used the feature extraction criteria which gives researchers opportunity to use many varieties of these features based on the used language and the text type. 
Furthermore,  we use trained models by one language to test summarization performance in the other language. 
P(  PNN)  0.4438 0.4249,  0.4627 0.4526 0.4337,  0.4715 0.4543 0.4344,  0.4742. 
Other research includes multi document summarization(  Vanderwende et al,  2007,  Harabagiu et al,  2007)  and summarization for specific domains(  Mo ens,  2007,  Reeve et al,  2007,  Ling et al,  2007). 
P(  PNN)  0.4414 0.4225,  0.4603 0.4486 0.4297,  0.4675 0.4676 0.4487,  0.4865. 
P(  PNN)  0.4453 0.4274,  0.4632 0.4586 0.4407,  0.4765 0.4603 0.4424,  0.4782. 
However,  most IR techniques that have been exploited in text summarization focus on symbolic level analysis,  and they do not take into account semantics such as synonymy,  polysemy,  and term dependency(  Hovy and Lin,  1997). 
Recently many experiments have been conducted for the text summarization task. 
During the s,  information retrieval(  IR)  was employed for the text summarization task(  Aone et al,  1997,  Gold stein et al,  1999,  Gong and Liu,  2001,  Hovy and Lin,  1997,  Kupiec et al,  1995,  Mani and Bloedorn,  1999,  Salton et al,  1997,  Teufel and Mo ens,  1997,  Yeh et al,  2002). 
Av_Rouge1(  PNN)  0.4587 0.4330,  0.4844 0.4756 0.4499,  0.5013 0.4793 0.4536,  0.5050. 
Table 11,  Table 12 show the results of PNN for the 100 Arabic and 100 English articles,  respectively. 
In this work,  sentences of each document are modeled as vectors of features extracted from the text. 
The PNN approach has better precision than the FNNN approach then GA approach then MR approach. 
P(  GMM)  0.5887 0.5704,  0.6071 0.5954 0.5771,  0.6138 0.6036 0.5853,  0.6220. 
P(  GMM)  0.5902 0.5718,  0.6085 0.5936 0.5753,  0.6119 0.6046 0.5873,  0.6219. 
P(  GMM)  0.5923 0.5750,  0.6097 0.5976 0.5803,  0.615 0.6092 0.5919,  0.6266. 
Others were about single and multiple sentence compression using parse and trim approach and a statistical noisy channel approach(  Zajic et al,  2007)  and conditional random fields(  Nomoto,  2007). 
The summarization task can be seen as a two class classification problem,  where a sentence is labeled as correct if it belongs to the extractive reference summary,  or as incorrect otherwise. 
All models performance evaluation based on precision(  English testing data). 
Chronologically select the set of sentences of highest scores based on the required compression rate. 
Av_Rouge1(  GMM)  0.6075 0.5778,  0.6372 0.6124 0.5827,  0.6421 0.6257 0.5960,  0.6554. 
It is clear from the figures that GMM approach gives the best results since GMM has a good capability to model arbitrary densities. 
Because of the lack of powerful computers and difficulty in nature language processing(  NLP) ,  early work on text summarization focused on the study of text genres such as sentence position and cue phrase(  Edmund son,  1969,  Luhn,  1958). 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
Most of the recent work in summarization uses this paradigm. 
Evaluate fitness of each genome(  we define fitness as the average precision obtained with the genome when the summarization process is applied on the training corpus) ,  and retain the fittest 10 genomes to mate for new ones in the next generation. 
The network weights and functions are backed by straightforward Bayesian probability,  giving them an edge over other network models that have to be gradually optimized using techniques like gradient descent. 
First,  we investigate the effect of each sentence feature on the summarization task. 
We can train the models on a data of a certain language and test these models on a data of another language. 
The proposed automatic summarization model. 
Some text features are language dependent like positive and negative keywords while some other features are language independent. 
All models performance evaluation based on the average Rouge score(  DUC 2001 testing data). 
All models performance evaluation based on precision(  DUC 2001 testing data). 
Table 17 shows the results of all models for the DUC 2001 testing data based on precision. 
After that we used all models for the summarization task using the maximum likelihood of each category as follows. 
AI exploited knowledge representations,  such as frames or templates,  to identify conceptual entities from a text and to extract relationships between entities by inference mechanisms. 
In the future work,  we will extend this approach to multi document summarization by addressing some anti redundancy methods which are needed,  since the degree of redundancy is significantly higher in a group of topically related articles than in an individual article as each article tends to describe the main point as well as necessary shared background. 
The genetic algorithm(  GA)  is exploited to obtain an appropriate set of feature weights using the 100 manually summarized Arabic documents and the 50 manually summarized English documents. 
Table 18 shows the results of all models for the DUC 2001 testing data based on the average Rouge score. 
We have exploited the MCBA + GA approach of Yeh et al,  for summarization as described in Section 4.2 using Eq. 
We use an intrinsic evaluation to judge the quality of a summary based on the coverage between it and the manual summary. 
Then we rank each document sentences based on this similarity value. 
The GMM approach performance evaluation based on precision(  English case). 
In this experiment,  we train all previously mentioned models on 8 Arabic features(  we exclude f = positive keyword and f = negative key word since they are language dependent)  and test these models on English data to check whether these eight features are language dependent or not. 
Therefore,  we have exploited the 10 features for summarization. 
Given a set of training vectors of a certain class,  an initial set of means is estimated using kmeans clustering. 
These manually summarized articles were used to train the previously mentioned five models. 
The analysis phase analyzes the input text and selects a few salient features. 
LexRank is used to compute sentence importance based on the concept of eigenvector centrality in a graph representation of sentences for multi document summarization task(  Erk an and Radev,  2004). 
All models performance evaluation based on precision(  Arabic testing data). 
In this experiment,  we train all previously mentioned models on the 10 English features(  using the same 50 English articles)  and test these models on the DUC 2001 data to investigate the proposed system performance on a news wire data. 
Table 15 shows the results of all models for the 100 English articles. 
Therefore,  GA can be used to specify the weight of each text feature. 
Figure   2 shows the proposed automatic summarization model. 
The automatic method which is used to determine whether there is a link between two sentences is the similarity(  vocabulary overlap)  between these two sentences. 
The GMM approach performance evaluation based on precision(  Arabic case). 
There are two types of summarization. 
Use this feature vector as an input of the GMM. 
Extractive summarization methods simplify the problem of summarization into the problem of selecting a representative subset of the sentences in the original documents. 
We divide the value by the sentence length to avoid the bias of its length(  tfi,  n and Length(  s)  are calculated using sentence s from the testing data). 
Table 1,  Table 2 show the MCBA + GA approach performance evaluation based on precision using the first five features for Arabic and English documents,  respectively. 
Table 13,  Table 14 show the results of GMM for the 100 Arabic and 100 English articles,  respectively. 
Abs tractive summarization may compose novel sentences,  unseen in the original sources. 
Table 16 shows the results of all models for the 100 Arabic articles. 
In this experiment,  we train all previously mentioned models on eight English features(  we exclude f = positive keyword and f = negative key word since they are language dependent)  and test these models on Arabic data like the previously mentioned experiment. 
Then we use all features in combination to train genetic algorithm(  GA)  and mathematical regression(  MR)  models to obtain a suitable combination of feature weights. 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
Figure   4 shows the structure of the PNNT implementation. 
A set of sentences is specified as a reference summary for each document based on the compression ratio. 
For a given set of class dependent reference models(  λ1,  λ2)  and one feature vector sequence X = x,  x,  …,  xn,  the minimum error Bays decision rule is. 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
In this section,  we investigate the effect of each feature parameter on summarization by using Eq. 
Save the output of the GMM for each sentence(  the result of formula(  30) ). 
Automated summarization dates back to the Fifties(  Luhn,  1958). 
(  10)  after using the defined weights from GA execution. 
The structure of PNN implementation. 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
The mixture weights,  means,  and covariances are then iteratively trained using the expectation maximization(  EM)  algorithm. 
We are going to exploit the MCBA + GA approach of Yeh et al,  for summarization and use it as a baseline approach. 
The MCBA + GA approach performance evaluation based on precision(  Arabic case). 
The MCBA+GA approach performance evaluation based on precision(  English case). 
(  32)  after using the defined weights from GA execution. 
The summarization precision associated with each feature for different compression rates(  Arabic case). 
The summarization precision associated with each feature for different compression rates(  English case). 
Table 3,  Table 4 show the summarization precision associated with lead approach and each feature for different compression rates for Arabic and English documents respectively. 
Mathematical regression is a good model to estimate the text feature weights(  Jann,  2005,  Richard,  2006). 
The use of Gaussian Mixture models as a classification tool is motivated by the interpretation that the Gaussian components represent some general output dependent features and the capability of Gaussian mixtures to model arbitrary densities(  Reynolds,  1995,  Pellom and Hansen,  1998,  Fattah et al,  a). 
(  10)  after using the defined weights from W. 
(  10)  with individual score using feature weight equal to 1. 
Using formula(  30) ,  a feature vector sequence X may be classified as one of the two classes(  summary or not summary). 
Using this approach,  we constructed a class dependent model for each category. 
One hundred Arabic and 50 English articles were manually summarized using compression rate of 30%. 
For instance,  to investigate the first feature(  sentence position)  on summarization performance,  we use the following equation. 
The lead method is known to be effective for document summarization of newspapers in lower compression ratio. 
The effect of each feature on summarization performance. 
It is clear from Table 15,  Table 16 that the precisions have decreased slightly when the models are trained on one language and tested on the other language. 
Therefore,  it is possible to train some models with some features and use them for another language. 
Figure   5,  Figure   6 show the total system performance in terms of precision for different compression rates in case of all models for Arabic and English articles,  respectively. 
Chronologically select the set of sentences of highest scores based on the required compression rate. 
The results associated with all models for different CR(  Arabic case). 
The results associated with all models for different CR(  English case). 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
Table 5,  Table 6 show the GA approach performance evaluation based on precision using the 10 features for Arabic and English documents,  respectively. 
Therefore,  feed forward neural network can be used to classify a sentence as summary or not summary based on its features. 
Rank all document sentences based on their scores then arrange them in a descending order. 
The system calculates the feature weights using Genetic algorithm. 
Rank all document sentences based on their scores then arrange them in a descending order. 
In (Riedhammer et-al. 2010), the authors present following contributions
This approach is based on the ICSI text summarization system(  Gil lick et al,  2008; Gil lick and Favre,  2009)  and was mo di ed for the fort he meeting domain in(  Gil lick et al,  2009)  . While most MMR implementations rely on words and their frequency throughout the data,  we could already show that using key phrases instead of words to model relevancy leads to better performance for meeting summarization(  Riedhammer et al,  a). 
Using key phrases to model relevance,  redundancy and concepts has already shown to outperform previous word based models(  Riedhammer et al,  a Gil lick et al,  2009)  and also provides a common ground for a fair comparison of sentence and concept based summarization models. 
The selected meeting extracts can then either be juxtaposed to form a short text summarizing a meeting or used as a starting point to enhance browsing experience Extractive summarization algorithms often rely on the measurement of two important aspects. 
In this section,  we detail two models for extractive summarization based on sentence level and concept level scoring. 
The concept based model can also be used for interactive summarization where the user is allowed to re ne the set of concepts and their weight so that sot hat they are more relevant to his needs Using ASR instead of manual transcripts results in auniform loss of performance for all systems,  none of which seems more a ected than the others. 
The similarity measures also share the key phrases and the underlying idea with the concept based model,  ensuring a fair comparison Beside better performance and scalability,  the concept based approach is not a ected by long ILP run times and provides greedy performance signi cantly better than thesentencebased models. 
Independently and from our work,  Takamura and Okumura(  2009)  introduced anILP formulation very similar to our previously published text summarization system(  Gil lick et al,  2008)  for what they call the Maximum Coverage Problem with KnapsackConstraints(  MCKP). 
In text summarization,  relevance is usually de ned by a(  user generated)  query. 
For future work on larger data,  modifying the weighting is definitely of interest Recent work on unsupervised key phrase extraction integrates TFxIDF and graph based models(  Liu et al,  2009)  However,  the focus of our work is to compare sentence andconceptbased summarization. 
In the unsupervised setup,  sentences with the same topical words get similar relevance assessments even if they are pronounced in very different contexts The most widely known algorithm for unsupervised summarization is maximum marginal relevance(  MMR;Carbonell and Gold stein,  1998). 
This is especially important for interactive systems as described for example in(  Riedhammer et al,  a Mieskes et al,  2007) ,  for which a fast responding summarization algorithm is required to give the user immediate feedback Pruning key phrases is intuitively less destructive than pruning sentences as the former reduces the possibility for a sentence to be included rather then excluding it completely Both performance and run time advantage match our ndings in text summarization where the conceptbasedILP system was top ranked in TAC’08 and TAC’09,  at aruntime of about one second per summary with approximately 1000 sentences and 1000 concepts per instance. 
Speech summarization originated from the porting of methods developed for text summarization. 
For example,  while summarizing broadcast news is very similar to the summarization of textual documents,  conversations are much less structured and involve the interaction between multiple speakers Approachesforspeech summarization are mostly extractive and result in a selection of sentences from the input utterances. 
In (Hearst 1997), the authors present following contributions
Some summarization algorithms extract sentences directly from the text. 
Multi paragraph Multipara graph subtopic segmentation should be useful for many text analysis tasks,  including information retrieval and summarization. 
TextTiling is geared towards expository text that is,  text that explicitly explains or teaches,  as opposed to,  say,  literary texts,  since expository text is better suited to the main target applications of information retrieval and summarization. 
With the increase in importance of multimedia information,  especially in the context of Digital Library projects,  the need for segmentation and summarization of alternative media types is becoming increasingly important. 
There are also potential applications in some other areas,  such as text summarization. 
Salton et al  (  1996)  have recognized the need for multi paragraph multipara graph units in the automatic creation of hypertext links as well as theme generation(  this work is discussed in Section 5). 
Moffat et al  (  1994)  find,  somewhat surprisingly,  that manually supplied sectioning information may lead to poorer retrieval results than techniques that automatically subdivide the text. 
This includes author determined segments,  marked orthographically(  paragraphs,  sections,  and chapters) (  Hearst and Pl aunt 1993; Salton,  Allan,  and Buckley 1993; Moffat et al   1994)  and or automatically derived units of text,  including fixed length blocks(  Hearst and Pl aunt 1993; Call an 1994) ,  segments motivated by subtopic structure(  TextTiles) (  Hearst and Pl aunt 1993) ,  or segments motivated by properties of the query(  Mittendorf and Sch iuble 1994). 
In (Alguliev et-al. 2013), the authors present following contributions
According to Mani and May bury(  1999) ,  automatic text summarization takes a partially structured source text from multiple texts written about the same topic,  extracts information content from it,  and presents the most important content to the user in a manner sensitive to the user s needs. 
The major concerns in graph based summarization researches include how to model the documents using text graph and how to transform existing web page ranking algorithms to their variations that could accommodate various summarization requirements(  Wenjie,  Furu,  Qin,  & Yanxiang,  2008). 
The goal of text summarization is to cover as many conceptual sentences as possible using only a small number of sentences. 
Text summarization is a good way to condense a large amount of information into a concise form by selecting the most important and discarding the redundant information. 
The potential of optimization based document summarization models has not been well explored to date. 
To evaluate our method we use the ROUGE metric(  Lin,  2004) ,  which is adopted by DUC as the official evaluation metric for text summarization. 
This contrasts with abs tractive summarization,  where the information in the text is rephrased. 
In (Almeida and Martins 2013), the authors present following contributions
Automatic text summarization is a seminal problem in information retrieval and natural language processing(  Luhn,  1958; Baxendale,  1958; Edmund son,  1969). 
We present a dual decomposition framework for multi document summarization,  using a model that jointly extracts and compresses sentences. 
In Stan Szpakowicz MarieFrancine Mo ens,  editor,  Text Summarization Branches Out. 
An alternative are coverage based models(  §2.1; Filatova and Hatzivassiloglou,  2004; Yih et al,  2007; Gil lick et al,  2008) ,  which seek a set of sentences that covers as many diverse concepts as possible redundancy is automatically penalized since redundant sentences cover fewer concepts. 
The three tasks are instances of structured predictors(  Bak r et al,  2007) ,  and for all of them we assume feature based models that decompose over parts. 
We propose multitask learning(  §4)  as a principled way to train compressive summarizers,  using auxiliary data for extractive summarization and sentence compression. 
In (Ferreira et-al. 2014), the authors present following contributions
One solution to this problem is offered by using text summarization techniques. 
Text summarization,  the process of automatically creating a shorter version of one or more text documents,  is an important way of finding relevant information in large text libraries or in the Internet. 
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
Text summarization(  Wang,  Li,  Wang,  & Deng,  2010) (  TS)  is a method that aims to create a compressed version of one or more documents,  extracting the essential information from them. 
In (Barzilay and Lapata 2005), the authors present following contributions
We present a novel entity based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text. 
First,  an automatic co reference resolution tool expectedly decreases in accuracy because it was trained on well formed human authored texts. 
Existing automatic evaluation measures such as BLEU(  Papineni et al,  2002)  and ROUGE(  Lin and Hovy,  2003) ,  are not designed for the coherence assessment task,  since they focus on content similarity between system output and reference texts. 
Semantic relatedness is computed automatically using Latent Semantic Analysis(  LSA,  Landau er and Duma is 1997)  from raw text without employing syntactic or other annotations. 
In (Glavaš G, Šnajder J 2014), the authors present following contributions
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
Despite this,  many contemporary information retrieval(  IR)  systems(  Cast ells,  Fernandez,  & Val let,  2007; Sarkar,  2012; Turney & Pan tel Pant el,  2010)  still implement or build upon the traditional retrieval models(  Ponte & Croft,  1998; Robertson & Jones,  1976; Salton,  Wong,  & Yang,  1975) ,  which rely on a shallow,  bagofwords representation of documents and keyword based queries. 
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Kulesza and Taskar 2012), the authors present following contributions
• The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
• An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;• A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;• A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011), the authors present following contributions
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Riedhammer et-al. 2010), the authors present following contributions
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain. 
The selected meeting extracts can then either be juxtaposed to form a short text summarizing a meeting or used as a starting point to enhance browsing experience Extractive summarization algorithms often rely on the measurement of two important aspects. 
For example,  while summarizing broadcast news is very similar to the summarization of textual documents,  conversations are much less structured and involve the interaction between multiple speakers Approachesforspeech summarization are mostly extractive and result in a selection of sentences from the input utterances. 
In this section,  we detail two models for extractive summarization based on sentence level and concept level scoring. 
We conclude on the be ne ts and drawbacks of the presented models and give an outlook on future aspects to improve extractive meeting summarization 2010 Else vier B.V. 
Evaluation measuresforsummarization performancelikeROUGE(  Lin,  2004)  or Pyramid(  Nenkova and Passonneau,  2004)  and later developments like Basic Elements(  Hovy et al,  2006)  score summaries based on an overlap of ngrams(  ROUGE) ,  summary content units(  manually annotated parts in the target text Pyramid)  or dependency parsing relations(  Basic Elements)  Formally,  let ci denote the presence of concept i in the summary and sj denote the presence of sentence j in the summary. 
This is especially important for interactive systems as described for example in(  Riedhammer et al,  a Mieskes et al,  2007) ,  for which a fast responding summarization algorithm is required to give the user immediate feedback Pruning key phrases is intuitively less destructive than pruning sentences as the former reduces the possibility for a sentence to be included rather then excluding it completely Both performance and run time advantage match our ndings in text summarization where the conceptbasedILP system was top ranked in TAC’08 and TAC’09,  at aruntime of about one second per summary with approximately 1000 sentences and 1000 concepts per instance. 
– For sentence based scoring,  we rst propose an global formulation for MMR as an integer linear program(  ILP). 
Fundamental frequency and energy contour,  speaking rate,  pauses,  presence of dis uenciesand repetitions have been used for characterizing relevant sentences(  Mas key and Hirschberg,  2005; Zhu and Penn,  2006; Inoue et al,  2004; Xie et al,  b)  Evaluation of speech summarization is quite di cult because no gold standard truth is available. 
In addition,  key phrases are used as concepts in the sub sentence scoring approach. 
sentence based scoring of relevance and redundancy,  and sub sentence based scoring with implicit redundancy. 
In the unsupervised setup,  sentences with the same topical words get similar relevance assessments even if they are pronounced in very different contexts The most widely known algorithm for unsupervised summarization is maximum marginal relevance(  MMR;Carbonell and Gold stein,  1998). 
Speech summarization originated from the porting of methods developed for text summarization. 
Most extractive summarization models rely on an assessment of the suitability of sentences for inclusion ina summary. 
Utterances occurring in both system summaries are typeset as italic It can be observed that the MMR based system favors longer sentences due to the implemented relevance scoring The probably most interesting fact is,  that the mmr ilp summary covers only 46 unique key phrases with a combined weight of 435 but the concept ilp summary covers 88 uniquekeyphrases with a combined weight of 778,  almost twice as much. 
This approach is based on the ICSI text summarization system(  Gil lick et al,  2008; Gil lick and Favre,  2009)  and was mo di ed for the fort he meeting domain in(  Gil lick et al,  2009)  . While most MMR implementations rely on words and their frequency throughout the data,  we could already show that using key phrases instead of words to model relevancy leads to better performance for meeting summarization(  Riedhammer et al,  a). 
However,  the fact that there might be two utterances with approximately the same wording but only one in the ground truth(  thus awarding zero score if the other was extracted)  leads to little adoption of this method in favor of the content oriented evaluations Baselines,  such as the rst sentences,  a random selection,  or the longest sentences can be used to calibrate results(  Riedhammer et al,  a Penn and Zhu,  2008). 
In text summarization,  relevance is usually de ned by a(  user generated)  query. 
This approach goes beyond pairs of sentences to tackle both relevancy and redundancy in the whole summary The idea of concepts has been around for some time especially in the text summarization community. 
Independently and from our work,  Takamura and Okumura(  2009)  introduced anILP formulation very similar to our previously published text summarization system(  Gil lick et al,  2008)  for what they call the Maximum Coverage Problem with KnapsackConstraints(  MCKP). 
Moreover,  humans do not compute similarity between sentences for selecting them in a summary,  they devise the importance of facts that they contain,  which is the motivation of our other global model The concept based summarizer using key phrases andILP for optimization signi cantly outperforms all utterance based systems on all evaluation scenarios. 
For future work on larger data,  modifying the weighting is definitely of interest Recent work on unsupervised key phrase extraction integrates TFxIDF and graph based models(  Liu et al,  2009)  However,  the focus of our work is to compare sentence andconceptbased summarization. 
The former,  which does not require training data,  is represented by algorithms ported from the text community,  such as variants of MMR(  Murray et al,  a Riedhammer et al,  a) ,  graph based methods(  Garg et al,  2009,  and concept based methods(  Filatovaand Hatzivassiloglou,  2004; Riedhammer et al,  b Takamura and Okumura,  2009)  Supervised approaches rely on a classi er,  usually a support vector machine(  Burg es,  1998) ,  to predict a binary class label for each input sentence indicating whether it should be included in the summary or not. 
As mentioned in the beginning of this section,  we limited the computation time to 60 min per meeting and reduced the number of utterances to the 50 highest scoring ones according to their key phrase weight in order to retrieve a(  possibly suboptimal)  result in reasonable time. 
For speaker role and dialog acts,  we used manual annotation of these features in the corpus For generating relevance predictions,  we rely on anAdaboost variant(  Boostexter Schapire and Singer,  2000)  that iteratively selects the best features while re weighting reweigh ting examples in order to focus on more di cult ones(  it often gives as good predictions as SVMs). 
Also,  it remains unclear if the key phrases extracted in(  Liu et al,  2009)  are of better quality,  as the authors did not provide summarization results and we could not compare our approach within their evaluation setup. 
This was also discussed in(  McDonald,  2007)  where the number of sentences had to be reduced to 100 for computational feasibility. 
These features,  among others,  have successfully been used for supervised meeting summarization(  e g,  Xie et al,  b). 
On the Ont he one hand,  unsupervised methods are very enticing for meeting summarization as they do not depend on extensive manually annotated in domain training data. 
Furthermore,  the summarization problem can be speci ed as single document,  i e,  produce a summary for an independent document,  or multi document,  i e,  produce a summary to represent a set of documents which usually cover a similar topic For this work,  we focus on unsupervised methods. 
In contrast to text,  where sometimes di erent words are used to express the same meaning,  people tend to use the same phrases as other discourse participants(  and also stick to that phrase throughout the whole conversation)  in order to nd a common ground for their communication. 
If no query is provided,  an ar ti cialquery is generated to represent the overall gist of the text In(  Filatova and Hatzivassiloglou,  2004) ,  the authors extracted atomic events from written language to use as concepts which are basically pairs of named entities(  relations)  and the words in between(  connectors)  The connectors are further reduced to content verbs or action nouns using an external information source(  in their case WordNet). 
A stud yon study on English and Chinese text data showed that bigramfrequencies are about an order of magnitude larger than gram frequencies(  Ha et al,  2002). 
This con rms previous results using a di erent,  variable length based evaluation setup,  as for example in(  Gil lick et al,  2009; Riedhammer et al,  a). 
Though the previous section provides theoretical models required to build the summarization systems,  the question of how to measure relevance and redundancy and how to nd the concepts remains open. 
Using key phrases to model relevance,  redundancy and concepts has already shown to outperform previous word based models(  Riedhammer et al,  a Gil lick et al,  2009)  and also provides a common ground for a fair comparison of sentence and concept based summarization models. 
We conclude with adiscussion of the scalability of the methods and their exibility towards practical use and,  in a second step,  abs tractive summarization. 
An alternative to automatic evaluation is to assess the usefulness of generated summaries on an information retrieval task(  Murray et al,  2008) ,  however this kind of evaluation involving humans is more expensive to perform. 
Instead,  multiple judges annotate sentences and write abstracts from which a metric,  e g,  ROUGE(  Lin,  2004) ,  Pyramid(  Nenkova and Passonneau,  2004) ,  Basic Elements(  BE; Hovyet al,  2006) ,  is applied to evaluate the quality of the result(  Hori and Furui,  2000; Murray et al,  b Liu and Liu,. 
A supervised system learns how to extract sentences given example documents and respective summaries. 
In (Azmi AM, Al-Thanyyan S 2012), the authors present following contributions
A comprehensive evaluation in Uz da et al  (  2008)  has concluded that automatic text summarization methods which are based on RST are better than extractive summarizers and those with hybrid methods produce worse summaries. 
In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. 
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013), the authors present following contributions
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
Hence,  there is a need for novel graph based summarizers that also consider the correlations among multiple terms and the differences between positive and negative term correlations. 
In fact,  on the one hand,  since they do not consider the underlying correlations among multiple terms,  some relevant facets of the analyzed data could be disregarded. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
• using association rules in graph based summarization to represent correlations among multiple terms,. 
,  the summarizer presented in this paper relies on a general purpose,  graph based approach that discovers and exploits high order correlations among multiple document terms. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
,  GraphSum is a graph based approach that discovers and exploits association rules to also consider the high order correlations among multiple terms. 
To represent the most significant correlations among multiple terms a graph based model,  named correlation graph,  is generated. 
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum(  Graph based Summarizer). 
The correlations among multiple terms are extracted using an established data mining technique,  i e,  association rule mining. 
Unlike all of the above mentioned approaches,  our summarizer discovers association rules from the analyzed document to also represent the correlations among multiple terms in the graph based model. 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
On the other hand,  some of the considered term correlations are potentially misleading,  because they represent negatively correlated associations among terms(  i e,  combinations of terms that occur less than expected in the analyzed data). 
Frequent item sets,  which represent correlations among terms,  are extracted from a transactional representation of the document collection and combined in a graph based model. 
To evaluate the impact of the greedy strategy on the summarization performance,  we compared the GraphSum performance with that achieved by a variant of our summarizer,  namely GraphSumBB,  that accomplishes the coverage task by exploiting a branchandbound algorithm. 
to discover relevant correlations among data. 
Hence,  we also tested two variants of the GraphSum summarizer,  in which maximal and closed frequent item sets are extracted rather than the whole set of frequent item sets. 
This paper presents GraphSum(  Graph based Summarizer) ,  a new multi document summarizer that relies on a graph based summarization strategy. 
Specifically,  graph based summarization focuses on building a graph in which nodes represent either single terms or document sentences,  whereas an edge weighs the strength of the relationship between a pair of nodes. 
The proposed approach entails building and evaluating a correlation graph,  in which the graph nodes represent sets of document terms of arbitrary size,  whereas the edges have a weight that indicates the strength of the correlation between the corresponding pair of nodes. 
However,  graph based summarizers sometimes do not achieve fairly high performance because of the intrinsic limitations of the graph based models. 
For the top ranked DUC’04 summarizer,  i e,  CLASSY,  we reported all of its submitted versions(  i e,  peer,  peer,  and peer). 
Table 3 reports the results that were achieved on the DUC’04 benchmark data sets by GraphSum,  ItemSum,  OTS,  TexLexAn,  the 8 humanly generated summaries,  and the 10 most effective summarizers that have been presented in DUC’04. 
It yields an approximated solution to the problem of selecting the subset of sentences that best covers the graph based model. 
To improve the quality of the generated model,  the model includes only the associations among terms that(  i)  occur frequently and(  ii)  have a strong(  either positive or negative)  correlation in the analyzed collection. 
,  or graph based algorithms. 
GraphSum(  Graph based Summarizer)  is a novel graph based multi document summarizer. 
Among them,  sentence based approaches(  e g,. 
• analyzing positive and negative term correlations separately for document summarization purposes,  and. 
For example,  graph based summarizers generate and exploit a graph to select the most relevant document sentences or keywords. 
The graph nodes,  which represent combinations of two or more terms,  are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations. 
)  in terms of the most commonly used Rouge evaluation scores. 
of the sentence terms, (  ii)  the sentence relevance within the cluster,  and(  iii)  the sentence length. 
A similar graph based model has also been adopted in. 
To effectively discriminate between reliable and unreliable term correlations,  a variant of an established graph ranking strategy. 
This block aims at representing the most significant term correlations hidden in the analyzed document collection by means of a graph based model,  called correlation graph. 
Parameter analysis and comparison between GraphSum and GraphSumBB in terms of Rouge F measure. 
For instance,  the first sentence of the document d contains the distinct terms Data,  Analysis,  and Mine. 
The procedure iterates until the graph based model is fully covered by the summary,  i e,  until the summary coverage vector contains only ones(  line 3). 
The same pair of terms occurs in the first two sentences of the document summary,  because the terms are deemed to be significant for summarization purposes. 
Specifically,  two nodes are connected by a bidirectional edge if their corresponding terms co occur frequently and are strongly correlated with each other(  either positively or negatively)  in the analyzed collection. 
Select the sentence with maximum relevance score among the ones in MaxOnesSentences∗/. 
(  i e,  the minimum significance levels of the mined data correlations). 
Rain ↔ Torrential,  Rain ↔ Heavy,  and Rain ↔ Water,  Creek are three examples of strong term correlations(  e g,  lift(  Rain,  Heavy)  = 46). 
At each iteration,  it selects,  among the sentences with maximum coverage the one with maximal relevance score(  see Formula 4) (  line 6). 
The contribution of the positive and negative term correlations will be differentiated during the summarization process,  as described in the following sections. 
In such a way,  summaries are less likely to contain the sentences in which a combination of terms are negatively correlated with each other. 
,  which also discriminates between positive and negative term set correlations. 
To this purpose,  GraphSum adopts a variant of the popular PageRank ranking algorithm. 
GraphSum does not rely on advanced semantics based models(  e g,  ontologies or taxonomies)  to perform document analysis. 
46. 
The result of the OR operation SC∗ will be denoted as the summary coverage vector throughout the section. 
Since the set covering optimization problem is NPhard,  we tackle the problem by means of a greedy strategy similar to the one that has previously been applied in. 
The association rule extraction task is traditionally accomplished by first generating all the possible subsets of the extracted frequent item sets and then evaluating the candidate associations. 
in the context of document summarization. 
the lift value is either in the range(  0,  max lift or higher than or equal to min lift,  where max lift and min lift are the maximum negative and the minimum positive correlation thresholds,  respectively. 
the support value is equal to or higher than a minimum support threshold min sup and. 
Specifically,  it selects only the association rules for which. 
Since the interest of the statistically independent term associations is marginal in our context of analysis,  GraphSum only considers the frequent and strongly correlated associations between pairs of term sets. 
The greedy sentence selection strategy considers the sentence model coverage to be the most discriminating feature,  i e,  sentences that cover the maximum number of graph nodes are selected first. 
Lift values significantly below 1 show negative correlation,  whereas values significantly above 1 indicate a positive correlation between the item sets A and B,  meaning that the implication between A and B holds more than expected in the source data. 
If lift(  A,  B)  is equal to or close to 1,  then the item sets A and B are not correlated with each other,  i e,  they are statistically independent. 
where s(  A → B)  and c(  A → B)  are the rule support and confidence,  respectively,  and s(  A)  and s(  B)  are the supports of the rule antecedent and consequent. 
GraphSum adopts the same strategy and also exploits the symmetry of the lift measure. 
(  1). 
Let A → B be an association rule. 
Definition 5 Association rule lift. 
could be used,  rather than the confidence index,  to measure the(  symmetric)  correlation between the body and the head of the extracted rules. 
At equal terms,  the sentence with maximal coverage that is characterized by the highest relevance score SR is preferred. 
38. 
To overcome this issue,  the lift(  or correlation)  measure. 
In fact,  when the rule consequent has a relatively high support value,  the corresponding rule could be characterized by a high confidence even if its actual strength is relatively low. 
A more detailed description of the adopted greedy strategy follows. 
Algorithm 1. 
38. 
However,  measuring the strength of a term set correlation in terms of rule confidence could be misleading. 
For example,  recalling the running example in Table 2,  the association rule Data → Analysis has a support equal to in T and a confidence equal to,  because the item set items et Data,  Analysis occurs twice in T,  whereas the implication Data → Analysis holds in half of the cases. 
Its lift l is given by. 
It applies the logic OR operator to the coverage vectors of the selected sentences,  i e,  SC∗ = SC1 ∧ … ∧ SCl,  and selects the subset of sentences for which the resulting binary vector contains the maximal number of ones. 
38. 
to avoid generating all the possible candidates. 
Unlike previous approaches(  e g,. 
22. 
) ,  the graph nodes could also represent a subset of terms with size higher than one(  e g,  Analysis,  Context). 
Some extracts of the correlation graphs that were mined from real life documents are reported in Section 4. 
,  both Data and Analysis are frequent with respect to the support threshold. 
Note that,  by the Apriori principle. 
Since the support of Data,  Analysis in the running example data set is 33% and lift(  Data,  Analysis)  =,  then the corresponding correlation graph G contains two distinct nodes,  which are related to the terms Data and Analysis and are linked by an edge with weight. 
The relevance score of a sentence sjk in the document collection measures the significance of a sentence in terms of the authority of its contained term sets(  nodes)  in the correlation graph. 
As an example,  suppose setting min sup to 1%,  max lift to,  and min lift to 10. 
It is defined as the normalized sum of the PageRank scores that have been assigned to each term set that occurs in the sentence. 
(  4). 
The bi directed edges are weighted by the corresponding rule lift value. 
where tjk is the transaction that is associated with the sentence sjk,  is the sum of the PageRank scores PRik that are associated with every correlation graph node ni covering tjk,  and is the total number of nodes that cover tjk. 
The sentence coverage indicates how much a sentence sjk is pertinent to the association rule graph G. 
To define the sentence coverage,  we first associate with each sentence sjk ∈ D a binary vector SCjk = sc,  …,  sc N,  which hereafter will be denoted as sentence coverage vector,  where N is the number of nodes that are contained in the correlation graph G,  and indicates whether a term set ni covers or not trjk(  see Section 3.2.1). 
Formally speaking,  given an arbitrary term set ni contained in is an indicator function that is defined as follows. 
A correlation graph G that is built on T is a bi directed graph for which the nodes are frequent item sets(  term sets)  in,  whereas its edges link arbitrary node pairs A and B such that either lift(  A,  B)  ∈(  0,  max lift or lift(  A,  B)  ⩾ min lift. 
Let be the set of frequent item sets that were mined from T by enforcing a minimum support threshold min sup. 
(  5). 
Let T be a transactional representation of a document collection D and min sup,  max lift,  and min lift three nonnegative numbers. 
Definition 6 Correlation graph. 
The coverage of a sentence sjk with respect to the correlation graph is defined as the number of ones in the coverage vector SCjk. 
To our purposes,  we formalize the problem of selecting the most representative document sentences in terms of coverage and relevance scores as a set covering problem. 
GraphSum addresses a set covering optimization problem to select the sentences with maximal model coverage and relevance score. 
The concept of correlation graph is formalized as follows. 
Specifically,  we focus on selecting the minimal number of sentences that are characterized by maximal score. 
Specifically,  since lift(  A,  B)  = lift(  B,  A) ,  to evaluate the interest of the correlation between the pair of term sets A and B GraphSum exclusively considers distinct pairs of disjoint frequent item sets A and B such that the union A ∪ B(  and,  hence,  the rule A → B)  is frequent with respect to the minimum support threshold min sup. 
45. 
It measures the strength of the implication. 
Let A and B be two disjoint item sets. 
Its confidence c(  A ∪ B)  is given by. 
B 0.091 0.096 0.093∗ 0.013 0.013 0.013∗. 
C 0.094 0.102 0.098 0.011∗ 0.012∗ 0.012∗. 
D 0.100 0.106 0.102 0.010∗ 0.010∗ 0.010∗. 
E 0.094 0.099 0.097 0.011∗ 0.012 0.012∗. 
F 0.086∗ 0.090 0.088∗ 0.008∗ 0.009∗ 0.009∗. 
G 0.082∗ 0.087∗ 0.084∗ 0.008∗ 0.008∗ 0.007∗. 
H 0.101 0.105 0.103 0.012∗ 0.013∗ 0.012∗. 
DUC’04 HUMANS A 0.088∗ 0.092∗ 0.090∗ 0.009∗ 0.010∗ 0.010∗. 
OTS 0.069∗ 0.079∗ 0.074∗ 0.008∗ 0.009∗ 0.009∗. 
ItemSum 0.083∗ 0.085∗ 0.084∗ 0.012∗ 0.014∗ 0.014∗. 
GraphSum 0.093 0.099 0.097 0.015 0.021 0.019. 
GraphSum performs significantly better than ItemSum,  OTS,  TexLexAn for all of the tested measures. 
Furthermore,  it performs significantly better than all the DUC’04 competitors in terms of ROUGE2 and ROUGESU4 F measure. 
Although it does not exploit neither advanced linguistic processing steps nor semantics based analysis,  GraphSum performs as good as the top ranked DUC’04 summarizers(  i e,  CLASSY and Peer)  in terms of ROUGESU4 Recall. 
GraphSum and CLASSY are the only summarizers that,  in some cases,  perform better than the humans. 
More specifically,  both of them outperform the humans in terms of ROUGESU4 F measure. 
texLexAn 0.059∗ 0.068∗ 0.063∗ 0.006∗ 0.007∗ 0.007∗. 
GraphSum performs significantly better than 4 out of 8 humans in terms of ROUGE2 F measure,  whereas the best CLASSY summarizer s version(  peer)  outperforms only 2 out of 8 humans. 
peer 0.081∗ 0.085 0.083∗ 0.010∗ 0.011∗ 0.011∗. 
peer 0.077∗ 0.080∗ 0.078∗ 0.012∗ 0.012∗ 0.012∗. 
13. 
at 95% significance level for all of the evaluated measures. 
Table 3. 
DUC’04 Collections. 
Comparisons between GraphSum and the other approaches. 
Statistically relevant differences in the comparisons between GraphSum(  standard configuration)  and the other approaches are starred. 
Top ranked scores in terms of each ROUGE evaluator and measure are written in boldface separately for the machine driven machined riven and humanly generated summaries. 
peer 0.078∗ 0.082∗ 0.080∗ 0.011∗ 0.012∗ 0.011∗. 
Summarizer ROUGE2 ROUGESU4. 
TOP RANKED DUC’04 PEERS peer 0.089∗ 0.095∗ 0.092∗ 0.015 0.017∗ 0.016∗. 
peer 0.076∗ 0.103∗ 0.086∗ 0.015 0.018∗ 0.016∗. 
peer 0.087∗ 0.091∗ 0.089∗ 0.015 0.016∗ 0.015∗. 
peer 0.086∗ 0.093∗ 0.089∗ 0.013 0.014∗ 0.014∗. 
peer 0.071∗ 0.085∗ 0.077∗ 0.012∗ 0.014∗ 0.013∗. 
peer 0.070∗ 0.087∗ 0.077∗ 0.012∗ 0.015∗ 0.012∗. 
peer 0.075∗ 0.080∗ 0.078∗ 0.012∗ 0.013∗ 0.012∗. 
R Pr F R Pr F. 
Hence,  GraphSum appears to be,  on average,  more effective than CLASSY on the DUC’04 collections. 
This section summarizes the results that were achieved by GraphSum on the real life news document collections(  see Section 4.1). 
Specifically,  Section 4.3.1 reports a qualitative comparison between the generated summaries,  whereas Section 4.3.2 evaluates the performance of both GraphSum and its competitors in terms of ROUGE scores. 
In (Kulesza and Taskar 2012), the authors present following contributions
We compare the expressive power of DPPs and MRFs,  characterizing the tradeo s in terms of modeling power and computational e ciency. 
In the rst,  we subtract a multiple of one of the vectors in V from all of the other vectors so that they are zero in the ithcomponent,  leaving us with a set of vectors spanning the subspace of V orthogonal to ei. 
• An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;• A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;• A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
We assume that the conditional DPP kernel L(  X ; θ)  is parameterized in terms ofa generic θ,  and let denote the conditional probability of an output Y given input X under parameter θ. 
Our input will be a still image depicting multiple people,  and our goal is to simultaneously identify the poses the positions of the torsos,  heads,  and left and right arms of all the people in the image. 
Suppose that we draw a tiling uniformly at random gray in a checkerboard pattern)  A domino tiling is a perfect cover of the Aztec diamond from among all possible tilings. 
Each person in each image is annotated by hand each of the four parts(  head,  torso,  right arm,  and left arm)  is labeled with the pixel location of a reference point(  e g,  the shoulder)  and an orientation selected orientations elected from among 24 discretized angles. 
identifying multiple human poses in images,  where there arecombinatorially many possible poses,  and we assume that the poses are diverse in that they tend not to overlap identifying salient lines of research in a corpus of computer science publications,  where the structures are citation chains of important papers,  and we want to nd a small number of chains that covers the ma jor topic in the corpus and building threads from news text,  where the goal is to extract from a large corpus of articles the mostsigni cant news stories,  and for each story present a sequence of articles covering the ma jor developments of that story through time We begin by de ning SDPPs and stating the structural assumptions that are necessary to make inference e cient we then show how these assumptions give rise to polynomialtimealgorithms using second order message passing. 
Initially,  the surface is empty iteratively,  particles arrive and bind uniformly at random to a location from among all locations that are not within a given radius of any previously bound particle When no such locations remain(  the jamming limit) ,  the process is complete Like the Mat ern processes,  RSA is a hardcore model,  designed primarily to capture packing distributions,  with much of the theoretical analysis focused on the achievable density If the set of locations is further restricted at each step to those found in an initially selected Poisson set Y,  then it is equivalent to a Mat ern Type III process Huber and Wolpert,  it therefore shares the same computational burdens. 
The Gibbs point process provides such an approach,  o ering a general framework for incorporating correlations among selected items Preston,  1976,  Ripley and Kelly,  1977,  Ripley,  1991,  Van Lie shout,  2000,  M ll er and Waagepetersen,  where U is an energy function. 
However,  DPPs are essentially unique among this class in having e cient and exact algorithms for probabilistic inference,  which is why they are particularly appealing models for machine learning applications. 
For now,  however,  we simply observe that,  as for MRFswith negative correlations,  repulsive Markov point processes are computationally intractable Even computing the normalizing constant for Equation(  76)  is NPhard in the cases outlined above Daley and VereJones,  2003,  M ll er and Waagepetersen,  On the other hand,  quite a bit of attention has been paid to approximate inference algorithms for Markov point processes,  employing pseudo likelihood Be sag,  1977,  Besaget al,  1982,  Jensen and Moll er,  1991,  Ripley,  1991. 
All of the following results extend easily to multiple training examples. 
Note that when multiple samples are desired,  the eigendecomposition needs to be performed only once Deshpande and Rademacher. 
correlations are always non positive Figure 4 shows the di erence between sampling a set of points in the plane using aDPP(  with Kij inversely related to the distance between points i and j) ,  which leads toa relatively uniformly spread set with good coverage,  and sampling points independently,  which results in random clumping. 
This expression has some nice intuitive properties in terms of volumes,  and,  ignoring the normalization in the denominator,  takes a simple and concise form. 
To demonstrate that SDPPs e ectively model characteristics of real world data,  we apply them to a multiple person pose estimation task Kulesza and Task ar,  2010. 
We demonstrate structured DPPs on a toy geographical paths problem,  a still image multiple pose estimation task,  and two high dimensional text threading tasks. 
As we have already seen,  the partition function,  despite being a sum over N terms,  can be written in closed form as det(  L + I). 
As a result,  any valid marginal kernel has to be the sum of aconstant matrix and a multiple of the identity matrix. 
Furthermore,  at least Y | − k + 1 of the terms must be strictly less than one,  because Note that,  since the columns of B are normalized,  each term in the product is at most otherwise there would be k orthogonal columns,  which would correspond to a cover. 
Moreover,  we might want the likelihood in terms of the marginal kernelK = L(  L + I)  −1 = I −(  L + I)  −1,  but simply plugging in these identities yields a expression that is somewhat unwieldy As alternatives,  we will derive some additional formulas that,  depending on context,  may have useful advantages. 
Eigendecomposing the dual representation C,  for instance,  requires O(  D)  time,  while normalization,  marginalization,  and sampling,  even when an eigendecomposition has been precomputed,  scale quadratically in D,  both in terms of time and memory In practice,  this limits us to relatively low dimensional diversity features φ; for example,  in our pose estimation experiments we built φ from a fairly coarse grid of 32 points mainly for reasons of e ciency. 
However,  we showed in Section that DPP inference can be recast in terms of a smaller dual representation C ; recall that,  ifB is the D × N matrix whose columns are given by By i = q(  y i)  φ(  y i) ,  then L = B(  cid. 
Since this approach does not incorporate a notion of diversity(  or any correlations between selected poses whatsoever) ,  we expect that we will frequently see multiple poses that correspond to the same person The second baseline is a simple non maximum suppression model Canny,  1986. 
In (Alguliev et-al. 2013), the authors present following contributions
(  1)  content coverage,  summary should contain salient sentences that cover the main content of the documents(  2)  diversity,  summaries should not contain multiple sentences that convey(  carry)  the same information and(  3)  length,  summary should be bounded in length. 
Let T = t,  t,  …,  tm represents all the distinct terms occurred in the document collection D,  where m is the number of terms. 
The LexRank defines sentence salience based on graph based centrality scoring of sentences. 
However,  this strategy has some drawbacks and the main one is the requirement of multiple runs for the fine tuning of penalty factors,  which would increase the computational time and degrade the efficiency of the algorithm. 
It translates the clustering summarization problem into minimizing the Kullback–Leibler divergence between the given documents and model reconstructed terms. 
According to(  Das & Suganthan,  2011)  the average run time of a standard DE algorithm can be affected by several factors such as the number of terms,  the number of sentences,  the population size,  the fitness computation and the number of generations. 
Using VSM,  each sentence si is represented using these terms as a vector in mdimensional space,  si = wi,  …,  wim,  where each component reflects weight of a corresponding term. 
A good summary,  as whole,  is expected to be the one with extensive coverage of the focuses presented in documents,  minimum redundancy,  and smooth connection among sentences. 
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
The target vector Up(  t)  is compared with the trial vector Zp(  t) ,  in terms of the objective function value and the better one survives into the next generation. 
In order to distinguish among its schemes,  the notation “DE/a/b/c” is used,  where “DE” denotes the Differential Evolution a specifies the vector to be mutated(  which can be random or the best vector)  ; b is the number of difference vectors used and c denotes the crossover scheme,  binomial or exponential. 
According to Mani and May bury(  1999) ,  automatic text summarization takes a partially structured source text from multiple texts written about the same topic,  extracts information content from it,  and presents the most important content to the user in a manner sensitive to the user s needs. 
DE has gradually become more popular and has been successfully applied to solve many optimization problems due to its strong global search ability,  causing widespread concern among scholars. 
The graph based ranking algorithms such as PageRank(  Br in & Page,  1998)  and HITS(  Klein berg,  1999)  have also been used in generic multi document summarization. 
The major concerns in graph based summarization researches include how to model the documents using text graph and how to transform existing web page ranking algorithms to their variations that could accommodate various summarization requirements(  Wenjie,  Furu,  Qin,  & Yanxiang,  2008). 
Zhao,  Wu,  and Huang(  2009)  propose a query expansion algorithm used in the graph based ranking approach for query focused multi document summarization. 
Hence,  the population either gets better in terms of the fitness function or remains constant but never deteriorates. 
Among the other methods,  the worst result shows Random. 
The centroid based method is the algorithm used in the MEAD system. 
Table 2 shows the ROUGE1,  ROUGE2,  ROUGEL and ROUGESU values and rankings of each method over DUC2002 data set. 
The bold entries represent the best performing methods in terms of average ROUGE evaluation metrics. 
The number in parentheses in each table slot shows the ranking of each method on a specific data set. 
In (Baralis et-al. 2012), the authors present following contributions
However,  previous approaches typically focus on single word significance while do not effectively capture correlations among multiple words at the same time. 
proposed to represent correlations among sentences by means of a graph based model. 
Frequent item set items et mining is a well established data mining technique to discover correlations among data. 
(  i)  the usage of an itemsetbased model to represent the most relevant and not redundant correlations among document terms,  and(  ii)  the selection of the minimal set of representative sentences that best covers the itemsetbased model. 
From a transactional representation of the document collection,  an highly informative and not redundant itemsetbased model is extracted to represent significant higher order correlations among document terms. 
Among them,. 
They commonly evaluate sentences according to cluster based or graph based models. 
is a widely exploratory technique to discover hidden correlations that frequently occur in the source data. 
This paper presents a novel multi document summarizer,  namely ItemSum(  Itemsetbased Summarizer) ,  that is based on an itemsetbased model,  i e,  a model composed of frequent item sets,  extracted from the document collection. 
is a widely exploratory data mining technique that focuses on discovering correlations,  i e,  item sets,  that frequently occur in the source data. 
Among the large set of previously proposed approaches focused on succinctly representing transactional data by means of item sets. 
To better discriminate among single word occurrences within each document,  ItemSum combines the usage of the itemsetbased model with a sentence relevance score,  computed from the bagofword sentence representation and based on the well founded tfidf statistics. 
∨ SCl,  generates a binary vector with the maximum number of s. 
In (Otterbacher et-al. 2009), the authors present following contributions
An interesting interpretation of the LexRank value of a sentence can be understood in terms of the concept of a random walk. 
In(  Erk an and Radev,  2004) ,  the concept of graph based centrality was used to rank a set of sentences for producing generic multi document summaries To compute LexRank,  the documents are rst segmented into sentences,  and then a similarity graph is constructed where each node in the graph representsa sentence. 
However,  in our approach,  answers are extracted from a set of multiple documents rather than on a documentbydocument basis. 
In more general terms,  this task is known as passage retrieval in information retrieval. 
LexRank isa random walk based method that was proposed for generic summarization Our contribution in this paper is to derive a graph based sentence ranking method by incorporating the query information into the original LexRankalgorithm,  which is query independent. 
Our method does not rank the input documents,  nor is it restricted in terms of the number of sentences that may be selected from the same document. 
When using any metric to compare sentence sand sentences and a query,  there is always likely to be a tie between multiple sentences(  or,  similarly,  there may be cases where fewer than the number of desired sentence shave sentences have similarity scores above zero). 
As shown in Table 7,  all fourLexRank systems outperformed the baseline,  both in terms of average MRRand TRDR. 
In fact,  the average number of content words among these 22 questions was slightly,  but not signi cantly,  higher than the. 
In (Harabagiu S, Lacatusu F 2005), the authors present following contributions
In Section 2,  we considered five different baseline topic representations that use terms(  TR1) ,  relations(  TR2) ,  document segments(  TR3) ,  sentence clusters(  TR4) ,  or semantic frames generated from multiple different documents(  TR5). 
Even though TR3 featured discourse structure information that presumably could not be captured by extracting terms or relations from single sentences alone,  we found that it did not include the necessary lexicosemantic knowledge needed to draw correspondences between semantically similar lexical items or categories of lexical items. 
While it is not surprising that these topic representations outperformed the simpler TRs we considered(  TR1 and TR2) ,  the real value of these methods can be seen as a comparison with the other more articulated TRs such as TR3 and TR4,  which sought to combine multiple different dimensions of the content of a document collection into a single topic representation. 
This hypothesis is further supported by the discrepancies in performance between the graph based theme representations and their better performing counterparts,  the linked list based theme representations. 
Based on these results,  we believe there is particular merit in creating topic representations which model the ideal content and structure of multi document summaries in terms of the information available from a collection of documents. 
With graph based topic representation(  TH1) ,  discourse relations identified between discourse segments are used to reorganize the content of a document collection into a new type of topic representation,  which equally considers the organization of individual documents and the salience of extracted information. 
In contrast,  a linkedlistbased topic representation(  TH2)  considers theme based elements in terms of their observed connectivity to other theme elements recognized in a document collection. 
In (Hearst 1997), the authors present following contributions
Because groups of words that are not necessarily closely related conceptually seem to work together to indicate subtopic structure,  I adopt a technique that can take into account the occurrences of multiple simultaneous themes rather than use chains of lexical cohesion relations alone. 
This viewpoint is also advocated by Skorochod'  ko(  1972) ,  who suggests discovering a text'  s structure by dividing it up into sentences and seeing how much word overlap appears among the sentences. 
(  It is,  however,  the case that the interlinked terms of sentences 57 to 71,  space,  star,  binary,  trinary,  astronomer,  orbit,  are closely related semantically,  assuming the appropriate senses of the words). 
For example,  sentences 37 to 51 contain dense interactions among the terms move,  continent,  shoreline,  time,  species,  and life,  and all but the latter occur only in this region. 
Their study explores the usefulness of multiple windows for organizing the contents of long texts,  hypothesizing that providing readers with spatial cues about the location of portions of previously read texts will aid in their recall of the information and their ability to quickly locate information that has already been read once. 
First,  no thesaurus classes are used(  only term repetition of morphological variants of the same word)  ; second,  multiple chains are allowed to span an intention and third,  chains at all levels of intentions are analyzed simultaneously. 
In (Hong et-al. 2015), the authors present following contributions
Experiments show that our model performs better than the systems that are combined,  which is comparable to the stateoftheart on multiple benchmarks. 
Furthermore,  we exclude the sentences that appear in multiple basic summaries from D,  then compute the three features above for the new D. 
We propose a supervised model to select among the candidate summaries. 
System identity features might be more helpful in selecting among the sentences that are generated by only one of the systems. 
Overall,  our combination model achieves very competitive performance,  comparable to the stateoftheart on multiple benchmarks. 
In (Alguliev et-al. 2011), the authors present following contributions
The paper(  He,  Shao,  Li,  Yang,  & Ma,  2008)  presents an approach based on estimation of content terms. 
In the process of estimating content terms,  it makes full use of the relevant feature and the information richness feature for assigning importance to each of them. 
The graph based methods first construct a graph representing the sentence relationships at different granularities and then evaluate the topic biased saliency of the sentences based on the graph. 
In recent years,  a variety of graph based methods have been proposed for multi document summarization(  Erk an and Radev,  2004,  Otterbacher et al,  2009,  Radev et al,  2001,  Wan and Xiao,  2009,  Wan et al,  2007,  Wei et al,  2008,  Zhang et al,  2008,  Zhang et al,  2008,  Zhao et al,  2009). 
This function also guarantees that in the summary will not be multiple textual units that convey the same information,  i e. 
In (Wang and Li 2012), the authors present following contributions
In general,  the terms of consensus methods or ensemble methods are commonly reserved for the aggregation of a number of different(  input)  systems. 
Previous research has shown that ensemble methods,  by combining multiple input systems,  are a popular way to overcome instability and increase performance in many machine learning tasks,  such as classification,  clustering and ranking. 
As a good ensemble requires the diversity of the individual members,  in this paper,  we first study the most widely used multi document summarization systems based on a variety of strategies(  e g,  the centroid based method,  the graph based method,  LSA,  and NMF) ,  and evaluate different baseline combination methods(  e g,  average score,  average rank,  Borda count,  median aggregation,  round robin scheme,  correlation based weighting method,  and graph based combination)  for obtaining a consensus summarizer to improve the summarization performance. 
conducts latent semantic analysis on terms by sentences matrix as proposed in Gong and Liu(  2001). 
a graph based summarization method recommending sentences by the voting of their neighbors(  Erk an & Radev,  2004). 
In (Riedhammer et-al. 2010), the authors present following contributions
For future work on larger data,  modifying the weighting is definitely of interest Recent work on unsupervised key phrase extraction integrates TFxIDF and graph based models(  Liu et al,  2009)  However,  the focus of our work is to compare sentence andconceptbased summarization. 
For mmr ilpand mcd ilp,  we reduced the number of candidate utterances to the top 50 in terms of the sum of the keyphraseweights,  in order to obtain a more feasible problem. 
For each input sentence,  a set of features is extracted and fed to a classi er in order to predict binary relevance labels as annotated in the int he AMI and ICSI data For this work,  we consider the following features fo reach for each utterance which are extracted from the manual transcriptions and annotations Duration of the utterance in seconds Position of the utterance in terms of the start time relative to the meeting duration Speaker dominance in terms of how much the speaker spoke speakers poke compared to the others Speaker role,  e g,  professor(  ICSI data)  or product manager(  AMI data)  . Word ngrams. 
In (Sipos et-al. 2012), the authors present following contributions
A summary is produced by maximizing an objective function that includes coverage and redundancy terms. 
This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intradocument sentence similarities. 
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Baralis et-al. 2013), the authors present following contributions
A parallel effort has been devoted to using clustering algorithms for summarizing documents. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Yang et-al. 2014), the authors present following contributions
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters. 
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information. 
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010), the authors present following contributions
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
Text summarization based on fuzzy logic system architecture. 
Automatic text summarization based on fuzzy logic. 
It is an extraction based multi document summarization system. 
Then,  it enters all the rules needed for summarization,  in the knowledge base of system. 
This approach is less expensive and more robust than a summarization technique based entirely on a single method. 
In query based text summarization. 
All features are computed over primitives,  syntactic,  linguistic,  or knowledge based information units extracted from the sentences. 
is a novel multi document text summarization approach,  which aims to build systems for relation identification and characterization that can be transferred across domains and tasks without modification of model parameters. 
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
These features are covering statistical and linguistic characteristics of a language. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
Query based extractive text summarization. 
In query specific opinion summarization system. 
Extractive summaries are formulated by extracting key text segments(  sentences or passages)  from the text,  based on statistical analysis of individual or mixed surface level features such as word phrase frequency,  location or cue words to locate the sentences to be extracted. 
is a multi document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. 
integrates multilingual summarization and multi document summarization capabilities using a multi engine,  core summarization system and provides fast,  interactive document access through hypertext summaries. 
Multi document extractive summarization deals with extraction of summarized information from multiple texts written about the same topic. 
Bayesian summarization. 
It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. 
Support for new languages is added to the system by developing modules conforming to interfaces for text preprocessing and primitive extraction for the language,  and using existing dictionary based translation methods,  or adding other language specific translation methods. 
Thirdly,  It has used the Markov Model to order the subtopics that the final summarization should contain and output the text summarization according to the sentence ranking score of all sentences within one subtopic as user'  requirement. 
w is linear statistical model of system(  the weights w,  w w in the equation)  m is total number of sentences in the training corpus. 
After ward,  a value from zero to one is obtained for each sentence in the output based on sentence characteristics and the available rules in the knowledge base. 
This approach uses conceptual vector space model to form a rough summarization,  and then calculate degree of semantic similarity of sentence for reducing its redundancy. 
The core engine is being designed in such a way that as additional resources,  such as lexical and other knowledge bases or text processing and MT engines,  become available from other ongoing research efforts they can be incorporated into the overall multi engine MINDS system. 
A good summary system should reflect the diverse topics of the document while keeping redundancy to a minimum. 
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form. 
system,  the sentences in a given document are scored based on the frequency counts of terms(  words or phrases). 
Automatic text summarization system. 
Summarization tools may also search for headings and other markers of subtopics in order to identify the key points of a document. 
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form. 
This paper focuses on extractive text summarization methods. 
The assumption behind this cue based Hub/Authority approach is that a good Hub word(  or phrase)  is the content that points to many good authorities sentences and a good authority sentence is a vertex that is pointed to by many good hub words. 
a question analysis and query reformulation module,  a latent semantic indexing based sentence scoring module,  a sentence polarity detection module,  and a redundancy removal module. 
With different opinions being put together & outlined,  every topic is described from multiple perspectives within a single document. 
A good summary system should extract the diverse topics of the document while keeping redundancy to a minimum. 
This method considers each characteristic of a text such as sentence length,  similarity to little,  similarity to key word and etc   as the input of fuzzy system. 
Core summarization problem of MINDS is taking a single text and producing a shorter text in the same language that contains all the main points in the input text. 
In (Harabagiu S, Lacatusu F 2005), the authors present following contributions
In our work,  we have considered both, (  1)  component based evaluations,  which evaluated each phase in the creation of a multi document summary separately,  and(  2)  intrinsic evaluations,  which evaluate the quality of each individual multi document summary generated by a summarization system. 
Now more than ever,  consumers need access to robust multi document summarization(  MDS)  systems,  which can effectively condense information found in several documents into a short,  readable synopsis,  or summary. 
In this article,  we perform what we believe to be the first comprehensive evaluation of the impact that different topic representation techniques have on the performance of a multi document summarization system. 
We separately evaluated the quality of compression when different topic representations were available to the summarization system. 
Our experiments evaluate the performance of a total of 40 multi document summarization systems. 
In Section 2,  we considered five different baseline topic representations that use terms(  TR1) ,  relations(  TR2) ,  document segments(  TR3) ,  sentence clusters(  TR4) ,  or semantic frames generated from multiple different documents(  TR5). 
We expect that theme based representations can be used to organize topic relevant information from multiple sources,  extracted from either, (  1)  a single sentence, (  2)  a cluster of sentences, (  3)  a discourse fragment,  or even(  4)  a cluster of documents. 
First,  we sought to quantify the impact that topic information has on each phase of a multi document summarization(  MDS)  system. 
Second,  we believe that our results represent a comprehensive and replicable study,  which demonstrates the effectiveness of a structured,  theme based approach to multi document summarization. 
We believe that these results prove the benefits of using the theme based topic representation for multi document summarization. 
In the rest of this section,  we describe how we recognized both types of relations for use in a summarization system. 
The second phase of MDS,  sentence compression,  is either based on linguistically motivated heuristics,  like those reported in Zajic et al. 
While the first method(  CM0)  uses no compression,  the second and third methods are based on, (  1)  an empirical method based on linguistic heuristics(  CM1) ,  and(  2)  a method based on the theme representation(  CM2). 
Work in multi document summarization(  MDS)  has long used automatically generated topic representations(  TR)  in order to approximate the information content of a collection of documents. 
Traditionally,  MDS systems have created informative summaries by selecting only the most relevant information for inclusion in a summary. 
Recent work in multi document summarization has leveraged information about the topics mentioned in a collection of documents in order to generate informative and coherent textual summaries. 
The problem of using topic representations for multi document summarization(  MDS)  has received considerable attention recently. 
While many of the top performing systems at the past DUC MDS evaluations have used TR to great effect,  relatively little work has sought to identify the properties of an ideal TR for a generic MDS task. 
In this article,  we describe five previously known topic representations and introduce two novel representations of topics based on topic themes. 
Automatic text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in recent years. 
In (Khan et-al. 2015), the authors present following contributions
exploits template based method to produce abs tractive summary from multiple news wire newspaper documents depending on the output of the information extraction(  IE)  system. 
A multi document summarization system,  GISTEXTER,  presented in. 
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015), the authors present following contributions
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 
With the construction of feature groups,  a latent variable is utilized to control the selection of them. 
Many extraction based summarization methods have been proposed in the past years. 
Experiment results on DUC2003 and DUC2004 data sets for text summarization and NUSWide data set for image summarization demonstrate that the introduction of group selection can indeed improve summary performance. 
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
abstract based and extraction based. 
Therefore,  in this paper,  we attempt to devise an approach to the generation of the topic aspect oriented summarization according to topic factors,  we call this TAOS(  Topic AspectOriented Summarization). 
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 
Of particular interest to us in this paper is the extractive multi document summarization,  where the obtained final summary is a subset of the elements from multiple input documents. 
Documents summarization can be generally categorized as two approaches. 
While stop words are removed,  our TAOS outperforms in the vast majority of metrics of ROUGE1,  and we think the reason is that the summaries generated by TAOS can convey more meaningful aspects than summaries by SubSVM due to the nature of group selection in TAOS. 
This means that TAOS is better than SubSVM in terms of LCS,  thus,  TAOS enables to extract long representative sentences for summarization. 
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization(  abbreviated as TAOS). 
A natural way to generate topic aspect oriented summarization is to assume that topic aspects can prefer some feature groups,  and the group sparsity can be introduced. 
BagofVisualWord is a image representation based on BoW(  BagofWord)  model. 
are widely used for the evaluation of text summarization performance. 
For image summarization task,  we also extract three kinds of features for each image. 
The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining. 
We compare our proposed approach with some stateoftheart methods on DUC2003,  DUC2004 data sets for text summarization and NUSWide data set for image summarization. 
The results show our method can generate meaningful summarization in terms of ROUGE and Jensen–Shannon Divergence metrics. 
For different topics set,  different feature groups are selected. 
In order to materialize the intuition of TAOS,  we first extract several groups of features according to topic factors,  and then a group norm penalty(  i e,  norm)  and latent variables are utilized to select overlapping groups of features. 
However,  the combinations of feature groups for each topic probably are different,  we therefore explore the structural SVM with latent variables. 
Large margin Group normSubmodular function Text image summarization. 
Denote the set of latent variables as to capture the combination of feature groups in different topics. 
The summarization of documents is the task of automatically extracting the latent gist of the given documents to indicate the main aspects in them. 
With the exponential growth of the text and image documents over Internet,  the appropriate summarization of data has become more and more attractive for the organization of large scale data. 
The proportion of stop words in original article is 38.5% and 38.7% on DUC2003 and DUC2004 data set,  respectively. 
In order to confirm the intuition that gold summaries prefer less stop words than the original articles,  we calculate the proportion of stop words in summaries and original articles respectively on both DUC2003 and DUC2004 data sets. 
However,  the performance improvement due to the overlapping stop words is nonsense,  because intuitively,  the stop words convey trivial information. 
,  Kmedoids. 
While the proportion in manually created summaries is 33.5% and 34.3%,  respectively. 
ROUGE1-P 0.31675 0.39459 0.38572 0.36286. 
The constraint of text summary length is set to 665 bytes for all the methods in our experiment follow the DUC default setting. 
The,  the similarity of two sentences,  is the common words in sentence i and j. 
In our TAOS,  we tune the trade off parameter C and the initial value of weight vector,  and finally the C is set to 1.0 and each component of is initialized with 60 to obtain the results we report in this paper. 
and DL(  Dictionary Learning). 
22. 
According to the ROUGE formula described above,  which calculate the overlapping grammar units,  we can know that the overlapping stop words between one predicted summary and the ground truth summary can improve the performance of ROUGE score. 
The comparisons of average performance on DUC2003 data set in terms of ROUGE metrics with stop words. 
Table 2. 
According to the analysis above,  we give out two kinds of experimental comparisons. 
the comparison with stop words and the comparison with stop words removed. 
Specifically,  when taking stop words into account,  SubSVM has a better performance on ROUGE1. 
While without stop words,  our TAOS outperforms in most metrics. 
According to this comparison,  we demonstrate that the summaries generated by TAOS has more meaningful information than summaries generated by SubSVM because removing meaning less words(  stop words)  leads to more serious performance degradation on ROUGE1 metrics for SubSVM. 
From the results in Table 2,  Table 3,  Table 4,  Table 5,  we can make the following observations. 
Hence,  we choose some unsupervised methods to compared with,  including AP(  Affinity Propagation). 
ROUGE1-R 0.29485 0.34975 0.39339 0.40146. 
Specifically,  when taking stop words into account,  BudSub and SubSVM have better performance in some metrics of ROUGE1. 
DSDR BudSub SubSVM TAOS. 
Metrics Methods. 
The best performance is shown in bold. 
• TAOS always has a higher ROUGEL score. 
This result shows that the stop words carry few meaningful information because of the lower degree of significance in gold summaries. 
• Generally,  our TAOS outperforms the other methods in most metrics of ROUGE1 on DUC2003 and DUC2004 data sets. 
Several stateoftheart methods including supervised and unsupervised are compared with the proposed TAOS for text and image summarization tasks respectively. 
SubSVM conducts structural SVM and sub modular function to perform multi document summarization and is shown to outperform other supervised methods. 
ROUGEL uses the longest common subsequence(  LCS)  metric and also includes ROUGEL-R/P/F. 
ROUGEN is an ngram metric and includes ROUGEN-R,  ROUGEN-P and ROUGEN-F for recall,  precision and F metric respectively. 
Usually,  ROUGE metrics compares how many of overlapping units(  e g,  ngram words or sentences)  between one predicted summary and one human labeled summary. 
18. 
ROUGE metrics. 
For example,  if the ith feature group is selected,  its corresponding latent variable hi is set to 1,  otherwise 0. 
Variance 200.905 57.298 136.758 55.917. 
length(  words)  23.832 15.809 22.003 16.270. 
Avg. 
Articles Summaries Articles Summaries. 
Data set DUC2003 DUC2004. 
The average sentence length and length variance on DUC2003 and DUC2004 data sets. 
Table 1. 
While the semantic information of the low level image features is not well understood yet,  we simply define each kind of feature as an individual feature group. 
Thus,  we totally define 13 feature groups for the text summarization task. 
In (Aliguliyev 2009), the authors present following contributions
In our study we focus on sentence based extractive document summarization. 
The proposed approach is a continue sentence clustering based extractive summarization methods,  proposed in Alguliev Alguliev,  R. 
We propose the generic document summarization method which is based on sentence clustering. 
A novel partitioning based clustering method and generic document summarization. 
Summarization of text based documents with a determination of latent topical sections and information rich sentences. 
The proposed sentence clustering based approach for generic single document summarization is presented in Section 3. 
Systems for extractive summarization are typically based on technique for sentence extraction,  and attempt to identify the set of sentences that are most important for the overall understanding of a given document. 
Sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive summaries. 
The centroid based method(  Erk an and Radev,  2004,  Radev et al,  2004)  is one of the most popular extractive summarization methods. 
www summarization com mead)  is an implementation of the centroid based method for either single or multi document summarizing. 
In paper Wan,  Yang,  and Xiao(  2007)  proposed a novel extractive approach based on manifold ranking of sentences to query based multi document summarization. 
On the other hand,  summarization task can also be categorized as either generic or query based. 
In paper Guo and Styli os(  2005)  is introduced an intelligent system,  the event indexing and summarization(  EIS)  system,  for automatic document summarization,  which is based on a cognitive psychology model(  the event indexing model)  and the roles and importance of sentences and their syntax in document understanding. 
In (Alguliev et-al. 2013), the authors present following contributions
This paper proposes an optimization based model for generic document summarization. 
Binwahlan,  Salim,  and Suanmali(  2010)  introduced a different hybrid model based on fuzzy logic,  swarm intelligence and diversity selection for text summarization problem. 
The experimental results provide strong evidence that the proposed optimization based approach is a viable method for document summarization. 
Zhao,  Wu,  and Huang(  2009)  propose a query expansion algorithm used in the graph based ranking approach for query focused multi document summarization. 
Other methods include NMFbased topic specification(  Lee et al,  2009,  Wang et al,  2008,  Wang et al,  2009)  and CRFbased summarization(  Sh en et al,  2007). 
The graph based ranking algorithms such as PageRank(  Br in & Page,  1998)  and HITS(  Klein berg,  1999)  have also been used in generic multi document summarization. 
The major concerns in graph based summarization researches include how to model the documents using text graph and how to transform existing web page ranking algorithms to their variations that could accommodate various summarization requirements(  Wenjie,  Furu,  Qin,  & Yanxiang,  2008). 
The centroid based method,  MEAD,  is one of the popular extractive summarization methods(  Radev,  Jing,  Stys,  & Tam,  2004). 
Up to now,  various extraction based techniques have been proposed for generic multi document summarization. 
A novel multi document summarization model based on the budgeted median problem proposed in Takamura and Okumura(  b). 
In (CR132), the authors present following contributions
The extrinsic evaluation,  also called task based evaluation,  has received more attention recently at the DARPASummarization Evaluation Conference Mani et al,  1998. 
,  uses modified TF*IDF to produce clusters of news articles on the same event We developed a new technique for multidocumentsummarization(  or MDS) ,  called centroidbasedsummarization(  CBS)  which uses as input the centroids of the clusters produced by CIDR to identify which sentences are central to the topic of the oft he cluster,  rather than the individual articles. 
Finally,  we describe two user studies that test our models of multidocumentsummarization. 
We also describe two new techniques,  based on sentence utility and subsumption,  which we have applied to the evaluation thee valuation of both single and multiple document summaries. 
The authors mention that their preliminary results indicate that multiple document son documents on the same topic also contain redundancy but they fall short of using MMR for multidocumentsummarization. 
Other researchers have also suggested improvement son improvements on the precision and recall measure for summarization. 
In (Sipos et-al. 2012), the authors present following contributions
The learning method applies to all sub modular summarization methods,  and we demonstrate its effectiveness for both pairwise as well as coverage based scoring functions on multiple data sets. 
A popular stochastic graph based summarization method is LexRank(  Erk an and Radev,  2004). 
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015), the authors present following contributions
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 
In this paper,  we propose FoDoSu which is a novel multi document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. 
To overcome these drawbacks,  we propose a novel multi document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. 
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
Folksonomy systems are classification systems derived from social tags assigned to multimedia content by everyday users. 
Given multiple documents that need to be summarized,  we first perform a preprocessing step so that the documents can be analyzed at different granularities(  i e,  word level and sentence level). 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
The computed semantic importance score and contribution are used for rating each sentence in multiple documents. 
To summarize multiple documents,  these sentences are sorted by their scores to select those that contribute the most(  line 16). 
Finally,  our system generated a final summary of the given documents based on the top n scored sentences in the Score Table. 
We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm. 
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document. 
Web 2.0 has led to the development of large Web based communities that support and facilitate collaboration among Internet users(  Huang,  Lin,  & Chan,  2012). 
Folksonomies became popular on Web 2.0 as part of social tagging applications,  such as social bookmarking and image annotations uploaded by users. 
In particular,  in the Web 2.0 environment,  users can obtain general and common information from a Folksonomy system,  such as Wikipedia,  Flickr,  or del ici ous. 
However,  these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an outofdate dictionary or thesaurus. 
However,  identification was possible using tag clusters in the Folksonomy system,  which leverages the collective intelligence of the users in analyzing the semantics of the words in the document. 
Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
We have shown that it is useful in efficiently analyzing the semantics of proper nouns and newly coined words. 
Traditionally,  research on extractive summarization is based on the position of a sentence in a document which measures the overall frequency of the words they contain such as the TFIDF technique(  Luhn,  1958; Edmund son,  1969; Bran dow Brand ow,  Mitze,  & Rau,  1995). 
Finally,  we demonstrated that analysis using tag clusters from the Folksonomy greatly affected document summarization. 
However,  this technique only measures word frequency in the documents,  and does not consider the semantics of the relationships between the words and sentences(  Cha li,  Has an,  & Joty,  2011). 
Figure   2 shows the framework of our multi document summarization system FoDoSu. 
Therefore,  the semantic analysis technique using WordNet created difficulty in analyzing the semantics of proper nouns and newly coined words. 
Some websites include tag clouds as a way to visualize tags in a Folksonomy. 
The rapid growth of the Internet and smart multimedia devices such as smart phones and tablet PCs allow users to find information easily through diverse media(  e g,  documents,  images,  videos,  and music). 
Although the abstraction based method can summarize a document more accurately than the extraction based method,  it is much more difficult and complex because it requires the use of high cost natural language processing technologies,  such as information fusion(  Barzilay,  McKeown,  & Elhadad,  1999) ,  sentence compression(  Knight & Mar cu,  2002) ,  and reformulation(  McKeown,  Klavans,  Hatzivassiloglou,  Barzilay,  & Es kin,  1999). 
Hennig and Labor(  2009)  proposed a multi document summarization method based on Probabilistic Latent Semantic Analysis(  PLSA) ,  which represents sentences and queries as probability distributions over latent topics. 
Dang and Luo(  2008)  proposed a method to detect key sentences using keyword extraction based on statistics and syn sets. 
Also,  the extracted sentences are given a weighted score based on the importance and contributions of the words in each sentence. 
In contrast,  the existing words in WordNet consisted of gerunds and participles,  parts of speech that are not semantically important. 
Ar ora and Ravindran(  2008)  used Latent Dirichlet Allocation(  LDA)  to capture the topics of the given documents and form a summary with sentences representing these topics. 
However,  it is not easy to analyze the relationship between the semantics of the words in Web documents because there are many proper nouns and newly coined words in the original documents which are not defined in WordNet. 
The extractive summarization approach involves assigning saliency scores to some units(  e g,  sentences,  paragraphs)  of the documents and extracting those units with the highest scores. 
The preprocessing module extracts sentences from the input documents,  and then performs tokenization. 
Most existing multi document summarization techniques analyze semantic relationships between words in a document by exploiting probability theory,  machine learning techniques,  and external knowledge bases such as WordNet.1 However,  these techniques suffer from high computational costs in the learning and summarization processes. 
Likewise,  in Figure   3,  FoDoSu does not gather the tag cluster France for analyzing semantics because the words in the tag cluster France do not exist in the original WFT. 
The first category has highly semantically related words such as airport,  plane,  a,  and so on. 
The remaining words were difficult to analyze semantically using WordNet and the tag clusters,  including number and symbol. 
In this section,  we present the results of our evaluation of multi document summarization. 
Zhu et al  (  2009)  proposed a tag oriented Web document summarization approach using both the document itself and the tags annotated on the document. 
First,  when analyzing the semantics of words in the document,  FoDoSu performed with low computational cost. 
Furthermore,  the results show that the proposed system using the HITS algorithm is more effective than other the HITS algorithm based systems for document summarization. 
DocHITS considers the documents and sentences as hubs and authorities in the HITS algorithm. 
For this reason,  we use the Flickr tag cluster instead of WordNet when analyzing the semantics of proper nouns and newly coined words. 
A Folksonomy is a system of classification derived from the practice of collaboratively creating and managing tags annotated by users. 
Therefore,  in addition to the frequency,  we exploit the tag cluster,  a Folksonomy system that can be obtained from Flickr,  to analyze the semantics of the words. 
The first category has highly semantically related words such as airport,  plane and a. 
Then,  we exploit the HITS(  Klein berg,  1999)  algorithm for analyzing the semantics and contributions of words in documents. 
First,  when analyzing the semantics of words in the document,  FoDoSu performed with low computational cost. 
To address this problem,  various document summarization techniques have been studied to efficiently summarize the core of a single original document. 
Although FoDoSu seems to gather tag clusters with different meanings for a given word,  these tags are filtered out before analyzing the semantics of the words if they do not exist in the original WFT. 
However,  when analyzing the semantics of the words,  there are many proper nouns and newly coined words in the documents such as the names of people and products. 
As a result,  the number of documents(  news,  blogs,  Web pages,  emails,  etc)  created on the Web has been rapidly increasing day by day. 
Also,  the tag cluster a has one category that includes many semantically related words such as airbus,  aircraft,  plane,  and aviation. 
However,  the tag clusters for France have four different categories with a variety of meanings,  and they have no words semantically related to airbus or a. 
In this paper,  we consider the word in the document as a hub and the tag clusters as authorities to exploit the HITS algorithm for analyzing the semantics of the words. 
Both data sets are open benchmark data sets from the Text Analysis Conference(  TAC)  for automatic summarization evaluation. 
In future work,  we will explore better methods for analyzing the semantics of words that are difficult to analyze,  such as proper nouns and newly coined words. 
For example,  the tag cluster of the word airbus contains social tags such as airport,  plane,  and a. 
FoDoSu detects the sentences that consist of highly semantically related words by calculating the rel gram score,  and then analyzes the meaningful words in the documents using the HITS algorithm with tag clusters. 
In this environment,  to find the necessary information,  users must manually review all of the searched documents without any assistance from search engines,  which requires too much time and effort. 
Over the past few years,  multi document summarization has received more attention and made much progress. 
After analyzing the importance and contribution of each word,  we calculate the sentence score and rank each sentence with WordCluster. 
However,  various evaluations indicate that multi document summarization is highly complex and demanding,  and there is still much work to be done before automatic summarizers catch up with human beings(  Dang & Owczarzak,  2008). 
This approach limits the semantic analyses between words in the document because it only analyzes the relationships between users and tags in the Folksonomy system. 
The proposed system exploits the tag clusters used in Folksonomy to analyze the semantics of the words with low computational cost. 
The most important and distinctive feature of the Folksonomy system is the creation of content by general users without restriction. 
Using tag clusters,  we analyze the importance of each word and the semantic relatedness among them,  and finally make a summary of the documents. 
The proposed multi document summarization system is presented in Section 3. 
In Section 2.1,  we first review the related work on various multi document summarization techniques. 
We had computed the statistical significance of the difference in ROUGE scores for the best performing FoDoSu against the other systems on both TAC 2008 and TAC 2009 data sets and found that the differences are statistically significant with P < 0.05(  using TTest). 
The results demonstrate the following advantages. 
Therefore,  it is necessary to demonstrate how the tag cluster affects document summarization. 
Figs. 
Second,  FoDoSu analyzed proper nouns and newly coined words,  such as the names of people and products. 
In the third case,  the HITS algorithm was not used(  WithoutHITS) ,  and in the final case,  the HITS algorithm(  WithHITS)  was used. 
Using TAC 2009,  our proposed system showed good performance when k = 4 and the number of words was 10. 
The words found only in the tag clusters were most proper nouns and newly coined words,  so these words could not be used to analyze the meaning of a word using WordNet. 
That is,  to investigate how the HITS algorithm helps the summarization performance of FoDoSu,  we have investigated the following three cases. 
On the other hand,  just 4% of all words were included in WordNet. 
To analyze the words,  we selected the top 20 words in the WFT0 of each Flickr s tag clusters. 
As shown in Figure   17,  87% of the words in the documents that had their semantics analyzed existed in both WordNet and Flickr s tag clusters. 
Analysis of the hub of word wi is quite similar to the analysis of the authority of wi. 
The majority of these words were common nouns that could be used to easily analyze the meaning of the words. 
A total of 6% of all the words were included in Flickr s tag clusters. 
In the first case,  we considered only the frequencies of words in the documents(  OnlyFREQ). 
In the second case,  we considered only TFIDF of words in the documents(  TFIDF). 
Figure   17 shows the ratio of words used to analyze semantics in TAC 2008 and TAC 2009 using WordNet and tag clusters from Flickr. 
The performance of WithHITS on TAC 2008 and TAC 2009 was improved by 8.9–11.5% compared to OnlyFREQ,  and by 2.7– 4.1% compared to WithoutHITS. 
OnlyFREQ shows the results when we considered only the word frequency(  e g,  a = 1,  b = c = 0 in Eq. 
As seen in Figure   15,  WithHITS shows the best performance in all cases. 
This results show that TFIDF is more effective than the word frequency when extracting the important words from documents. 
On the other hand,  the result of TFIDF can always perform better than that of OnlyFREQ on both data sets. 
13 and 14 demonstrate the effect of changing the number of rel grams on summarization. 
We also found similar results using ROUGESU4 in Figure   16. 
As expected,  this means that it is not sufficient to consider only the frequencies of words when summarizing documents. 
15 and 16,  OnlyFREQ show the worst performance in all cases. 
That is,  WithHITS performs best in all cases. 
The performance of WithHITS on TAC 2008 and TAC 2009 was improved by 7.1–9.5% compared to OnlyFREQ,  and by 2.1–5.2% compared to WithoutHITS. 
From these experiments,  we confirmed that FoDoSu effectively selects the sentences containing significant words by exploiting the HITS algorithm with tag clusters when summarizing documents. 
As depicted in Figs. 
Tables 3 and 4 show the results when comparing our proposed system FoDoSu,  which uses TFIDF and HITS algorithm together,  with the related multi document summarization techniques on TAC 2008 and TAC 2009. 
(  7) )  and WithHITS shows the results when we considered all the parameters(  TFIDF of words,  HITS scores,  and rel gram scores). 
On the other hand,  WithoutHITS shows the results when we consider TFIDF of the words and rel gram scores(  e g,  a = c = 1,  b = 0 in Eq. 
(  7) ). 
In these experiments,  we had compared our system with the methods called DocHITS and ClusterHITS as baseline systems that used the HITS algorithm for summarizing documents. 
TFIDF shows the results when we considered TFIDF of the words instead of the word frequency(  e g,  a = 1,  b = c = 0 in Eq. 
On the other hand,  ClusterHITS first detects theme clusters in the document set using popular clustering algorithms and then considers these theme clusters and sentences as hubs and authorities,  respectively,  in the HITS algorithm. 
We had also compared our system with the systems NIST,  ceaList,  LPN1,  Veness Team which results were provided by TAC 2008 and TAC 2009. 
This is due to the fact that,  in case of WithHITS,  the analysis of important words using the HITS algorithm with tag clusters was performed when summarizing documents. 
As shown in Tables 3 and 4,  our proposed system FoDoSu always performs better than the other systems on both data sets. 
15 and 16 show the Fmeasure of ROUGE2 and ROUGESU4 on data sets TAC 2008 and TAC 2009. 
(  8) ). 
Then,  the hub score h(  wi)  increases by 1,  otherwise it is zero. 
(  1) ,  wi =(  w,  w,  ...,  wn)  is the ith word in the documents,  and ci =(  c,  c,  ...,  cn)  is the frequency of the ith word. 
The final word frequency table WFT’ is represented as follows. 
WFT0 ¼ f w c wc w c wc ... 
wn cn wcn gT ; ð2Þ where wci is the WordCluster of wi. 
WFT0 is used to analyze the contribution of each word in the documents in Section 3.3. 
To calculate the contribution of each word in the WFT’,  we apply the Hypertext Induced Topic Search(  HITS)  algorithm to our system. 
Originally,  the HITS was a type of link analysis algorithm to rate Web pages. 
The HITS algorithm classifies each Web page as a hub or authority. 
A good hub is a page that points to many good authorities,  and a good authority is a page that is linked to by many good hubs. 
To analyze the contribution of each word,  we consider the WordCluster as an authority,  and the words in the WFT0 as a hub. 
Figure   4 shows the relationship between each WordCluster and each word in the WFT0 as an authority and a hub in the HITS algorithm. 
In an authority,  the WordCluster of a particular word includes words in the WFT0 which have a high semantic relationship to it. 
A hub contains a particular word included in the WordCluster of the other word in the WFT0. 
The HITS calculates a contribution score for every word in the WFT0. 
Eq. 
(  3)  shows the formula for calculating the HITS of word wi,. 
Eq. 
(  4)  shows an authority measurement of the word wi. 
If word wj in the WFT0 exists in the WordCluster of word wi,  the authority of a(  wi)  increases by 1,  otherwise it is zero. 
Then the number of words in the WFT0 that have words in the WordCluster of word wi is summed. 
The authority of word wi has a high score,  indicating that the WordCluster of the word wi is an important word in a given document because there are many words that have a high semantic relationship with it. 
In contrast,  Eq. 
The number of words in the WFT0 having words wj in the WFT0 is summed. 
(  5)  shows a hub measuring the word wi for the case that the WordCluster of wj consists of word wi in the WFT0. 
The updated word frequency table(  WFT0)  in Figure   3 shows the final table generated by this procedure,  in which the count and WordCluster information is updated. 
First,  we construct a WFT to calculate the frequency of each word in the documents,  which can be represented as WFT In Eq. 
These words are filtered out and not used because they do not exist in the original WFT. 
For example,  in Table 1,  the word airbus has two tag categories. 
After construction of the WFT,  it is sorted by the frequency c. 
However,  it is hard to be certain that a word with high frequency is important in the documents. 
Flickr,  a popular collaborative tagging application for pictures,  provides tag clusters which are groups consisting of a tag and its related tags. 
To achieve this,  the Flickr system statistically measures which tags coincide with other tag words when a user tags media. 
Although these tag clusters are continuously updated and refined,  there is still a gap between the tags and the actual content of the images(  Liu,  Hua,  Wang,  & Zhang,  2010). 
For example,  when exploring Flickr tag clusters for the word apple,  four different categories are returned. 
laptop,  fruit,  smartphone,  and NYC(  Barbuto,  Contaldi,  & Senatore,  2012). 
It is hard to analyze the semantics of these words using WordNet because it does not cover proper nouns and newly coined words. 
Table 1 shows an example of a tag cluster from Flickr. 
A tag cluster consists of words highly related to a given tag. 
This has the benefit of providing useful information for the semantic analysis of words. 
As shown in Table 1,  the tag airbus has two categories. 
On the other hand,  the second category has less related words such as heath row and London. 
As this example shows,  we can identify a strong semantic association between the tags airbus and a. 
The additional benefit of tag clusters comes from the fact that they cover most words,  including proper nouns and newly coined words that are not defined in WordNet. 
For instance,  the word a is the model number of the airbus,  which is a kind of aircraft,  but is not defined in WordNet. 
Therefore,  it is hard to analyze the semantics of the word a using WordNet. 
Figs. 
Figure   3 shows the procedure used to construct a WFT. 
Given the initial word frequency table,  we obtain tag clusters of each word in the WFT from Flickr. 
This indicates that airport,  plane,  and a have a high semantic closeness with airbus in the tag space. 
Then,  we collect every tag that corresponds to each word existing in the original WFT and count the frequency of the collected tags to update the frequency of each word in the WFT. 
The tags in each tag cluster are also stored together in the updated word frequency of the WFT. 
Whereas,  the second category has less related words such as heath row and London. 
We also varied the number of words for which the rel gram was computed from 10 to 20. 
In (Sarkar 2010), the authors present following contributions
Multi document summarization is a process,  which produces a condensed representation of the contents of multiple related text documents collected from heterogeneous sources for human consumption and facilitates very rapid assimilation by human beings of the main points from related documents. 
(  1)  extracting important textual units from multiple related documents, (  2)  removing redundancies and(  3)  reordering or fusion of the units to produce a fluent summary. 
In extractive multi document summarization task,  summary sentences come from multiple source documents,  and picking sentences out of context may result in an incoherent summary. 
ranks sentences based on their similarities to the centroid. 
A centroid of a cluster of documents is defined as a pseudo document consisting of words with TF*IDF scores greater than a threshold,  where TF means term frequency which has been computed by the average number of occurrences of a word across a set of the documents to be summarized and IDF(  inverse document frequency)  signifying the rarity of a word in a text corpus,  is inversely proportional to the document frequency of a word. 
After the input documents are formatted and segmented,  the sentences are ranked based on two important features. 
,  text summarization was viewed as a problem of summarizing the similarities and differences in information content among the documents in a collection. 
Before applying sentence ranking algorithm,  input documents are broken in to a cluster of sentences. 
The major steps of the extractive multi document summarization are. 
TopK sentences are then selected based on a compression ratio. 
Centroid based multi document summarization. 
Document frequency of a word is defined as the number of documents that contain the word. 
that uses greedy approach to sentence selection and redundancy removal,  the clustering based approach controls redundancies in the final summary by clustering sentences to identify themes of common information and selecting one or two representative sentences from each cluster in to the final summary. 
Document frequency is the corpus statistics of a word and computed on a large collection of documents. 
The most summarization systems are either(  1)  sentence extraction based or(  2)  using sentence extraction as a primary component of a system. 
This paper presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system. 
The draft summaries are produced after ranking the sentences based on their scores and selecting Kt op ranked sentences. 
presents a pilot study on improving the sentence extraction based summarization performance by sentence compression. 
Few approaches identify repetitive phrases from a cluster of documents and use information fusion techniques to form a fluent summary. 
For managing a vast hoard of online or offline information,  summarization can be the useful means because the users can decide about the relevance of an individual document or a document cluster using just summary information. 
A basic element(  BE)  based sentence compression algorithm has been discussed in. 
A centroid is a pseudo document consisting of words with TF*IDF scores greater than a predefined threshold and TF(  term frequency)  = Average occurrences of a word across the input collection of the documents to be summarized and IDF(  Inverse Document Frequency)  is computed on a corpus using the formula. 
For our experiments,  we develop(  1)  a primary summarization system,  which extracts sentences to form a draft summary and(  2)  a trimming component,  which accepts a draft summary for revision. 
IDF=log(  N df)  where N number of documents in the corpus and df(  document frequency)  indicates the number of documents in which a word occurs. 
Two modifiers are taken to be similar when the term based similarity between them is greater than a threshold value(  which is 0.5 in our setting). 
based/VBN Saudi/NNP billionaire/NN Osama/NNP bin/NN Laden/NNP who/WP has/VBZ been/VBN accused/VBN…. 
Compared to the BEbased sentence compression algorithm that concentrates only on removing the redundant BEs(  basic elements)  from a summary,  our summary compression algorithm eliminates unimportant portions of the individual sentences and phrase level redundancy across the summary sentences. 
The BE(  basic elements)  based sentence compression. 
We re implement a centroid based sentence extraction method. 
Recently,  a sentence trimming based summary revision approach has been presented in. 
For example,  the phrase,  former Chilean dictator Au gusto Pinochet,  is tagged by the tagger as “former/JJ Chilean/JJ dictator/NN Augusto/NNP Pinochet/NNP” and “Augusto/NNP Pinochet/NNP” constitutes a named entity since it is a sequence of NNPs. 
In (Fang et-al. 2015), the authors present following contributions
Of particular interest to us in this paper is the extractive multi document summarization,  where the obtained final summary is a subset of the elements from multiple input documents. 
After obtaining the parameter,  here we discuss how to predict a summarization of multiple documents from a given topic set. 
Given multiple documents about a topic,  it is therefore desirable to discern most of the discriminative features according to topic factors(  e g,  preference of quantitative or technicality). 
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 
abstract based and extraction based. 
Many extraction based summarization methods have been proposed in the past years. 
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 
Documents summarization can be generally categorized as two approaches. 
Therefore,  in this paper,  we attempt to devise an approach to the generation of the topic aspect oriented summarization according to topic factors,  we call this TAOS(  Topic AspectOriented Summarization). 
The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining. 
The summarization of documents is the task of automatically extracting the latent gist of the given documents to indicate the main aspects in them. 
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization(  abbreviated as TAOS). 
With the exponential growth of the text and image documents over Internet,  the appropriate summarization of data has become more and more attractive for the organization of large scale data. 
BagofVisualWord is a image representation based on BoW(  BagofWord)  model. 
For text summarization task,  since the summaries are consist of individual sentences,  we totally extract three kinds of features of each sentence from documents. 
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
We can extract different kinds of features from documents and construct several groups of features which have different intrinsic discriminative power for describing the aspects of one summary. 
the multi documents to be summarized,  the length of summary L,  and the learned parameters. 
We compare our proposed approach with some stateoftheart methods on DUC2003,  DUC2004 data sets for text summarization and NUSWide data set for image summarization. 
The purpose of utilization of a sub modular scoring function is that it rewards summaries that are similar to many elements(  sentences or images)  in the original documents(  encourages coverage of one summary)  and also penalizes summaries that contain similar elements(  encourages diversity of one summary). 
Avg. 
In (CR132), the authors present following contributions
The authors mention that their preliminary results indicate that multiple document son documents on the same topic also contain redundancy but they fall short of using MMR for multidocumentsummarization. 
the development of a centroid based multidocumentsummarizer,  the use of cluster based sentence utility(  CBSU)  and cross sentence informational subsumption(  CSIS)  for evaluation of single andmultidocument summaries,  two user studies that support our findings,  and an evaluation of MEAD. 
,  uses modified TF*IDF to produce clusters of news articles on the same event We developed a new technique for multidocumentsummarization(  or MDS) ,  called centroidbasedsummarization(  CBS)  which uses as input the centroids of the clusters produced by CIDR to identify which sentences are central to the topic of the oft he cluster,  rather than the individual articles. 
We also describe two new techniques,  based on sentence utility and subsumption,  which we have applied to the evaluation thee valuation of both single and multiple document summaries. 
Finally,  we describe two user studies that test our models of multidocumentsummarization. 
We propose a utilitybasedevaluation scheme,  which can be used to evaluate both single document and multidocumentsummaries. 
Their metric is used as an enhancement to a query based summary whereasCSIS is designed for query independent(  a k a,  generic)  summaries. 
An event cluster,  produced by a TDT system,  consists of chronologically ordered news articles from multiple sources,  which describe an event as it develops over time. 
Cluster based sentence utility(  CBSU,  or utility)  refers to the degree of relevance(  from 0 to 10)  of aparticular sentence to the general topic of the entire cluster(  for a discussion of what is a topic,  see Allan et al   1998. 
In that paper,  MMRis used to produce summaries of single documents that avoid redundancy. 
Event clusters range from 2 to documents from which MEAD produces summaries in the form of sentence extracts. 
In this paper we will describe how multidocumentsummaries are built and evaluated. 
The extrinsic evaluation,  also called task based evaluation,  has received more attention recently at the DARPASummarization Evaluation Conference Mani et al,  1998. 
MEAD is significantly different from previous work on multi document summarization Radev &McKeown,  1998; Carbon ell and Gold stein,  Mani and Bloedorn,  1999; McKeown et al,  1999. 
We would also like to explore how the techniques we proposed here can be used for multiligual multidocumentsummarization. 
We used a new utility based technique,  CBSU,  for the fort he evaluation of MEAD and of summarizers in general. 
By giving credit for “ less than ideal' '  sentence sand sentences and distinguishing the degree of importance between sentences,  the utility based scheme is a more natural model to evaluate summaries. 
Cluster of d documents with n sentences(  compression rate = r). 
Sentences are laid in the same order as they appear in the original documents with documents ordered chronologically. 
Since the baseline of random sentence selection is already included in the evaluation formulae,  we used the Lead based method(  selecting the. 
In (Li et-al. 2015a), the authors present following contributions
Second,  to estimate the importance of the oft he bi grams big rams,  in addition to the internal features based on the test documents(  e g,  document frequency,  bi gram big ram positions) ,  we propose to extract features by leveraging multiple external resources(  such as word embedding from additional corpus,  Wikipedia,  Dbpedia,  WordNet,  SentiWordNet). 
identifying important summary sentences from one or multiple documents. 
Second,  to estimate the bi grams big rams weights,  in addition to using information from the test documents,  such as document frequency,  syntactic role in a sentence,  etc,  we utilize a variety of external resources,  includinga corpus of news articles with human generated summaries,  Wiki documents,  description of name entities from DBpedia,  WordNet,  and SentiWordNet.Discriminative features are computed based on these external resources with the goal to better represent the importance of a bi gram big ram and its semantic similarity with the given query. 
We use a rich set of features to represent each bi gram big ram candidate,  including internal features based on the test documents,  and features extracted from external resources. 
Our experimental results on multiple TAC data sets show the competitiveness of our proposed methods. 
These concepts are often obtained by selecting bigramsfrom the documents. 
Some stateoftheart summarization systems use integer linear programming(  ILP)  based methods that aim to maximize the important concepts covered in the summary. 
But these kinds of work involve using complex linguistic information,  often based on syntactic analysis Since the language concepts(  or bi grams big rams)  can be considered as key phrases of the documents,  the other line related to our work is how to extract and measure the importance of key phrases from documents. 
In this paper,  we improve such bi gram big ram based ILP summarization methods from different aspects. 
Extractive summarization is a sentence selection problem. 
Optimization methods have been widely used in extractive summarization lately. 
The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic queryhttp. 
This proves that the features from DBpedia and sentiwordnetprovide additional information not captured by the internal features from the documents Table 4. 
Definitions of the extent of the Indian subcontinent differ but it usually includes the core lands of India,  Pakistan,  and Bangladesh the percentage of the correct bi grams big rams(  those in the human reference summaries)  by our proposed selection method and the original ICSI system which justICSI SystemSystemused document ICSI frequency based selection. 
The order is based on its individual impact when combined with internal features. 
Martin sand Martins and Smith(  2009)  leveraged ILP technique to jointly select and compress sentences for multidocumentsummarization. 
In(  Medelyan et al,  2009) ,  Wikipedia based key phrases are determined base don based on a candidate s document frequency multiplied by the ratio of the number of Wikipedia articles containing the candidate as a link to the number of articles containing the candidate. 
In (Wang and Li 2012), the authors present following contributions
Given a collection of documents,  a variety of summarization methods based on different strategies have been proposed to extract the most important sentences from the original documents. 
Experimental results on DUC2002 and DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems,  and our proposed weighted consensus summarization method outperforms other combination methods. 
In extractive summarization the important sentences are selected from original documents based on their assigned scores. 
average score,  average rank,  Borda count,  median aggregation,  round robin scheme,  correlation based weighting method,  and graph based combination) ,  and experimental results on DUC2002 and DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems,  and our proposed weighted consensus summarization method outperforms other combination methods. 
LSA and NMF are both factorization based techniques which extract the semantic structure and hidden topics in the documents and select the sentences representing each topic as the summary. 
The centroid based summarization usually includes the sentences of the highest similarities with all the other sentences in the documents into the summary,  which is good since these sentences deliver the majority of information contained in the documents,  however the redundancy needs to be further removed and the subtopics in the documents are hard to detect. 
Previous research has shown that ensemble methods,  by combining multiple input systems,  are a popular way to overcome instability and increase performance in many machine learning tasks,  such as classification,  clustering and ranking. 
Multi document summarization aims to generate a compressed summary by extracting the major information in a collection of documents sharing the same or similar topics. 
In addition,  latent semantic analysis(  LSA)  and nonnegative matrix factorization(  NMF)  have also been used to produce the summaries by selecting semantically and probabilistically important sentences in the documents(  Gong & Liu,  2001). 
The problem of combining multiple ranking results into a consensus ranking is known as rank aggregation(  As lam and Montague,  2001,  Erp and Schomaker,  2000,  Manmatha et al,  2001). 
Since different summarization systems rank the sentences in the document collection using various strategies,  the results from each system can be viewed as a ranking of the sentences. 
The success of ensemble methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. 
In (Li et-al. 2013), the authors present following contributions
In addition,  previous research on joint modeling for compression and summarization suggested that the labeled extraction and compression data sets would be helpful for learning a better joint model(  Daum,  2006; Martins and Smith,  2009)  We hope that our work on this guided compression will also be of benefit to the future joint modeling studies Using our created compression data,  we traina supervised compression model using a variety of word,  sentence,  and document level features During summarization,  we generate multiple compression candidates for each sentence,  and use theILP framework to select compressed summary sentences. 
c 2013 Association for Computational Linguisticsmary guided compression combined with ILPbasedsentence selection for summarization in this paper We create a compression corpus for this purpose Using human summaries for a set of documents,  we identify salient words in the sentences. 
Prior work using such pipeline methods simply uses generic sentence based compression for each sentence in the documents,  no matter whether compression is done before or after summary sentence extraction. 
During summarization,  we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. 
Knight andMarcu(  2000)  utilize the noisy channel and decision tree method to perform sentence compression Lin(  2003)  shows that pure syntactic based compression may not improve the system performance Zajic et al  (  2007)  compare two sentence compression approaches for multi document summarization,  including a parseandtrim and a noisy channel approach Gal anis and Androutsopoulos(  2010)  use the maximum entropy model to generate the candidate compressions by removing the branches from the source sentences Liu and Liu(  2013)  coup lethe couple the couplet he sentence compression and extraction approaches for summarizing the spoken documents Wang et al(  2013)  design a series of learning based compression models built on parse trees,  and integrate the min them in query focused multi document summarization Prior studies often rely heavily on the generic sentence compression approaches(  McDonald,  Nomoto,  2007; Clarke and Lapata,  2008; Thadaniand McKeown,  2013)  for compressing the sentence sin sentences in the documents,  yet a generic compression system may not be the best fit for the summarization purpose In this paper,  we adopt the pipeline based compressive summarization framework,  but propose anovel guided compression method that is catered to the tot he summarization task. 
this is the current word s document frequency based on the 10 documents associated with each topic Bi gram Big ram document frequency. 
we train a supervised guided compression model using our created compression data,  wit ha variety of features then we use this model to generate nbest compressions for each sentence we feed the multiple compressed sentences to the ILP framework to select the best summary sentences. 
For generic sentence compression,  there may be multiple good human compressions for this sentence,  such as those listed in the table. 
In future,  we would like to further explore the reinforcement relationship between keywords and summaries(  Wan et al,  2007) ,  improve the readability of the sentences generated from the guided compression system,  and report results using multiple evaluation metrics(  Nenkova et al,  2007; Louis and Nenkova,  2012)  as well as performing human evaluations.AcknowledgmentsPart of this work was done during the first author s internship in Bosch Research and Technology Center. 
We conduct experiment son experiments on the TAC 2008 and 2011 summarization data set sand sets and show that by incorporating the guided sentence compression model,  our summarization system can yield significant performance gain as compared to the tot he stateoftheart. 
Our summarization system consists of three key components. 
In (Alguliev et-al. 2011), the authors present following contributions
The particle swarm optimization(  PSO)  algorithm is a stochastic population based search algorithm inspired by the social behavior of bird flocks or schools of fish. 
This model also guarantees that in the summary can not be multiple sentences that convey the same information. 
The summary type factor is similar to the style output factor indicated by Jones(  2007) ,  who identifies three of the same types of summary as Tucker,  indicative,  informative and critical,  and an additional one,  aggregative,  in which varied or multiple sources are summarized in relation to each other. 
With the rapid development of information communication technologies a huge amount of electronic documents has been produced and collected in the World Wide Web and digital libraries. 
NMF selects more meaningful sentences than the LSArelated methods,  because it can use more intuitively interpretable semantic features and grasp the innate structure of documents. 
Depending on the number of documents to be summarized,  the summary can be a single document or a multi document. 
Wang,  Zhu,  Li,  and Gong(  2009)  proposed a Bayesian sentence based topic model(  BSTM)  for multi document summarization by making use of both the word document and word sentence associations. 
The proposed model is quite general and can also be used for single and multi document summarization. 
► We define the objective function by weighted combination of the two objective functions based on the cosine and the NGDbased similarity measures. 
In (Khan et-al. 2015), the authors present following contributions
exploits template based method to produce abs tractive summary from multiple news wire newspaper documents depending on the output of the information extraction(  IE)  system. 
(  1)  fully automatic summarization of single news wire newspaper document, (  2)  fully automatic summarization of multiple news wire newspaper documents on single subject by generating document extracts and(  3)  fully automatic summarization of multiple news wire newspaper documents on single subject by generating document abstracts. 
Moreover,  this method could not handle or capture the information about similarities and differences across multiple documents. 
In this paper,  we propose a framework that will automatically fuse similar information across multiple documents and use language generation to produce a concise abs tractive summary. 
This particular GA based experiment is evaluated against multi documents in DUC 2002 data set. 
Content selection for summary is made by ranking the predicate argument structures based on optimized features,  and using language generation for generating sentences from predicate argument structures. 
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling. 
First,  it employs semantic role labeling to extract predicate argument structure(  semantic representation)  from the contents of input documents. 
A fuzzy ontology based approach. 
In (Goldstein et-al. 2000), the authors present following contributions
Conventional IR systems find and rank documents based on maximizing relevance to the user query(  Salton,  1970; van Rijsbergen,  1979; Buckley,  1985; Salton,  1989). 
Specifically,  we are constructing sets of 10 documents,  which either contain a snapshot of an event from multiple sources or the unfoldment of an event over time. 
(  i)  the need to carefully eliminate redundant information from multiple documents,  and achieve high compression ratios, (  ii)  take into account information about document and passage similarities,  and weight different passages accordingly,  and(  iii)  take temporal information into account. 
This is the focus of our work reported here,  including the necessity to eliminate redundancy among the information content of multiple related documents. 
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
This paper discusses a text extraction approach to multi document summarization that builds on single document summarization methods by using additional,  available information about the document set as a whole and the relationships between the documents. 
Our approach addresses these issues by using domain independent techniques based mainly on fast,  statistical processing,  a metric for reducing redundancy and maximizing diversity in the selected passages,  and a modular framework to allow easy parameterization for different genres,  corpora characteristics and user requirements. 
Our system(  1)  primarily uses only domain independent techniques,  based mainly on fast,  statistical processing, (  2)  explicitly deals with the issue of reducing redundancy without eliminating potential relevant information,  and(  3)  contains parameterized modules,  so that different genres or corpora characteristics can be taken into account easily. 
In (Baralis et-al. 2013), the authors present following contributions
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
(  i)  sentence based,  if they partition the documents into sentences and select the most informative ones. 
Unlike all of the above mentioned approaches,  our summarizer discovers association rules from the analyzed document to also represent the correlations among multiple terms in the graph based model. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
In (Carenini et-al. 2007), the authors present following contributions
Wan et al generate an affinity graph from multiple documents and use this graph for summarization. 
Since we are studying multiple email summarization in a conversational context,  we required that each conversation contained at least 4 emails. 
Stolfo et al   study the behavior model of email users based on the social network analysis among email correspondences. 
In other words the chain in clue word is not only lexical but also conversational,  and typically spans over several emails(  i e,  documents). 
Agrawal et al   extract social networks from newsgroups. 
In contrast,  for clue words,  the linkage is based on the existing conversation structure which is represented by the quotation graph. 
In (Alguliev et-al. 2013), the authors present following contributions
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
According to Mani and May bury(  1999) ,  automatic text summarization takes a partially structured source text from multiple texts written about the same topic,  extracts information content from it,  and presents the most important content to the user in a manner sensitive to the user s needs. 
(  1)  content coverage,  summary should contain salient sentences that cover the main content of the documents(  2)  diversity,  summaries should not contain multiple sentences that convey(  carry)  the same information and(  3)  length,  summary should be bounded in length. 
This paper proposes an optimization based model for generic document summarization. 
The model generates a summary by extracting salient sentences from documents. 
The experimental results provide strong evidence that the proposed optimization based approach is a viable method for document summarization. 
In (Radev et-al. 2001), the authors present following contributions
Summarization relies on the principles of text redundancy and unevenly distributed information content to extract highly informative snippets(  often keywords or sentences)  from a document which can be read by the user in lieu of the original document(  s)  In WebInEssence,  we have adopted an approach to summarization of multiple documents called centroid based summarization(  Radev et al,  2000). 
Centroid based summarization of multi document clusters puts special emphasis on portions of these documents that are most central to the topic of the entire cluster. 
WebInEssence is designed to help end users e ectively search for useful information and automatically summarize selected documents based on the users personal pro les. 
We not only support both single and multiple document summarization,  but also allow the user to specify the summarization compression ratio as well as to get per cluster summaries of automatically generated clusters,  which,  we believe,  are more valuable to online users and give them more exibility and control of the summarization results. 
Both single document summarization for a single URL and multiple document summarization for a cluster of URLs are supported in our system More related work can be found on the Extractor web site. 
In (Fung P, Ngai G 2006), the authors present following contributions
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
An unsupervised segmental Kmeans method is used to iteratively cluster multiple documents into different topics(  stories)  and learn the parameters of parallel Hidden Markov Story Models(  HMSM) ,  one for each story. 
Our HMSM method also provides a simple way to compile a single meta summary for multiple documents from individual summaries via state labeled sentences. 
In (Otterbacher et-al. 2009), the authors present following contributions
In this paper,  we focus on the query based or focused summarization problem where we seek to generate a summary of a set of related documents given a speci c aspect of their oft heir common topic formulated as a natural language query. 
A key challenge for passage retrieval for QA is that,  when attempting to retrieve answers to questions from a set of documents published by multiple sources over time(  e g. 
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Khan et-al. 2015), the authors present following contributions
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling. 
First,  it employs semantic role labeling for semantic representation of text. 
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015), the authors present following contributions
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization. 
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (CR92), the authors present following contributions
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS)  Speci cally,  a set of reader comments associated with the news reports are also collected. 
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS). 
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a), the authors present following contributions
We develop a Ranking framework upon Recursive Neural Networks(  R2N2)  to rank sentences for multi document summarization. 
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015), the authors present following contributions
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 
Therefore,  in our work,  computing importance,  non redundancy and coherence is tightly integrated and taken care of simultaneously Our graph based summarization technique has the further advantage of being completely unsupervised We apply our graph based summarization technique to scienti c articles from the journal PLOS Medicine,  a high impact open access journal from the medical domain. 
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015), the authors present following contributions
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 
Given a collection of articles spanning different topics,  but with similar titles,  automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy as a topic DAG. 
Let G(  V,  E)  be the DAG structured topic hierarchy with V topics. 
Given a DAGstructured topic hierarchy and a subset of objects,  we investigate the problem of finding a subset of DAGstructured topics that are induced by that subset(  of objects). 
Our approach is based on sub modular maximization and mixture learning,  which has been successfully used in applications such as document summarization(  Lin,  2012)  and image summarization(  Tschiatschek et al,  2014) ,  but has never been applied to topic identification tasks or,  more generally,  DAG summarization. 
Wikipedia s category hierarchy consists of more than M categories(  topics)  arranged hierarchically. 
Furthermore,  our approach also generalizes these clustering approaches,  since one of the components in our mixture of sub modular functions is defined via these unsupervised approaches,  and maps a given clustering to a set of topics in the hierarchy. 
The facility location function,  defined as f(  S)  = P i V maxj S sij,  is a natural model for kmedoids and exemplar based clustering,  and has been used in several summarization problems(  Tschiatschek et al,  2014; Wei et al,  a). 
Another extension is the hierarchical LDA(  Blei et al,  2004) ,  where topics are joined together in a hierarchy by using the nested Chinese restaurant process. 
Nonparametric extensions of LDA include the Hierarchical Dirichlet Process(  Teh et al,  2006)  mixture model,  which allows the number of topics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. 
We introduce a family of sub modular functions to identify an appropriate set of topics from a DAG structured hierarchy of topics for a group of documents. 
Topics higher up in the hierarchy are abstract and less specific. 
Desirable properties of the chosen topics include document coverage,  specificity,  topic diversity,  and topic homogeneity,  each of which,  we show,  is naturally modeled by a sub modular function. 
Unlike previous work,  which focuses on clustering the set of documents using the topic hierarchy as features,  we directly pose the problem as a sub modular optimization problem on a topic hierarchy using the documents as features. 
The middle portion of Figure 1 depicts a topic hierarchy with associated documents. 
All the above mentioned problems can be modeled as the problem of finding the most representative subset of topic nodes from a DAGStructured topic hierarchy. 
The problem then,  is to organize the articles into multiple groups where each group contains articles of similar nature(  topics)  and has an appropriately discerned group heading. 
One can often define a natural topic hierarchy to categorize these objects. 
Disambiguations may be seen as paths in a hierarchy leading to different articles that arguably could have the same title. 
For example,  the topics Physics,  Chemistry,  and Mathematics can be summarized into a topic Science. 
The association of the documents with the existing topic hierarchy is also well studied. 
Let D be the set of documents that are associated with one or more of these topics. 
In our approach,  we assume,  individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy. 
In each of these approaches,  unlike our proposed approach,  an existing topic hierarchy is not used,  nor is any additional object topic information leveraged. 
Systems such as WikipediaMiner(  Milne,  2009) ,  TAGME(  Ferragina and Scaiella,  2010)  and several annotation systems such as(  Dill et al,  2003; Mihalcea and Csomai,  2007; Bunescu and Pas ca,  2006)  attach topics from Wikipedia(  and other catalogs)  to the documents by establishing the hard or soft links mentioned above. 
We use a large margin framework to learn convex mixtures over the set of sub modular components. 
The pachinko allocation model(  PAM) (  Li and McCallum,  2006)  captures arbitrary,  nested,  and possibly sparse correlations between topics using a DAG. 
Disambiguation pages on Wikipedia are used to resolve conflicts in article titles that occur when a title is naturally associated with multiple articles on distinct topics. 
Several existing text collection data sets such as 20 Newsgroup,  Reuters work with a predefined set of topics. 
For example,  in text and image classification problems,  each document or image is assigned a hierarchy of labels — a baseball page is assigned the labels baseball and sports Moreover,  many of these applications,  naturally have an existing topic hierarchy generated on the entire set of objects(  Rousu et al,  2006; Barutcuoglu et al,  2006; ling Zhang and hua Zhou,  2007; Silla and Freitas,  2011; Tsoumakas et al,  2010). 
Our proposed techniques can summarize such large sets of labels into a smaller and more meaningful label sets using a DAGstructured topic hierarchy. 
In this paper,  we investigate structured prediction methods for learning weighted mixtures of sub modular functions to summarize topics for a collection of objects using DAGstructured topic hierarchies. 
It assumes an existing label hierarchy in the form of a tree. 
We cannot,  however,  generate a subset of topics from a large existing topic DAG that can act as summary topics,  using PAM. 
These systems are able to extract signals from a text document and identify Wikipedia articles and or categories that optimally match the document and assign those article category names as topics for the document. 
Other information,  provided say by unsupervised approaches such as LDA and its variants,  can also be utilized by defining a sub modular function that expresses coherence between the chosen topics and this information. 
Our problem is different from traditional summarization tasks since we have an underlying DAG as a topic hierarchy that we wish to summarize in response to a subset of documents. 
Moreover,  the root label is always applied and it is very likely that many labels near the top level of the label hierarchy are also classified as relevant to the group of documents. 
Penalty based diversity. 
Medelyan et al  (  Medelyan et al,  2008)  and Ferragina et al  (  Ferragina and Scaiella,  2010)  detect topics for a document using Wikipedia article names and category names as the topic vocabulary. 
A summary set of topics should cover most of the documents. 
Finally,  we utilize a large margin formulation for learning mixtures of these sub modular functions,  and show how we can optimally learn them from training data. 
This function is sub modular,  but is not in general monotone,  and has been used in document summarization(  Lin and Bilmes,  2011) ,  as a dispersion function(  Borodin et al,  2012) ,  and in image summarization(  Tschiatschek et al,  2014). 
(  1)  we use the label hierarchy to infer a set of labels for a group of documents(  2)  we do not enforce the label hierarchy to be a tree as it can be a DAG; and(  3)  generalizing HSLDA to use a DAG structured hierarchy and infer labels for a group of documents(  e g,  combining into one big document)  also may not help in solving our problem. 
In particular,  we focus on producing a clustering of documents where clusters are encouraged to honor a predefined DAG structured topic hierarchy. 
This material is based upon work supported by the National Science Foundation under Grant No   IIS1162606,  and by a Google,  a Microsoft,  and an Intel research award. 
A natural extension of this definition to a set of topics T is defined as Γ(  T)  = t T(  t). 
In the below,  we define a variety of sub modular functions that capture the above properties,  and we then describe a large margin learning framework for learning convex mixtures of such components. 
Figure 1 describes the topic summarization process for creation of the disambiguation page for Apple. 
To alleviate this problem,  we adopt cluster based evaluation metrics. 
These topics are observed to have a parent child(  isa)  relationship forming a DAG. 
QC Functions As Barrier Modular Mixtures. 
We can now adopt Jaccard Index,  F measure,  and NMI(  Normalized Mutual Information)  based cluster evaluation metrics described in(  Manning et al,  2008). 
Summarization is the task of extracting information from a source that is both small in size but still representative. 
The association links between the documents and topics can be hard or soft. 
Furthermore,  if a document is attached to a topic t,  we assume that all the ancestor topics of t are also relevant for that document. 
In case of a hard link,  a document is attached to a set of topics. 
We argue that many formulations of this problem are natural instances of sub modular maximization,  and provide a learning framework to create sub modular mixtures to solve this problem. 
Set S ∗ is the summary topics scored best. 
A topic t is said to cover a set of documents Γ(  t) ,  called the transitive cover of the topic t,  if for all documents i ∈ Γ(  t) ,  either i is associated directly with topic t or with any of the descendant topics of t in the topic DAG. 
Given a(  ground set)  collection V of topics organized in a preexisting hierarchical DAG structure,  and a collection D of documents,  chose a size K ∈ Z representative subset of topics. 
The notion of best describing topics is characterized through a set of desirable properties - coverage,  diversity,  specificity,  clarity,  relevance and fidelity - that K topics have to satisfy. 
Given a budget of K,  our objective is to choose a set of K topics from V,  which best describe the documents in D. 
However,  we exclude the similarity based functions described in section 3.1.2. 
This indicates that the clusters of articles produced by our technique resembles the clusters of articles present on the disambiguation page better than other techniques. 
By excluding similarity based functions,  we can compare the quality of the results with and without O(  n 2)  functions. 
We learn the mixture weights from the training set and use them during inference on the test set to subset K topics through the sub modular maximization(  Equation 1). 
In 60% of the disambiguation queries,  SMMLcov and SMMLcov+sim approach produce higher JI,  F and NMI than all other methods. 
For example,  one of the groups on the Matrix disambiguation page has a name Business and government and there is no Wikipedia category by that name. 
In Figures b and c we measure the number of test instances(  i e,  disambiguation queries)  in which each of the algorithms dominate(  win)  in evaluation metrics. 
SMMLcov+sim. 
Unfortunately,  these names may not correspond to the Wikipedia category(  topic)  names. 
Every group of articles on the Wikipedia disambiguation page is assigned a name by the editors. 
In all these experiments,  we performed 5 fold cross validation to learn the parameters from 80% of the disambiguation pages and evaluated on the rest of the 20%,  in each fold. 
We show that the sub modular mixture learning and maximization approaches,  i e,  SMMLcov and SMMLcov+sim outperform other approaches in various metrics. 
This makes the inference time complexity On. 
Our proposed techniques SMMLcov and SMMLcov+sim outperform other techniques consistently. 
We validated our approach by comparing against several baselines described below. 
We eliminated few administrative categories such as Hidden Categories,  Articles needing cleanup,  and the like. 
Wikipedia category structure is used as the topic DAG. 
We collected about 8000 disambiguation pages that had at least four groups on them. 
We parsed the contents of Wikipedia disambiguation pages and extracted disambiguation page names,  article groups and group names. 
In (Bing et-al. 2015), the authors present following contributions
We propose an abstraction based multi document summarization framework that can construct new sentences by exploring more fine grained syntactic units than sentences,  namely,  noun verb phrases. 
Existing multi document summarization(  MDS)  methods fall in three categories. 
Most summarization systems adopt the extraction based approach which selects some original sentences from the source documents to create a short summary(  Erk an and Radev,  2004; Wan et al,  2007). 
Existing multi document summarization(  MDS)  works can be classified into three categories. 
(  2)  To simulate compression based summarization,  we can adapt our framework to conduct sentence selection and sentence compression in a joint manner. 
(  1)  To simulate extraction based summarization,  we just need to constrain that the highest NP and the highest VP from the same sentence are selected simultaneously. 
extraction based,  compression based and abstraction based. 
Yet,  these compressive summarization models cannot merge facts from different source sentences,  because all the words in a summary sentence are solely from one source sentence. 
To this end,  some researchers apply compression on the selected sentences by deleting words or phrases(  Knight and Mar cu,  2000; Lin,  2003; Zajic et al,  2006; Harabagiu and Lacatusu,  2010; Li et al,  2015) ,  which is the compression based method. 
extraction based approaches,  compression based approaches,  and abstraction based approaches. 
The data set of traditional summarization task in TAC 2010 is employed as the development tuning data set. 
Based on the tuning set,  the key parameters of our model are set as follows. 
Researchers also explored some variants of the typical MDS setting,  such as query chain focused summarization that combines aspects of update summarization and query focused summarization(  Baum el et al,  2014) ,  and hierarchical summarization that scales up MDS to summarize a large set of documents(  Christensen et al,  2014). 
Researchers developed an information extraction based approach that extracts information items(  Ge nest Gen est Gene st and Lapalme,  2011)  or abstraction schemes(  Ge nest Gen est Gene st and Lapalme,  2012)  as components for generating sentences. 
Different from existing abstraction based approaches,  our method first constructs a pool of concepts and facts represented by phrases from the input documents. 
In the process of new sentence generation,  the compatibility relation between NP and VP and a variety of summarization requirements are jointly considered. 
The data set of traditional summarization task in Text Analysis Conference(  TAC)  2011 is used to evaluate the performance of our approach. 
Therefore,  in recent summarization evaluation workshops such as TAC,  the pyramid is used as the major metric. 
The pseudo timestamp of a sentence is defined as the earliest timestamp of its VPs and the sentences are ordered based on their pseudo timestamps. 
Many existing extraction based and compression based MDS approaches could be regarded as special cases under our framework. 
Different weights are assigned to SCUs based on their frequency in model summaries. 
Moreover,  some works(  Liu et al,  2012; Kageb ˚ ack et al,  2014; De nil et al,  2014; Cao ¨ et al,  2015)  utilized deep learning techniques to tackle some summarization tasks. 
On the other hand,  abstraction based approaches can generate new sentences based on the facts from different source sentences. 
However,  the usability of such features depends on the availability of predefined categories in the summarization task,  as well as the availability of training data with the same predefined categories for estimating feature weights. 
System 22 is an extraction based method that picks the original sentences,  hence it achieves higher score in Q grammaticality,  while our approach has some new sentences with grammar mistakes,  which is a common problem for abs tractive methods and deserves more future research effort. 
Meanwhile,  the constructed sentences should satisfy the constraints related to summarization requirements such as NP/VP compatibility. 
Experimental results on TAC 2011 summarization data set show that our framework outperforms the top systems in TAC 2011 under the pyramid metric. 
Different types of salience can be incorporated in our framework,  such as position based method(  Yih et al,  2007) ,  statistical feature based method(  Wood send Woods end and Lapata,  2012) ,  concept based method(  Li et al,  2011) ,  etc   One key characteristic of our approach is that the considered basic units are phrases instead of sentences. 
In our implementation,  we adopt a concept based weight incorporating the position information. 
In abs tractive summarization,  we do not prefer to generate many short sentences. 
X i l(  Ni)  ∗ i + X j l(  Vj)  ∗ j ≤ L, (  16)  where l( )  is the word based length of a phrase. 
In contrast,  our framework does not define any category specific feature and only uses TAC 2010 data to tune the parameters for general summarization purpose. 
We adopt those resolution rules that are able to achieve high quality and address our need for summarization. 
Passonneau et al  (  2013)  showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores,  yielding more accurate pyramid scores than string matching based automated methods(  Harnly et al,  2005). 
Otherwise,  the similarity is calculated with the above Jaccard Index based method. 
The second component constructs new sentences by selecting and merging phrases based on their salience scores,  and ensures the validity of new sentences using a integer linear optimization model. 
The abstractivebased approaches gather information across sentence boundary,  and hence have the potential to cover more content in a more concise manner. 
This idea is based on two observations. 
Some works,  albeit less popular,  have studied abstraction based approach that can construct a sentence whose fragments come from different source sentences. 
Compression based approaches have been investigated to alleviate the above limitation. 
However,  extraction based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. 
Extraction based approaches are the most studied of the three. 
The position based term frequency is used in the concept weighting scheme. 
Compared with the sentence fusion approaches that compute salience scores of sentence clusters,  our proposed framework explores a more fine grained textual unit(  i e,  phrases) ,  and maximizes the salience of selected phrases in a global manner. 
In particular,  Sieve 1,  2,  3,  4,  5,  9,  and 10 in the package are used. 
We use some heuristics to find compatibility,  and then expand the compatibility relation to more phrases by extracting co reference. 
In (Wang and Li 2012), the authors present following contributions
Multi document summarization is a fundamental tool for document understanding and has received much attention recently. 
As a good ensemble requires the diversity of the individual members,  in this paper,  we first study the most widely used multi document summarization systems based on a variety of strategies(  e g,  the centroid based method,  the graph based method,  LSA,  and NMF) ,  and evaluate different baseline combination methods(  e g,  average score,  average rank,  Borda count,  median aggregation,  round robin scheme,  correlation based weighting method,  and graph based combination)  for obtaining a consensus summarizer to improve the summarization performance. 
A variety of multi document summarization methods have been developed in the literature. 
Given a collection of documents,  a variety of summarization methods based on different strategies have been proposed to extract the most important sentences from the original documents. 
Different multi document summarization methods base on different strategies and usually produce diverse outputs. 
Multi document summarization Weighted consensus. 
Multi document summarization aims to generate a compressed summary by extracting the major information in a collection of documents sharing the same or similar topics. 
Thus,  multi document summarization has attracted much attention in recent years,  and many applications have been developed. 
With the explosive growing of the volume and complexity of document data(  e g,  news,  blogs,  web pages)  on the Internet,  multi document summarization provides a useful solution for understanding documents and reducing information overload. 
In this paper we focus on extractive multi document summarization. 
To the best of our knowledge,  so far there are only limited attempts on using ensemble methods in multi document summarization(  Wang & Li,  2010). 
Section 2 discusses the related work on multi document summarization and consensus ranking methods. 
Experiments on DUC2002 and DUC2004 data sets demonstrate the performance improvement using various consensus multi document summarization methods,  and our proposed weighted consensus scheme outperforms the other baseline combination methods. 
In this paper,  we study four most widely used multi document summarization systems(  i e. 
In the section,  we describe four typical multi document summarization methods and eight aggregation methods implemented in our experimental study. 
Multi document summarization has been widely studied recently. 
the centroid based method,  the graph based method,  LSA,  and NMF)  and propose a weighted consensus summarization method to combine the results from single summarization systems. 
average score,  average rank,  Borda count,  median aggregation,  round robin scheme,  correlation based weighting method,  and graph based combination) ,  and experimental results on DUC2002 and DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems,  and our proposed weighted consensus summarization method outperforms other combination methods. 
In extractive summarization the important sentences are selected from original documents based on their assigned scores. 
One work related to ensemble summarization is described in Thapar,  Mohamed,  and Rajasekaran(  2006) ,  where a graph based meta summarization approach by comparing the document graph of individual summary with the centric graph for all the summary from different summarization systems is proposed. 
a graph based summarization method recommending sentences by the voting of their neighbors(  Erk an & Radev,  2004). 
Each of the individual summarization methods ranks the sentences based on different criteria. 
In addition,  the performance of WCS is also better than the graph based combination because the graph based method only considers the subset of sentences selected by individual summarization systems. 
CW,  ULARA,  and WCS) ,  our WCS method optimizes the weighted distance between the consensus sentence ranking to individual rankings and updates the weights and consensus ranking iteratively,  which is closer to the nature of consensus summarization than other approximation based weighted methods such as CW and ULARA and avoids trivial solutions. 
The centroid based summarization usually includes the sentences of the highest similarities with all the other sentences in the documents into the summary,  which is good since these sentences deliver the majority of information contained in the documents,  however the redundancy needs to be further removed and the subtopics in the documents are hard to detect. 
Some query based summarization systems are also proposed(  Gold stein et al,  1999,  Wan and Yang,  2007). 
For example,  Language Computer Corporation(  LCC) (  LCC,  xx xx) ,  a DUC participant,  that proposes a system combining the question answering and summarization system and using knearest neighbor clustering based on cosine similarity for the sentence selection. 
Other methods include CRFbased summarization(  Sh en,  Sun,  Li,  Yang,  & Chen,  2007) ,  and hidden Markov model(  HMM)  based method(  Conroy & O’Leary,  2001). 
The most commonly used methods are centroid based,  which usually rank sentences in the document collection according to their scores calculated by a set of predefined features,  such as term frequency inverse sentence frequency(  TFISF) (  Lin and Hovy,  2002,  Radev et al,  2004) ,  sentence or term position(  Lin and Hovy,  2002,  Yih et al,  2007) ,  and number of keywords(  Yih et al,  2007). 
• Centroid based methods. 
This scheme is consistent with the strategies in most cluster based summarization methods on selecting sentences with different summary lengths. 
Finally the sentences are re ranked reran ked based on their average scores. 
There are several most widely used extractive summarization methods as follows. 
Although various summarization approaches have been developed in literature,  few efforts have been reported on aggregating document summarization methods. 
where K is the number of summarization systems,  and Score k(  Si)  is the normalized individual score by the kth system. 
Another type of methods use sentence graph representation and select sentences based on the votes from their neighbors using the ideas similar to PageRank(  Erk an and Radev,  2004,  Mihalcea and Tarau,  2005). 
They also develop an EMbased algorithm which uses each ranking as an observation to estimate the parameters for combining the ranking lists(  Klementiev & Roth,  2008). 
The graph based methods such as LexPageRank apply graph analysis and take the influence of other sentences into consideration,  which provides a better view of the relationships embedded in the sentences. 
LSA and NMF are both factorization based techniques which extract the semantic structure and hidden topics in the documents and select the sentences representing each topic as the summary. 
Suppose there are K single summarization methods,  each of which produces a ranking for the sentences containing in the document collection. 
These individual summarization methods are selected as the representatives of the most widely used types of summarization methods,  and they are fundamentally different in both algorithm design and implementation,  which makes them diverse and complimentary with each other. 
In (Alguliev et-al. 2013), the authors present following contributions
This paper proposes an optimization based model for generic document summarization. 
Zhao,  Wu,  and Huang(  2009)  propose a query expansion algorithm used in the graph based ranking approach for query focused multi document summarization. 
The graph based ranking algorithms such as PageRank(  Br in & Page,  1998)  and HITS(  Klein berg,  1999)  have also been used in generic multi document summarization. 
A novel multi document summarization model based on the budgeted median problem proposed in Takamura and Okumura(  b). 
Up to now,  various extraction based techniques have been proposed for generic multi document summarization. 
The work(  Sarkar,  2010)  presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system. 
Wang et al  (  2009)  propose a Bayesian sentence based topic model(  BSTM)  for multi document summarization by making use of both the term document and term sentence associations. 
We implemented the proposed model on multi document summarization task. 
The experimental results provide strong evidence that the proposed optimization based approach is a viable method for document summarization. 
Document summarization,  especially multi document summarization in essence is a multi objective optimization problem. 
The multi document summarization task has turned out to be much more complex than summarizing a single document,  even a very large one. 
For effective multi document summarization,  it is important to reduce redundant information in the summaries and extract sentences which are common to given documents. 
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
Single document summarization can only condense one document into a shorter representation,  whereas multi document summarization can condense a set of documents into a summary. 
Multi document summarization can be considered as an extension of single document summarization and used for precisely describing the information contained in a cluster of documents and facilitate users to understand the document cluster. 
In this paper,  we focus on extractive multi document summarization. 
As compared to generic summarization that must contain the core information central to the source documents,  the main goal of query focused multi document summarization is to create from the documents a summary that can answer the need for information expressed in the topic or explain the topic. 
The problem of using topic representations for multi document summarization has received considerable attention recently. 
First,  it introduces two novel topic representations that leverage sets of automatically generated topic themes for multi document summarization. 
It shows how these new topic representations can be integrated into a stateoftheart multi document summarization system. 
Bollegala,  Okazaki,  and Ishizuka(  2010)  presented a bottom up approach to arrange sentences extracted for multi document summarization. 
Other methods include NMFbased topic specification(  Lee et al,  2009,  Wang et al,  2008,  Wang et al,  2009)  and CRFbased summarization(  Sh en et al,  2007). 
The major concerns in graph based summarization researches include how to model the documents using text graph and how to transform existing web page ranking algorithms to their variations that could accommodate various summarization requirements(  Wenjie,  Furu,  Qin,  & Yanxiang,  2008). 
The centroid based method,  MEAD,  is one of the popular extractive summarization methods(  Radev,  Jing,  Stys,  & Tam,  2004). 
Redundancy is one of the important issues in multi document summarization. 
The potential of optimization based document summarization models has not been well explored to date. 
Motivated by recent progress in optimization based document summarization(  Alguliev and Aliguliyev et al,  2011,  Alguliev et al,  2011)  in this paper we propose a novel document summarization model which simultaneously considers content coverage and redundancy. 
We implemented our model on multi document summarization task. 
We also showed that the resulting summarization system based on the proposed optimization approach is competitive on the DUC2002 and DUC2004 data sets. 
BSTM is similar to the FGB summarization since they are all based on sentence based topic model. 
Researchers all over the world working on multi document summarization are trying different directions to see methods that provide the best results(  Tao et al,  2008,  Wan,  2008,  Wang et al,  2008,  Wang et al,  2011,  Wang et al,  2009). 
Multi document summarization has been widely studied recently. 
Binwahlan,  Salim,  and Suanmali(  2010)  introduced a different hybrid model based on fuzzy logic,  swarm intelligence and diversity selection for text summarization problem. 
Depending on the number of documents,  summarization techniques can be classified into two classes. 
single document and multi document(  Fattah and Ren,  2009,  Zajic et al,  2008). 
A multi document summary can be used to concisely describe the information contained in a cluster of documents and to facilitate the users to understand the document cluster. 
In general,  document summarization can be divided into extractive summarization and abs tractive summarization. 
Second,  it presents eight different methods of generating multi document summaries. 
Digital archives have been built up in almost every level of egovernment hierarchy. 
In order to statistically compare the performance of OCDsumSaDE with other summarization methods,  we use a nonparametric statistical significance test,  called Wilcox on s matched pairs signed rank based statistical test(  Hollander & Wolfe,  1999) ,  to determine the significance of our results. 
Unlike the MMR that uses greedy approach to sentence selection and redundancy removal,  the clustering based approaches control redundancy in the final summary by clustering sentences to identify themes of common information and selecting one or two representative sentences from each cluster into the final summary(  Alguliev and Aliguliyev,  2008,  Aliguliyev,  a,  Aliguliyev,  2010,  Wang et al,  2011). 
In (Heu et-al. 2015), the authors present following contributions
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
Hennig and Labor(  2009)  proposed a multi document summarization method based on Probabilistic Latent Semantic Analysis(  PLSA) ,  which represents sentences and queries as probability distributions over latent topics. 
Most existing multi document summarization techniques analyze semantic relationships between words in a document by exploiting probability theory,  machine learning techniques,  and external knowledge bases such as WordNet.1 However,  these techniques suffer from high computational costs in the learning and summarization processes. 
Over the past few years,  multi document summarization has received more attention and made much progress. 
However,  various evaluations indicate that multi document summarization is highly complex and demanding,  and there is still much work to be done before automatic summarizers catch up with human beings(  Dang & Owczarzak,  2008). 
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 
The proposed multi document summarization system is presented in Section 3. 
Figure   2 shows the framework of our multi document summarization system FoDoSu. 
In this section,  we present the results of our evaluation of multi document summarization. 
The advantages of our approach for multi document summarization are as follows. 
In Section 2.1,  we first review the related work on various multi document summarization techniques. 
Multi document summarization techniques can be classified into two approaches. 
In addition,  we will improve our proposed multi document summarization. 
In this paper,  we propose FoDoSu which is a novel multi document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. 
In the literature,  the development of multi document summarization has been largely promoted by the Document Understanding Conferences(  DUC)  2 and Text Analysis Conferences(  TAC)  .3. 
To overcome these drawbacks,  we propose a novel multi document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. 
Traditionally,  research on extractive summarization is based on the position of a sentence in a document which measures the overall frequency of the words they contain such as the TFIDF technique(  Luhn,  1958; Edmund son,  1969; Bran dow Brand ow,  Mitze,  & Rau,  1995). 
Tables 3 and 4 show the results when comparing our proposed system FoDoSu,  which uses TFIDF and HITS algorithm together,  with the related multi document summarization techniques on TAC 2008 and TAC 2009. 
Furthermore,  the results show that the proposed system using the HITS algorithm is more effective than other the HITS algorithm based systems for document summarization. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
Although the abstraction based method can summarize a document more accurately than the extraction based method,  it is much more difficult and complex because it requires the use of high cost natural language processing technologies,  such as information fusion(  Barzilay,  McKeown,  & Elhadad,  1999) ,  sentence compression(  Knight & Mar cu,  2002) ,  and reformulation(  McKeown,  Klavans,  Hatzivassiloglou,  Barzilay,  & Es kin,  1999). 
In contrast,  the abs tractive summarization approach takes the essence of the source document to build a summary by using natural language processing techniques. 
Both data sets are open benchmark data sets from the Text Analysis Conference(  TAC)  for automatic summarization evaluation. 
To address this problem,  various document summarization techniques have been studied to efficiently summarize the core of a single original document. 
Finally,  our system generated a final summary of the given documents based on the top n scored sentences in the Score Table. 
Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems. 
Web 2.0 has led to the development of large Web based communities that support and facilitate collaboration among Internet users(  Huang,  Lin,  & Chan,  2012). 
Dang and Luo(  2008)  proposed a method to detect key sentences using keyword extraction based on statistics and syn sets. 
Zhu et al  (  2009)  proposed a tag oriented Web document summarization approach using both the document itself and the tags annotated on the document. 
Finally,  we demonstrated that analysis using tag clusters from the Folksonomy greatly affected document summarization. 
The importance of a sentence is determined by its corresponding score,  where s is a sentence in the given multi document. 
Also,  the extracted sentences are given a weighted score based on the importance and contributions of the words in each sentence. 
One is the extractive summarization approach and the other is the abs tractive summarization approach(  Mani,  2001). 
The performance of WithHITS on TAC 2008 and TAC 2009 was improved by 8.9–11.5% compared to OnlyFREQ,  and by 2.7– 4.1% compared to WithoutHITS. 
This results show that TFIDF is more effective than the word frequency when extracting the important words from documents. 
On the other hand,  the result of TFIDF can always perform better than that of OnlyFREQ on both data sets. 
Then,  the hub score h(  wi)  increases by 1,  otherwise it is zero. 
In (Ye et-al. 2007), the authors present following contributions
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document(  s)  that can be preserved after summarization. 
In this paper,  we review and detail our approach to automatic,  multi document extractive summarization. 
In practice in multi document summarization,  we have found that sentences that have the same inventory of concepts rarely express different semantics between concepts. 
DCL can handle these problems naturally,  since it does not need the exact number of clusters,  and all involved local topics are organized in a hierarchy. 
Text summarization Document concept lattice Concept Semantic. 
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests. 
Based on this lattice,  the summary is an optimized selection of a set of distinct and salient local topics that lead to maximal coverage of concepts with the given number of sentences. 
Text summarization is the process of distilling the most important information from sources to produce an abridged version for a particular users and tasks(  Mani & May bury,  1999). 
We design a summarization algorithm based on DCL that explores the combinations of representative sentences from the distinct local topics in the observation. 
The experiment results of our participation of DUC 2005 and 2006 evaluations,  along with follow on tests,  demonstrate that DCL is a competitive model for summarization as compared with existing technologies based on sentence clustering and sentence sorting. 
Most MMR based systems(  Barzilay and Elhadad,  1997,  Carbon ell and Gold stein,  1998,  Radev et al,  2004)  would differentiate these sentences into three groups {1,  2,  3,  4}(  each containing unique terms with respect to each other) ,  {5,  6}(  sharing two groups of two terms from {1,  4}) ,  and {7}(  sharing one term from each sentence {1,  4}). 
These MMRbased systems typically use a method to choose the kth sentence using some measure of the sentence s redundancy and diversity with the set of sentences already flagged as part of the summary. 
Most summarization systems to date do not select the set of summary sentences as a single step. 
A problem is that this greedy process may lead the summarization system to pick a local optimum instead of a global one. 
Experimental results from DUC evaluations and follow on tests indicate that our DCLbased approach is competitive with the stateoftheart. 
Section 5 depicts the algorithm for constructing the summary based on DCL. 
• We motivate our approach by considering an automatic summarization evaluation framework where summaries are judged on answer loss. 
Based on the discussion above,  concepts are considered as the terms that remain after removing closed class words in the source sentences. 
We introduce this evaluation criterion as a way of motivating our summarization approach. 
Given the notion of semantic concepts,  the extractive summarization approach is equivalent to picking a set of sentences that represent as many salient concepts as possible. 
In summarization tasks such as DUG,  summary length is judged based on word count rather than sentence count. 
We report the experiment results based on DUC 2005 and 2006 corpus and discuss our extended experiments in Section 6. 
Evaluating the quality of a summary is a fundamental problem in summarization. 
Query based summarization can be handled in a similar way,  in which the source texts are first filtered for relevance to the query,  and only answers that appear in the filtered set would be considered in the denominator of Eq. 
For query based summarization,  we need to consider whether the selected sentences are relevant to the user s intention expressed by their query. 
When the answer length is set to a single sentence,  MMRbased systems would select sentence 7 as the summary. 
Based on this lattice structure,  we can extract a summary customized to a desired length. 
These measures are based on the comparisons between system crafted and human crafted summaries or evaluated by human accessors directly. 
In contrast to other methods,  our summarization method uses a global selection strategy that seeks to minimize answer loss. 
We thus need to weight the importance of each sentence based on which answers in contains and the importance of each contained answer. 
It is equivalent to measuring the factual recall of a summary based on a list of answers to questions derived from the source document set. 
These properties are described as redundancy and diversity in Katz s G model(  Katz,  1996)  and are used as basis in many current summarization approaches. 
From our own informal study of question and answers in both summarization and QA systems,  we observe that answers usually describe abstract or concrete entities and their actions. 
As we have previously discussed,  DCL and its algorithm for summarization are based on the evaluation framework,  which pursues a compression version of the original document(  s)  with minimal answer loss. 
If we set out to build a summarization system that minimizes answer loss,  we need a definition of what an answer is and then an algorithm to minimize the loss. 
To comprehensively evaluate the DCLbased summarization,  we utilize the whole DUC corpora to test the system. 
Figure   7 uses a simple example to demonstrate the procedure of summary generation based on DCL. 
Following our work in DUC 2005 and 2006,  we proposed a document concept lattice(  DCL)  model and the corresponding algorithm for summarization. 
In (Liu et-al. 2015), the authors present following contributions
Multi document summarization is the process of generatinga short version of given materials to indicate its main ideas As the number of documents on the web exponentially increases,  text summarization has attracted a growth of attention since it can help people get the topic within a short time Most existing studies are extraction based methods. 
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time. 
malize the task of multi document summarization as an optimization problem according to the above properties,  and use simulated annealing algorithm to solve it. 
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization. 
In this paper,  we use the correlation of the least different summary sentence pairs to measure diversity Based on these three requisites,  we design a twolevelsparse representation model to tackle the multidocumentsummarization problem. 
Multi document summarization aims at reducing the long documents into short length sentences,  which helps readers quickly grasp the general information of the document set Though over the past 50 years,  the problem has been addressed from many different perspective in varying domain sand domains and using various paradigms,  it is still a nontrivial task There are two main approaches in text summarization. 
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 
In this paper,  we propose a novel model to tackle the problem of multi document summarization. 
In our model,  the task of multi document summarization is regarded as a document reconstruction problem which contains diversity naturally. 
The task of multi document summarization is to selectk } from the,  s candidate set which best describe the subjects. 
We observe that an effective multi document summarization should meet three key requirements. 
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 
In this section,  we describe the details of our proposed framework MDSSparse(  MultiDocument Summarization based on TwoLevel Sparse Representation Model)  The most challenging part is to properly model summaries. 
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem. 
Extensive experiments on summarization benchmark data sets DUC2006 and DUC2007 show that our proposed model is effective and outperforms the stateoftheartalgorithms. 
(  Conroyand O leary 2001)  model the problem of extracting a sentence from a document using hidden Markov model(  HMM)  Graph based models like PageRank(  Br in and Page 1998)  and HITS(  Klein berg 1999)  build similarity graph of sentences,  and use in uence propagation algorithms to give each sentence a score. 
As multi document texts often describe one central topic and some subtopics,  the sentences can be categorized into groups. 
Based on the assumption,  we think ago od summary should meet three key requirements. 
Intuitively,  multi document set always have one central topic and some subtopics,  indicating that the summary sentences should also be categorized into groups. 
Most existing systems use rank model to select the sentences with highest scores to form the summarization. 
But in real life,  a good summarization should not only contain the main ideas of the oft he whole topic,  it should conclude the whole document set documents et. 
Although some methods tried to reduce the redundancy(  Li et al   2009) ,  nding balance between wide coverage and minimum redundancy is anon trivial task In this paper,  an ideal summary is assumed to represent the whole document set,  namely,  by reading the summarization instead of the whole set one can understand the general idea of the original documents. 
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 
techniques is that they seem to ignore the redundancy and coverage in summarization Sparse coding is proved to be very useful in image processing(  denoting,  in painting,  super resolution). 
It was rst introduced into document summarization in(  He et al   2012). 
But they do not consider the sparsity of the summarization. 
Our experiments show that sparsity is critical for high quality summarizations. 
Here,  we denote S ∗ as summary set. 
Each document set consists of several articles written by various authors,  which is also the ground truth of the evaluation. 
Every sentence is either used in its entirety or not at all for constructing a summary. 
The length of a result summary is limited by 250 tokens(  white space whites pace delimited)  Evaluation Metric We use the Rouge(  Lin 2004)  evaluation toolkit,  which is adopted by DUC for automatic summarization evaluation. 
It measures summary quality by counting overlapping units such as the ngram,  word sequence sand sequences and word pairs between the candidate summary and the reference summary. 
This work is supported by National Natural Science Foundation of China(  Grant No   61170091). 
As mentioned in(  He et al   2012) ,  RougeNis de ned as follows. 
where n stands for the length of the ngram,  Ref is the set of reference summaries. 
In (Zhao et-al. 2009), the authors present following contributions
This paper presents a novel query expansion method,  which is combined in the graph based algorithm for query focused multi document summarization,  so as to resolve the problem of information limit in the original query. 
Query focused summarization Query expansion Graph based ranking. 
Inspired by the success of graph based ranking,  we propose an approach of combining query expansion in the graph based algorithm for query focused multi document summarization. 
Query focused multi document summarization(  i e,  topic focused multi document summarization)  is a particular kind of multi document summarization. 
In this section,  we give a brief overview of our query focused multi document summarization system,  as follows. 
Query focused multi document summarization has drawn much attention in recent years due to its applicability in real world applications. 
Wan et al  (  b)  proposed a graph based method for simultaneous text summarization and keyword extraction by using not only relationships between sentences,  but also relationships between words,  and relationships between words and sentences. 
In Section 3,  we present in detail our graph based summarization algorithm combined with query expansion. 
A variety of multi document summarization methods have been developed over the years,  most of which are extractive. 
Most extractive summarization methods score sentences based on features such as sentence position,  term frequency,  similarity with the document centroid(  Radev,  Jing,  Stys,  & Tam,  2004) ,  etc   These features can be combined using either weighted linear combination or machine learning algorithms such as Support Vector Machine,  Maximum Entropy or Conditional Random Field to get an overall ranking score for each sentence. 
As mentioned in Section 1,  motivated by the success of PageRanklike ranking algorithms in webpage ranking,  a number of graph based approaches have been applied to automatic text summarization. 
Mihalcea and Tarau(  2004)  independently proposed another graph based random walk model TextRank,  which is similar to LexRank,  except that it is applied to single document summarization,  and it does not consider similarity threshold in sentence extraction. 
Our data is from DUC3 2005 and DUC 2006 in which query focused multi document summarization is the only task. 
Compared to the previous query expansion methods used in text summarization that usually take word synonyms as expansions,  we select from the document set both informative and query relevant words based on the graph ranking results,  add them into the original query and use the updated query to perform graph ranking again. 
In this paper,  we propose a query expansion algorithm used in the graph based ranking approach for query focused multi document summarization. 
Our query focused multi document summarizer employs both graph based sentences ranking and sentencetoword relations to implement query expansion,  and then uses the expanded query to further improve the graph ranking process. 
Recently,  graph based ranking algorithms have been successfully used in text summarization(  Erk an and Radev,  2004,  Mihalcea and Tarau,  2004; Wan et al,  a,  Wan et al,  b). 
The query analysis is also investigated by a number of systems to either extract key words from the query based on the word weight calculated in different ways or analyze the question type of a query if there exists,  which indicates what kind of information the query is looking for(  Li et al,  2005). 
Due to these limitations,  we decided to expand the query based on the information in the document cluster rather than the external resources. 
Other typical query focused summarization methods such as maximal marginal relevance(  MMR)  combine query relevance with information novelty in text summarization(  Carbon ell and Gold stein,  1998,  Gold stein et al,  2000). 
Based on these requirements,  most query focused summarizers are extended from generic summarizers by incorporating query related features,  such as the similarity between a sentence and the query. 
Use a graph based ranking algorithm to rank all the sentences in the documents where the original query is used. 
The main difference between generic summarization and query focused summarization is that for the latter the information expressed in the summary should be not only informative and novel as much as possible,  but also biased towards a specific user query. 
Our future work will be applying the query expansion to other summarization approaches and furthermore,  employing machine learning algorithms to automatically learn the parameters. 
When applied to the task of text summarization,  these approaches can make full use of the relationships between sentences and find the most important sentences to be extracted into the summary. 
Compared to generic summarization,  query focused summarization requires the summary biased to a specific query besides the general requirement for a summary. 
In contrast to the task of question answering(  QA)  that mainly focuses on simple factoid questions and results in precise answers such as person,  location or date,  etc,  in the case of query focused summarization,  the queries are mostly real world complex questions(  e g,  what impact has the Chunnel had on the British economy and the life style of the British is a query example). 
Such complex questions make the summarization task more difficult and the big challenge is how to understand the question well and thus bias the answer towards it. 
Perform our query expansion method based on the ranking results in step 1,  and update the query. 
Use the newly expanded query to perform graph based ranking algorithm again for sentence ranking. 
We use the graph based ranking algorithm similar to the topic sensitive LexRank in Otterbacher et al  (  2005)  as our first step to rank sentences in the documents,  which is also used for comparison as our baseline without query expansion. 
Currently most of the query focused summarization systems base their work on generic summarization systems. 
The initial focus on the query focused task was SUMMAC which was the first large scale,  developer independent evaluation of automatic text summarization systems(  Mani et al,  1998) ,  and it was then replaced by the DUC evaluation which was mainly for the generic summarization at its early stage. 
Recently much more focus has been put on the query focused summarization task compared to the generic one. 
In (Aliguliyev 2009), the authors present following contributions
www summarization com mead)  is an implementation of the centroid based method for either single or multi document summarizing. 
In paper Wan,  Yang,  and Xiao(  2007)  proposed a novel extractive approach based on manifold ranking of sentences to query based multi document summarization. 
The article Fung and Ngai(  2006)  presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story flow). 
Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users,  and multi document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. 
In our study we focus on sentence based extractive document summarization. 
We propose the generic document summarization method which is based on sentence clustering. 
The proposed approach is a continue sentence clustering based extractive summarization methods,  proposed in Alguliev Alguliev,  R. 
A novel partitioning based clustering method and generic document summarization. 
Summarization of text based documents with a determination of latent topical sections and information rich sentences. 
Text summarization is the process of automatically creating a compressed version of a given text that provides useful information to users,  and multi document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic(  Wan,  2008). 
The proposed sentence clustering based approach for generic single document summarization is presented in Section 3. 
Systems for extractive summarization are typically based on technique for sentence extraction,  and attempt to identify the set of sentences that are most important for the overall understanding of a given document. 
Sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive summaries. 
The centroid based method(  Erk an and Radev,  2004,  Radev et al,  2004)  is one of the most popular extractive summarization methods. 
On the other hand,  summarization task can also be categorized as either generic or query based. 
In paper Guo and Styli os(  2005)  is introduced an intelligent system,  the event indexing and summarization(  EIS)  system,  for automatic document summarization,  which is based on a cognitive psychology model(  the event indexing model)  and the roles and importance of sentences and their syntax in document understanding. 
In centroid based summarization,  the sentences that contain more words from the centroid of the cluster are considered as central. 
We have presented the approach to automatic document summarization based on clustering and extraction of sentences. 
The proposed approach is a continue sentence clustering based extractive summarization methods,  proposed in Alguliev et al,  2005,  Aliguliyev,  2006,  Alguliev and Alyguliev,  2007,  Aliguliyev,  2007. 
We propose the generic document summarization method which is based on sentence clustering. 
In our study we focus on sentence based extractive summarization. 
supervised techniques that rely on preexisting document summary pairs,  and unsupervised techniques,  based on properties and heuristics derived from the text. 
In the past,  extractive summarizers have been mostly based on scoring sentences in the source document. 
The purpose of present paper to show,  that summarization result not only depends on optimized function,  and also depends on a similarity measure. 
Results of experiment have showed that proposed by us NGDbased dissimilarity measure outperforms the Euclidean distance. 
In paper(  Sh en,  Sun,  Li,  Yang,  & Chen,  2007)  each document is considered as a sequence of sentences and the objective of extractive summarization is to label the sentences in the sequence with 1 and 0,  where a label of 1 indicates that a sentence is a summary sentence while 0 denotes a non summary sentence. 
The summarization techniques can be classified into two groups. 
In this paper we also demonstrated that the summarization result depends on the similarity measure. 
The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. 
A query based summary presents the information that is most relevant to the given queries(  Dunlavy et al,  2007,  Fisher and Roark,  2006,  Li et al,  2007,  Wan,  2008)  while a generic summary gives an overall sense of the document s content(  Alguliev and Aliguliyev,  2005,  Alguliev et al,  2005,  Aliguliyev,  2006,  Alguliev and Alyguliev,  2007,  Aliguliyev,  2007,  Dunlavy et al,  2007,  Gong and Liu,  2001,  Jones,  2007,  Li et al,  2007,  Salton et al,  1997,  Wan,  2008). 
In (Glavaš G, Šnajder J 2014), the authors present following contributions
The extractive multi document summarization model selects sentences based on the relevance of the individual event mentions and the temporal structure of events. 
Building on these insights,  we present a multi document summarization model that filters the text based on the relevance of event mentions. 
In Section 4,  we present and evaluate the event centered IR model based on event graphs and graph kernels,  while in Section 5 we present and evaluate the event centered multi document summarization model. 
There are a number of approaches to multi document summarization that first semantically annotate the sentences and then assign relevance scores to sentences based on these annotations,  e g. 
Event based multi document summarization was first proposed by Daniel et al  (  2003)  who,  following the TDT definition of an event(  Allan,  2002) ,  selected sentences based on relevance for one or more sub events of the topic at hand. 
Building on event graphs,  we present novel models for information retrieval and multi document summarization. 
Building on event graphs,  we propose novel models for event centered information retrieval and multi document text summarization. 
Experimental evaluation shows that our retrieval model significantly outperforms well established retrieval models on event oriented test collections,  while the summarization model outperforms competitive models from shared multi document summarization tasks. 
Nevertheless,  studies on event based text summarization are rare(  Daniel,  Radev,  & Allison,  2003; Filatova & Hatzivassiloglou,  2004; Li,  Wu,  Lu,  Xu,  & Yuan,  2006). 
This dearth of studies is rather surprising if one considers that news stories primarily describe real world events(  i e,  an event is a dominant information concept in news) (  Pan & Kosicki,  1993; Van Dijk,  1985)  and that following an event through several news wires is a prototypical application of multi document summarization(  Barzilay,  McKeown,  & Elhadad,  1999). 
The third contribution of this article is a novel event centered multi document summarization model. 
This article aimed to bridge this gap and addressed event centered retrieval and summarization based on sentence level event extraction. 
We demonstrate that our models achieve significant improvements over well established models on event centered IR tasks as well as over competing methods for multi document summarization. 
The model selects sentences for the summary based on the estimated relevance of event mentions computed using event graphs,  and it achieves a significant performance gain over competitive text summarization methods. 
Similarly,  existing multi document summarization models do not specifically account for the semantics of sentence level events. 
The event centered multi document summarization employs event graphs to assign relevance scores to the individual event mentions and then exploits the structure of event graphs to propagate the relevance to temporally related events. 
The results obtained on the multi document summarization tasks prove that both events and temporal relations between them are useful for recognizing the most relevant information within a group of topically related documents. 
Canhasi and Kononenko(  2014)  propose a query focused graph based multi document summarization in which,  in addition to links denoting the similarity between sentences,  they introduce links between sentences and the query denoting how similar individual sentences are to the query. 
In this work,  we present an extractive multi document summarization model based on the extraction of sentence level event mentions and the temporal structure of documents. 
In this work,  we aim to bridge that gap,  and we propose event oriented retrieval and summarization models based on sentence level event extraction. 
Because our event centered summarization model performs extractive non focused multi document summarization of documents in English,  we perform the evaluation on data sets that conform to the same criteria. 
Clustering of sentences for the purpose of removing redundancy is a common step in multi document summarization(  Christensen,  Mausam,  & Etzioni,  2013; Saggion & Gaizauskas,  2004). 
We believe that the presented models make an important contribution to information retrieval and multi document summarization. 
The non focused multi document summarization tasks for English are the DUC2002 Extractive Summarization Task(  Over & Liggett,  2002) (  token summaries)  and the DUC2004 Task 2(  Over & Yen,  2004) (  token summaries). 
Conceptually,  their approach is similar to the event based summarization model we propose in this article. 
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
To construct the event graphs,  we combine machine learning and rule based models to extract sentence level event mentions and determine the temporal relations between them. 
In this article,  we present event graphs,  a novel event based document representation model that filters and structures the information about events described in text. 
Third,  we provide an overview of event based approaches to text summarization. 
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
In (Hong et-al. 2015), the authors present following contributions
We present a novel framework of system combination for multi document summarization. 
Second,  even though many systems have been proposed for multi document summarization,  the output of them are often available only on one data set or even unavailable. 
System combination in summarization has also been regarded as rank aggregation,  where the combined system re ranks reran ks the input sentences based on the ranks of those sentences assigned by the basic systems. 
The tasks include generic(  DUC 2001–2004)  and query focused(  TAC 2008,  2009)  multi document summarization. 
Traditionally in summarization,  features are derived based on the input(  denoted as I). 
A handful of papers have studied system combination for summarization. 
Based on the ranks of the input sentences assigned by different systems(  i e,  basic systems) ,  methods have been proposed to re rank these sentences(  Wang and Li,  2012; Pei et al,  2012). 
Recent work shows that stateoftheart summarization systems generate very different summaries,  despite the fact that they have similar performance(  Hong et al,  2014). 
This method selects among the outputs of the basic systems,  based on their overlaps with the input in terms of DG. 
Our model performs better than the systems that we combined based on manual and automatic evaluations. 
However,  only a handful of papers have leveraged this idea for summarization. 
These features are derived not only based on the input,  but also based on the basic summaries and the summary input pairs from the New York Times(  NYT)  corpus(  Sandhaus,  2008). 
Thapar et al  (  2006)  combine the summaries from different systems,  based on a graph based measure that computes summary input or summary summary similarity. 
The most closely related papers are the ones that investigated automatic evaluation of summarization without human references(  Louis and Nenkova,  2009; Saggion et al,  2010) ,  where the effectiveness of several summary input similarity metrics are examined. 
The baselines that only consider the consensus between different systems perform poorly(  voting,  summarization on summaries,  JSH). 
Our model performs extractive summarization,  but still has similar R compared to theirs On the TAC 09 data,  the best system uses a supervised method that weighs bi grams big rams in the ILP framework by leveraging external resources(  Li et al,  2015). 
A similar approach has been used to generate candidate summaries for single document summarization(  Ceylan et al,  2010). 
Therefore,  effective system combination appears to be difficult using methods based on a single indicator. 
Based on this fact,  the combined summary should include sentences that appear in the summaries produced by different systems. 
It also has a slightly lower R and a higher R compared to ClusterCMRW(  Wan and Yang,  2008) ,  a graph based system that achieves the highest R on the DUC 02 data. 
These features capture content importance from different perspectives,  based on different sources. 
The four systems all perform extractive summarization,  which directly selects sentences from the input. 
In order to find a better learning method,  we have experimented with support vector regression(  SVR) (  Drucker et al,  1997)  6 and SVMRank(  Joachims,  1999)  .7 SVR has been used for estimating sentence(  Ouyang et al,  2011)  or document(  Aker et al,  2010)  importance in summarization. 
This method solicits annotators to score a summary based on its coverage of summary content units,  which are identified from human references. 
Rather,  their evaluations compare the summaries to the input based on the overlap of DG. 
Thapar et al  (  2006)  propose to iteratively include sentences,  based on the overlap of DG between the current sentence and(  1)  the original input,  or(  2)  the basic summaries. 
R and R are the most widely used metrics in summarization literature. 
In (Huang et-al. 2010), the authors present following contributions
The encouraging results indicate that the multi objective optimization based framework for document summarization is truly a promising research direction. 
Most of the stateofchart summarization systems are based on extracting the most salient and non redundant sentences to composite the final summaries. 
Document summarization,  especially multi document summarization in essence is a multi objective optimization problem. 
With the vast amount of information from the WWW,  multi document summarization. 
The potential of optimization based document summarization models has not been well explored to date. 
In the following sections,  we will first present the multi objective optimization model for the task of multi document summarization,  and then we present the strategies which are adopted in the solution of the optimization problem. 
The query oriented multi document summarization task defined in DUC aims to producing a concise and well organized summary for a set of relevant documents and a query that reflects the user s information need. 
It is worth noting that a real optimization based summarization method is different from the existing non optimization based methods in two remarkable aspects. 
In this paper,  we consider document summarization as a multi objective optimization problem involving four objective functions,  namely information coverage,  significance,  redundancy and text coherence. 
These functions measure the possible summaries based on the identified core terms and main topics(  i e. 
Section 3 introduces query sensitive term ranking and clustering and proposes an optimization model based on it. 
Different from previous work which focused on single objective optimization,  we explicitly define multiple summary evaluation criteria and formulate them as separate objective functions which are measured based on query sensitive core terms and main topics built from core term clusters. 
We choose the DUC 2005 and 2006 query oriented summarization tasks to exam the proposed model. 
Section 2 reviews existing optimization methods for document summarization. 
The main evaluation forum providing opportunities for researchers to exchange their ideas and experiences in document summarization is the Document Understanding Conference(  DUC). 
They measure the possible summaries based on the identified core terms and main topics(  i e. 
Three algorithms were presented and evaluated,  including a greedy approximate method,  a dynamic programming approach based on solutions to the knapsack problem,  and an exact algorithm that used an integer linear programming formulation of the problem. 
Then,  an algorithm based on stack decoder was developed to search for the best combination of the sentences that maximized the sum of the scores subject to the length constraint. 
In this study,  we consider document summarization as a multi objective optimization problem. 
As far as we know,  the idea of optimizing summarization was mentioned as early as 2004 in. 
,  the summarization task was defined as a global inference problem which attempted to optimize three properties jointly,  i e,  relevance(  or significance) ,  redundancy and length. 
Otherwise,  the optimization solutions will degenerate to the traditional non optimization based(  e g. 
} 1 2 n T = t t t,  from documents D based on term co occurrence statistics provided with the query context. 
Let S be the generated summary containing the sentence s,  s,  …,  sn,  we formally define the above document summarization requirements as the following multi objective optimization problem. 
Consequently,  the objective functions are then formulated based on T and C. 
In (Liu et-al. 2009), the authors present following contributions
The proposed approach to query based multi document summarization is evaluated by following the main task of DUC07. 
It can be said that our approach has achieved a stateoftheartperformance in query based multi document summarizations. 
In aquerybased summarization setting,  the correlation between user queries and sentences to be scored is established from both the micro(  i e. 
The experiments,  both on ageneric single document summarization evaluation,  and on a querybasedmultidocument evaluation,  verify the e ectiveness of the proposed measures and show that the proposed approach achieves a stateoftheartperformance. 
generic single generics ingle document summarization and query based multi document summarization. 
A crucial problem in the sentence selection phase is how to eliminate information redundancy between sentences to be included in a summary,  which is especially important in a multi document summarization system. 
By contrast,  those producing summaries relevant to user queries are called query based summarization. 
from the micro perspective - at word level,  and from the macro perspective at sentence level(  or at word string level) ,  either absence will cause the performance drop and(  c)  We have incorporated linguistic knowledge,  and presented aquery expansion method based on an existing ontology,  achieving an observable performance improvement in the proposed summarization approach. 
Based on this assumption,  we propose a new word signi cance measure and further present novel sentence ranking formulas for extractive summarization. 
Therefore,  we propose an automatic document summarization approach using a new word signi cance measure based on word information The rest of the paper is organized as follows. 
Each document is equivalent to a network of information,  and each piece of information is associated with one content bearing word or words Based on this assumption,  the issue of summarizing a text becomes the issue of nding the set of text segments with the largest quantity of information undera particular length constraint. 
The experiments,  both on the task 1 of DUC02 which focuses on generic single document summarizing,  and on the main task of DUC07 whose goal is to produce a query based summary for a set of related documents,  illustrate that our proposed approaches achieve a stateoftheart performance Particularly noteworthy about our approach are the following points. 
Section 2 presents a review of related work on summarization,  providing readers with a comprehensive scenario. 
Automatic text summarization can save people time and e ort in acquiring knowledge from a single document or a collection of texts. 
(  a)  We rede Were de ne a new qu anti cation measure of word importance in NLP tasks,  and successfully apply it to document summarization(  b)  We have proposed a novel solution to the issue of correlating sentences to be ranked with user queries. 
The proposed approach is described in Section 3,  followed by evaluations both on a generic single document setting,  and on a query based multidocumentsetting in Section 4. 
Research on summarization can be traced back to Luhn. 
Document summarization can be viewed as a reductive distilling of source text through content condensation,  while words with high quantities of information are believed to carry more content and thereby importance. 
Based on this proposed word information qu anti cation measure,  a sentence ssigni cance value for a generic summarization is computed using(  5)  where m is the number of words in sentence s,  I(  w)  is set to 1 when w is a keyword,  otherwise,  info(  w)  is equivalent to info(  w)  in(  2)  when w is a keyword,  otherwise When the documents to be processed are news articles,  a feature about sentence. 
The generic single document summarizer based on(  6)  is compared with the one in Wan et al. 
In this paper,  we propose a new qu anti cation measure for word signi cance used in natural language processing(  NLP)  tasks,  and successfully apply it to an extractive text summarization approach. 
Then,  an iterative reinforcement approach is employed to simultaneously conduct document summarization and keyword extraction. 
Normally,  the formula assigns a score to each sentence to represent its signi cance based on a set of features such as sentence position,  sentence length,  rhetorical structure. 
Accordingly,  the reliability there liability of statistical calculations based on a document is not comparable to that tot hat on a language The proposed measure given in(  2)  seems similar in form to the cross entropy In information theory,  the cross entropy between two discrete probability distributions p and q is de ned as(  3) ,  which measures the average number of bits needed to identify an event from a set of possibilities However,  its essential di erence from(  2)  lies in that the two involved possibilitiesp and q must distribute over the same probability space. 
,  is a package widely used in recent document summarization evaluations such as DUC.ROUGE scores have been shown to correlate very well with manual evaluations,  at least in the news article domain. 
In (Cao et-al. 2015a), the authors present following contributions
We develop a Ranking framework upon Recursive Neural Networks(  R2N2)  to rank sentences for multi document summarization. 
Experiments on the DUC 2001,  2002 and 2004 multi document summarization data sets show that R2N2 outperforms stateoftheart extractive summarization approaches. 
We conduct extensive experiments on the DUC 2001,  2002 and 2004 multi document summarization data sets. 
Recently,  treating multi document summarization as a sub modular maximization problem has attracted a lot of interest(  Si pos Sip os,  Shivaswamy,  and Joachims 2012; Dasgupta,  Kumar,  and Ra vi 2013). 
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 
This paper presents a Ranking framework upon Recursive Neural Networks(  R2N2)  for the sentence ranking task of multi document summarization. 
R2N2 makes three contributions to multi document summarization. 
Graph based models play a leading role in the summarization area. 
LexRank(  Erk an and Radev 2004)  is a popular stochastic graph based summarization approach,  which computes sentence importance grounded on the concept of eigenvector centrality in a graph of sentence similarities. 
Here we focus on the generic multi document summarization task,  which was carried out in DUC 2001,  2002 and 2004. 
Meanwhile,  LexRank(  Erk an and Radev 2004) ,  a common graph based summarization model,  is introduced as an extra baseline. 
Graph based methods have a lot of extensions. 
In contrast to these unsupervised approaches,  there are also numerous efforts on supervised learning for summarization where a model is trained to predict the importance. 
Then regression is processed at all the non terminal nodes,  based on the learned features concatenating these handcrafted ones. 
For example, (  Wan and Yang 2008)  pairs graph based methods with clustering. 
We adopt this baseline to display the performance of regression approaches based on these features. 
We take ROUGE2 recall as the main metric for comparison due to its high capability of evaluating automatic summarization systems(  Owczarzak et al   2012). 
Extractive summarization(  Over and Yen 2004)  aims to generate a short text summary for a document or a set of documents through selecting salient sentences in the document(  s). 
Greedy based Sentence Selection In each step of selection,  the sentence with maximal salience is added into the summary,  unless its similarity with a sentence already in the summary exceeds a threshold. 
The other is integer linear programming(  ILP) (  Hu and Wan 2013)  based sentence selection. 
ILP based Sentence Selection Similar to previous work(  McDonald 2007; Gil lick and Favre 2009; Gal anis,  Lampouras,  and Androutsopoulos 2012) ,  we also consider the way of ILP to select sentences. 
Sentence ranking has been extensively investigated in extractive summarization. 
Generally,  there are two major components in extractive summarization systems,  namely sentence ranking and sentence selection. 
In addition,  based on the ranking scores of both words and sentences,  we design a optimized sentence selection method. 
In (Yang et-al. 2014), the authors present following contributions
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information. 
We conduct a series of experiments on the DUC 2004 generic multi document summarization data set and the DUC2007 query based multi document summarization data set. 
Ranking based clustering Sentence clustering Theme based summarization. 
Most of the summarization work done till date is based on the sentence extraction framework,  which ranks sentences according to various prespecified criteria and then selects the most salient sentences from the original documents to form a concise summary. 
Section 4 presents ranking based sentence clustering framework and their application to multi document summarization. 
Thus,  good sentence clusters are the guarantee of good summaries in theme based summarization. 
Meanwhile,  in multi document summarization,  the number of documents to be summarized can be very large. 
This makes information redundancy appears to be more serious in multi document summarization than in single document summarization. 
Experimental results show that the ranking based clustering framework is able to generate more reasonable sentence clusters and in turn lead to better summarization performance. 
Recall that our goal is to obtain more accurate sentence clusters and generate good summaries in theme based summarization. 
to evaluate the performance of the generated summarization based on the generated clusters. 
where Score(  Ck)  is formulated as the normalized cosine similarity between a theme cluster and the whole document set for generic summarization,  or between a theme cluster and a given query for query based summarization. 
The figures demonstrate the significant role of the proposed ranking based clustering framework in summarization. 
The effectiveness of the proposed framework is demonstrated by both the cluster quality analysis and the summarization evaluation conducted on the DUC 2004 and DUC2007 data sets. 
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters. 
Automatic document summarization,  which is a process of reducing the size of documents while preserving the important semantic content,  is an essential technology to overcome this obstacle. 
While Q directly evaluates the quality of the generated clusters,  we are also interested to know whether the improved quality of clusters can further enhance the quality of sentence ranking and thus consequently raise the performance of summarization. 
Therefore,  we evaluate the ROUGEs in each iteration of ranking based clustering as well. 
The experimental results show that the framework is able to generate more reasonable sentence clusters that,  in turn,  lead to more meaningful summarization performance. 
Section 2 reviews related work on sentence clustering for summarization. 
(  3)  Thorough experimental studies using intrinsic cluster quality evaluation method and extrinsic summarization method are conducted to verify the effectiveness and robustness of the proposed approach. 
Then each sentence in these clusters will be readjusted based on the new measure. 
In (Almeida and Martins 2013), the authors present following contributions
We present a dual decomposition framework for multi document summarization,  using a model that jointly extracts and compresses sentences. 
Fast and accurate query based multi document summarization. 
For extractive summarization,  we used the DUC 2003 and 2004 data sets(  a total of 80 multi document summarization problems). 
Up to now,  extractive systems have been the most popular in multi document summarization. 
Sentence compression as a component of a multi document summarization system. 
Statistics based summarization step one. 
In this limit,  inference for task #1(  compressive summarization)  is based solely on the model learned from that task s data,  recovering the approach of BergKirkpatrick et al  (  2011). 
Multi document summarization via budgeted maximization of sub modular functions. 
Centroid based summarization of multiple documents. 
A study of global inference algorithms in multi document summarization. 
Coverage based extractive summarization can be formalized as follows. 
Multi document summarization by maximizing informative content words. 
The test partition contains 48 multi document summarization problems each provides 10 related news articles as input,  and asks for a summary with up to 100 words,  which is evaluated against four manually written abstracts. 
Compared with previous work based on integer linear programming,  our approach does not require external solvers,  is significantly faster,  and is modular in the three qualities a summary should have. 
The three tasks are instances of structured predictors(  Bak r et al,  2007) ,  and for all of them we assume feature based models that decompose over parts. 
We decode with AD3,  a fast and modular dual decomposition algorithm which is orders of magnitude faster than ILPbased approaches. 
sentence extraction,  utility based evaluation,  and user studies. 
The goal is to take advantage of existing data for related tasks,  such as extractive summarization(  task #2) ,  and sentence compression(  task #3). 
We presented a multitask learning framework for compressive summarization,  leveraging data for related tasks in a principled manner. 
In the NAACLANLP Workshop on Automatic Summarization. 
All approaches above are based on integer linear programming(  ILP) ,  suffering from slow run times,  when compared to extractive systems. 
A second inconvenience of ILPbased approaches is that they do not exploit the modularity of the problem,  since the declarative specification required by ILP solvers discards important structural information. 
In (CR92), the authors present following contributions
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS)  Speci cally,  a set of reader comments associated with the news reports are also collected. 
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS). 
In the typical multi document summarization(  MDS)  setting,  the input is a set of documents reports about the same topic event. 
We name such aparadigm of extension as reader aware multi document summarization(  RAMDS)  We give a real example taken from a data set collected by us to illustrate the importance of RAMDS. 
To tackle this RAMDS problem,  we propose a sparsecodingbasedmethod that is able to calculate the salience of the text units by jointly considering news reports and reader comments. 
However,  the problem setting of RAMDS is more challenging because the considered comments are about an event with multiple reports spanning a time period,  resulting in diverse and noisy comments To tackle the above challenges,  we propose a sparsecodingbased method that is able to calculate the salience of the oft he text units by jointly considering news reports and reader comments. 
Some previous works explore the effect of comments or social contexts in single document summarization(  such as blog summarization)  Hu et al,  Yang et al,  2011. 
To tackle this RAMDS problem,  we propose a sparsecodingbased method jointly considering news reports and reader comments. 
The rewriting consideration is jointly assessed together with other summarization requirements under a uni ed optimization model. 
Moreover,  many works Liu et al,  K ageb ack et al,  2014; De nil et al,  2014; Cao et al,  utilized deep learning techniques to tackle summarization tasks As more and more user generated content is available,  one natural extension of the setting is to incorporate such content regarding the event so as to directly or indirectly improve the generated summaries with greater user satisfaction. 
proposed an extraction based approach that employs a manifold ranking method to calculate the salience of each sentence. 
We take semantic units as sentences here,  and assume that for each sentence xi,  there is a coef cient variable ai,  named expressiveness score,  to represent the contribution of this sentence in the reconstruction Based on the spirit of sparse coding,  we directly regard each news sentence xi as a candidate basis vector,  and al lxi all xi s are employed to reconstruct the semantic space of the topic,  including X and Z. 
proposed a guided sentence compression framework to generate compressive summaries by training a conditional random eld(  CRF)  based on a annotated corpus. 
The similarity is calculated with the Jaccard Index based method Speci cally,  this objective maximizes the salience score o f. 
where the coef cient aj s are the expressiveness scores and all the target vectors share the same coef cient vector A here To harness the characteristics of the summarization problem setting more effectively,  we re ne the preliminary error formulation as given in Eq. 
Note that the rewriting consideration is conducted for different candidates for the purpose of the assessment of the oft he effects on summarization in the optimization framework Consequently,  no actual permanent rewriting operations are conducted during the optimization process. 
We propose a compression based uni ed optimization framework which explores a ner syntactic unit,  namely,  noun ver b. 
To tackle the RAMDS problem,  we propose an unsupervised compressive summarization framework. 
for a model similar to Filatova and Hatzivassiloglou,  based on the weighted sum of the concepts(  approximated bybigrams). 
A sparsecodingbasedmethod is proposed to reconstruct the semantic space of atopic,  revealed by both the news sentences i e,  xi s and the comment sentences i e,  zi s,  on the news sentences. 
After the above preparation steps,  we will introduce our summarization model in Section 2.5. 
In (Alguliev et-al. 2011), the authors present following contributions
In recent years,  a variety of graph based methods have been proposed for multi document summarization(  Erk an and Radev,  2004,  Otterbacher et al,  2009,  Radev et al,  2001,  Wan and Xiao,  2009,  Wan et al,  2007,  Wei et al,  2008,  Zhang et al,  2008,  Zhang et al,  2008,  Zhao et al,  2009). 
Wang,  Zhu,  Li,  and Gong(  2009)  proposed a Bayesian sentence based topic model(  BSTM)  for multi document summarization by making use of both the word document and word sentence associations. 
In Haghighi and Vanderwende(  2009)  have been presented an exploration of content models for multi document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics. 
The University of Michigan s summarization system,  named MEAD,  was initially developed to produce multi document extractive summaries. 
A transductive approach(  Amini & Usunier,  2009)  for extractive multi document summarization identifies topic themes within a document collection,  which help to identify two sets of relevant and irrelevant sentences to a question. 
The proposed model is quite general and can also be used for single and multi document summarization. 
We implemented our model on multi document summarization task. 
Paper(  Lee,  Park,  Ahn,  & Kim,  2009)  presents a novel generic document summarization method using the generic relevance of a sentence based on negative matrix factorization(  NMF). 
Tao et al  (  2008)  have designed word based and sentence based association networks(  WAN and SAN for short,  respectively)  and proposed word and sentence weighting approaches based on how much co occurrence information they contain,  and applied to text summarization. 
The pioneer work for diversity based text summarization is MMR(  maximal marginal relevance) ,  it was introduced by Carbon ell and Gold stein(  1998). 
Many approaches have been proposed for text summarization based on the diversity. 
In Yang and Wang(  2008) ,  a novel summarization model based on fractal theory has been presented. 
single and multi document summarization. 
Here,  we implemented our model on multi document summarization task. 
In our study we use the cosine similarity and the NGDbased similarity measure because summary to the document or the entire document cluster are supposed to be important in summarization(  Aliguliyev,  2009,  Aliguliyev,  2010,  Alguliev and Alyguliev,  2008,  Alguliev and Aliguliyev,  2009). 
In order to investigate how the relative contributions from the cosine measure and the NGDbased measure between sentences influence the summarization performance,  we propose to define the final objective function by linearly combing the objective function based on the cosine similarity measure and the objective function based on the NGDbased similarity measure as follows. 
Single document summarization can only condense one document into a shorter representation,  whereas multi document summarization can condense a set of documents into a summary. 
Multi document summarization can be considered as an extension of single document summarization and used for precisely describing the information contained in a cluster of documents and facilitate users to understand the document cluster. 
► We define the objective function by weighted combination of the two objective functions based on the cosine and the NGDbased similarity measures. 
In (Fang et-al. 2015), the authors present following contributions
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization(  abbreviated as TAOS). 
Of particular interest to us in this paper is the extractive multi document summarization,  where the obtained final summary is a subset of the elements from multiple input documents. 
Many extraction based summarization methods have been proposed in the past years. 
In most recently,  a large margin learning method is proposed for multi document summarization in. 
for extractive multi document summarization. 
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
SubSVM conducts structural SVM and sub modular function to perform multi document summarization and is shown to outperform other supervised methods. 
abstract based and extraction based. 
The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining. 
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 
Documents summarization can be generally categorized as two approaches. 
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 
Therefore,  in this paper,  we attempt to devise an approach to the generation of the topic aspect oriented summarization according to topic factors,  we call this TAOS(  Topic AspectOriented Summarization). 
BagofVisualWord is a image representation based on BoW(  BagofWord)  model. 
For image summarization task,  we also extract three kinds of features for each image. 
the multi documents to be summarized,  the length of summary L,  and the learned parameters. 
The greedy algorithm initializes an empty summarization and in each step,  the sentence(  or image)  which can maximize the scoring function is added into the summarization. 
We compare our proposed approach with some stateoftheart methods on DUC2003,  DUC2004 data sets for text summarization and NUSWide data set for image summarization. 
In (Sipos et-al. 2012), the authors present following contributions
In this paper,  we present a supervised learning approach to training sub modular scoring functions for extractive multi document summarization. 
The learning method applies to all sub modular summarization methods,  and we demonstrate its effectiveness for both pairwise as well as coverage based scoring functions on multiple data sets. 
In this paper we focus on extractive multi document summarization,  where the final summary is a subset of the sentences from multiple input documents. 
A popular stochastic graph based summarization method is LexRank(  Erk an and Radev,  2004). 
A current stateoftheart method for document summarization was recently proposed by Lin and Bilmes(  2010) ,  using a sub modular scoring function based on inter sentence similarity. 
To illustrate the generality of our approach,  we also provide experiments for a coverage based model originally developed for diversified information retrieval(  Swaminathan et al,  2009). 
This ability to easily train summarization models makes it possible to efficiently tune models to various types of document collections. 
In particular,  we find that our learning method can reliably tune models with hundreds of parameters based on a training set of about 30 examples. 
Due to the diminishing returns property of monotone sub modular set functions and their computational tractability,  this class of functions provides a rich space for designing summarization methods. 
Work on extractive summarization spans a large range of approaches. 
Later it was extended(  Gold stein et al,  2000)  to support multi document settings by incorporating additional information available in this case. 
The HMM is assumed to produce the summary by having a chain transitioning between summarization and non summarization states(  Conroy and O leary,  2001)  while traversing the sentences in a document. 
Another clustering based algorithm(  Nomoto and Matsumoto,  2001)  is a diversity based extension of MMR that finds diversity by clustering and then proceeds to reduce redundancy by selecting a representative for each cluster. 
Another approach based on sub modularity(  Qazvinian et al,  2010)  relies on extracting important key phrases from citation sentences for a given paper and using them to build the summary. 
This paper presented a supervised learning approach to extractive document summarization based on structual SVMs. 
It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. 
They use the diversified retrieval method proposed in Yue and Joachims(  2008)  for document summarization. 
Their work focuses on learning a particular compression model based on ILP inference,  while our work explores learning a general and large class of sentence selection models using sub modular optimization. 
In (0067), the authors present following contributions
Another reason why SC is so popular is its potential utility for extractive text summarization,  single or multi document(  Mani,  2001). 
It is this multi document summarization scenario which motivates our work. 
A well known challenge for extractive multi document summarization systems is to produce non redundant summaries. 
We considered the task of generating a short informative summary for a set of related sentences,  called multi sentence compression,  which arises naturally in the context of multi document text summarization. 
We consider the task of summarizing a cluster of related sentences with a short sentence which we call multi sentence compression and present a simple approach based on shortest paths in word graphs. 
• We present a simple and robust word graph based method of generating succinct compressions which requires as little as a part of speech tagger and a list of stop words. 
In the case of sentence compression being used for text summarization,  one disposes of a rich context to identify important words or phrases. 
In particular,  we use the Viterbi algorithm to find the sequence of words of a predefined length n which maximizes the bi gram big ram probability(  MLEbased). 
The article initial sentence is known to provide a good summary of the article and has become a standard competitive baseline in summarization. 
Two challenges of SC as well as text summarization are(  i)  important content selection and(  ii)  its readable presentation. 
Furthermore,  based on the feedback from a preliminary evaluation,  we provided an example in which we made clear that summaries consisting of a few phrases which cannot be reformulated as a complete sentence(  e g,  Early Monday a U.S. 
To date the work on sentence fusion is completely dependency syntax based. 
In the future,  it would be of interest to compare our method with a syntax based fusion method. 
(  ii)  such sentence clusters are noisier than what one would expect in a summarization pipeline. 
We implement the Kshortest paths algorithm to find the fifty shortest paths from start to end using the weighting function in(  4). 
We filter all the paths which are shorter than eight words and which do not pass a verb node. 
Finally,  we re rank the remaining paths by normalizing the total path weight over its length. 
This way we obtain the path which has the lightest average edge weight. 
In (Radev et-al. 2001), the authors present following contributions
Centroid based summarization of multi document clusters puts special emphasis on portions of these documents that are most central to the topic of the entire cluster. 
In this paper,  we present our recent work on the development of a scalable personalized webbasedmultidocument summarization and recommendation system. 
We will introduce an e ective search engine to summarize clusters of related web pages which provide more contextual and summary information to help users explore the retrieval result more e ciently We describe in this paper a system,  WebInEssence,  which blends the traditional information retrieval technology with advanced document clustering,  document recommendation,  and multi document summarization technology in an integrated framework Text summarization is the process of selecting the most salient information in one or more textual documents. 
Summarization relies on the principles of text redundancy and unevenly distributed information content to extract highly informative snippets(  often keywords or sentences)  from a document which can be read by the user in lieu of the original document(  s)  In WebInEssence,  we have adopted an approach to summarization of multiple documents called centroid based summarization(  Radev et al,  2000). 
Multi document summarization is also not supported by their system. 
Generic Search + SummarizationThe system can also enable users to hand select the documents(  URLs)  that they are interested in and perform single document or multi document summarization. 
The multi document summarization algorithm attempts to identify these themes and to identify the most salient passages from the selected documents using a cluster centroid which is computed automatically from the entire list of hits selected by the user. 
multi document summarization is automatically applied to each cluster to generate the summary. 
In this paper,  we presented what we believe is the rst multi document summarization and document recommendation system(  called WebInEssence)  deployed on the Web. 
When the input consists of more than one document,  we talk about multidocumentsummarization. 
They use MetaCrawler to performwebbased search and automatically generate summaries for each URL retrieved. 
They only support single supports ingle document summarization in their engine and the compression rate of the summarizer is also noncustomizable. 
We address some of the design issues to improve the scalability and readability of our multi document summarizer included in WebInEssence. 
• Caching documents All documents retrieved and indexed by WebInEssence are cached locally and the local copies are used in the summarization process unless these documents have changed on the Web. 
Ne to et al   describes a text mining tool that performs document clustering and text summarization They used the Auto class algorithm to perform document clustering and used TF*ISF(  an adaptation of TF*IDF)  to perform sentence ranking and generate the summarization output(  Ne to et al,  2000)  Our work is di erent from theirs in that we perform personalized summarization based on the retrieval result from a generic personalized web based search. 
We not only support both single and multiple document summarization,  but also allow the user to specify the summarization compression ratio as well as to get per cluster summaries of automatically generated clusters,  which,  we believe,  are more valuable to online users and give them more exibility and control of the summarization results. 
In (Ferreira et-al. 2014), the authors present following contributions
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
As already mentioned,  multi document summarization can be classified into generic and query based summarization. 
This paper presents a multi document summarization system that concisely extracts the main aspects of a set of documents,  trying to avoid the typical problems of this type of summarization. 
Both works(  Canhasi and Kononenko,  2014,  Luo et al,  2013)  are only suitable for query based multi document summarization. 
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
Hence,  anti redundancy methods are crucial in multi document summarization(  Atkinson & Munoz,  2013). 
TS can also be classified according to the number of documents simultaneously analyzed as single and multi document summarization(  Nenkova & McKeown,  2012). 
Multi document summarization Extractive summarization Sentence clustering. 
It is important to stress that the DUC 2002 data set is still the most widespread benchmark used today for multi document summarization analysis and that to the best of the knowledge of the authors of this paper no other summarization system surpassed the performance figures published in NIST(  2002). 
Multi document summarization addresses the problems of text overload,  as many documents share similar topics(  Atkinson & Munoz,  2013). 
The same techniques used in single document summarization systems apply to multi document ones in multi document summarization some issues as the degree of redundancy and information diversity increase,  however. 
Generic summarization systems extract the main ideas from a text collection,  while query based ones select sentences related to a specific query performed by the user. 
Currently,  query based summarization has drawn a higher degree of interest in the community,  due to its immediate applicability in commercial systems such as automatic customer services. 
For instance,  PRCN(  Luo et al,  2013)  is statistical framework to find relevance,  coverage and novelty in multi document summarization. 
Alguliev and his collaborators(  Alguliev,  Aliguliyev,  & Mehdiyev,  2013)  propose a generic document summarization method which is based on sentence clustering. 
In (Sarkar 2010), the authors present following contributions
This paper presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system. 
Centroid based multi document summarization. 
In another approach to multi document summarization. 
For managing a vast hoard of information,  summarization can be a useful means because users can decide about the relevance of an individual document using just summary information and multi document summaries can also enable users to identify the main theme(  central idea)  of a cluster of texts very rapidly. 
Information Overload,  Multi document summarization,  Natural Language Processing,  Syntactic Trimming. 
Multi document summarization is a process,  which produces a condensed representation of the contents of multiple related text documents collected from heterogeneous sources for human consumption and facilitates very rapid assimilation by human beings of the main points from related documents. 
The major steps of the extractive multi document summarization are. 
Redundancy is one of the important issues in multi document summarization. 
In extractive multi document summarization task,  summary sentences come from multiple source documents,  and picking sentences out of context may result in an incoherent summary. 
,  where authors have shown an improvement in the performance of the multi document summarization through sentence trimming,  but comparisons with stateofthe art summarization approaches have not been presented. 
The most summarization systems are either(  1)  sentence extraction based or(  2)  using sentence extraction as a primary component of a system. 
presents a pilot study on improving the sentence extraction based summarization performance by sentence compression. 
For managing a vast hoard of online or offline information,  summarization can be the useful means because the users can decide about the relevance of an individual document or a document cluster using just summary information. 
Multi document summaries can enable the users to identify the main theme(  central idea)  of a cluster of texts very rapidly. 
TopK sentences are then selected based on a compression ratio. 
In (Goldstein et-al. 2000), the authors present following contributions
Multi document summarization differs from single in that the issues of compression,  speed,  redundancy and passage selection are critical in the formation of useful summaries. 
This paper discusses a text extraction approach to multi document summarization that builds on single document summarization methods by using additional,  available information about the document set as a whole and the relationships between the documents. 
It builds upon previous work in single document summarization and takes into account some of the major differences between single document and multi document summarization. 
In future work,  we will integrate work on multi document summarization with work on clustering to provide summaries for clusters produced by topic detection and tracking. 
We will also investigate how users can effectively use multi document summarization through interactive interfaces to browse and explore large document sets. 
Following is a list of requirements for multi document summarization. 
Multi document summarization - capable of summarizing either complete documents sets,  or single documents in the context of previously summarized ones - are likely to be essential in such situations. 
Articles often have errors(  such as billion reported as million,  etc)  ; multi document summarization must be able to recognize and report source inconsistencies. 
This paper discusses an approach to multi document summarization that builds on previous work in single document summarization by using additional,  available information about the document set as a whole,  the relationships between the documents,  as well as individual documents. 
The co reference problem in summarization presents even greater challenges for multi document than for single document summarization(  Baldwin and Morton,  1998). 
Some of these approaches to single document summarization have been extended to deal with multi document summarization(  Mani and Bloedern,  1997; Gold stein and Carbon ell,  1998; TIPSTER,  b Radev and McKeown,  1998; Mani and Bloedorn,  1999; McKeown et al,  .. 
This section discusses our current implementation of a multi document summarization system which is designed to produce summaries that emphasize"  relevant novelty"  Relevant novelty is a metric for minimizing redundancy and maximizing both relevance and diversity. 
In the previous sections we discussed the requirements and types of multi document summarization systems. 
In order to construct these data sets,  we attempted to categorize user'  s information seeking goals for multi document summarization(  see Section 3). 
In (Baralis et-al. 2012), the authors present following contributions
Multi document summarization,  Text mining,  Frequent item set items et mining. 
In the context of multi document summarization,  the selection of the most relevant and not redundant sentences belonging to a collection of textual documents is definitely a challenging task. 
In the context of multi document summarization,  a summary is composed of the most representative sentences belonging to a document collection. 
Although it has been widely used in transactional data analysis,  to the best of our knowledge,  its exploitation in document summarization has never been investigated so far. 
This paper presents a novel multi document summarizer,  namely ItemSum(  Itemsetbased Summarizer) ,  that is based on an itemsetbased model,  i e,  a model composed of frequent item sets,  extracted from the document collection. 
Since it pushes the item set items et selection into the mining process,  it is particularly suitable for being applied in text summarization. 
To the best of our knowledge,  this is the first attempt to exploit frequent item sets in text summarization. 
A parallel research effort has been devoted to formalizing the summarization task as a maximum coverage problem with Knapsack constraints based on sentence relevance within each document. 
It automatically selects the most representative and not redundant sentences to include in the summary by considering both sentence coverage,  with respect to a concise and highly informative itemsetbased model,  and a sentence relevance score,  based on tfidf statistics. 
In this paper we present a multi document summarizer that combines the knowledge provided by an itemsetbased model with a statistical evaluator,  based on tfidf statistics,  to select the most representative and not redundant sentences. 
Summarization is a challenging data mining task that focuses on constructing a succinct and informative description of a data collection. 
To better discriminate among single word occurrences within each document,  ItemSum combines the usage of the itemsetbased model with a sentence relevance score,  computed from the bagofword sentence representation and based on the well founded tfidf statistics. 
Its efficiency and effectiveness in discovering succinct transactional data summaries makes it particularly suitable for the application to text summarization. 
Unlike previous approaches,  it exploits an entropy based heuristics to drive the mining process and select the most informative yet not redundant item sets without the need of a post pruning step. 
In (Lin and Bilmes 2010), the authors present following contributions
ILP Algorithms,  however,  can sometimes either be expensive for large scale problems or themselves might only be heuristic without associated theoretical approximation guarantees In this paper,  we study graph based approaches for multi document summarization. 
Erk an and Radev(  2004)  introduced a stochastic graph based method,  LexRank,  for computing the relative importance of textual units for multi document summarization. 
In multi document summarization,  redundancy is aparticularly important issue since textual units from different documents might convey the same information. 
We evaluated our approach on the data set ofDUC’04(  2004)  with the setting of task 2,  which is a multi document summarization task on English news articles. 
Indeed,  severalgraphbased methods have been proposed for extractive summarization in the past. 
Recent work in(  Lin et al,  2009)  presents a graph based approach where an undirected weighted graph is built for the document to be summarized,  and vertices represent the candidate sentences and edge weights represent the similarity between sentences. 
One well known graph based sub modular function that measures the similarity of S to the remainder V \ S is the graph cut function. 
As mentioned in Section 1,  our approach is graph based,  not only because a graph is a natural representation of the relationships and interactions between textual units,  but also because many sub modular functions are well defined on a graph and can naturally be use din used in measuring the summary quality Suppose certain pairs(  i,  j)  with i,  j ∈ V are similar and the similarity of i and j is measured by anon negative value w i,  j. 
The summary extraction procedure is done by maximizing a sub modular set function under a cardinality constraint Inspired by(  Lin et al,  2009) ,  we perform summarization by maximizing sub modular functions undera budget constraint. 
We are fortunate in that,  like convexity in the continuous domain,  sub modularity seems to arise naturally in a variety of discrete domains,  and as we will see below,  extractive summarization is one of them. 
InLexRank the importance of sentences is computed based on the concept of eigenvector centrality in the int he graph representation of sentences. 
One major problem of MMR is that it is non optimal because the decision is made based on the scores at the current iteration. 
Mihalcea andTarau also proposed an eigenvector centrality algorithm on weighted graphs for document summarization(  Mihalcea and Tarau,  2004). 
In (Zajic et-al. 2008), the authors present following contributions
collective message summarization(  CMS)  applies a multi document summarization approach,  while individual message summarization(  IMS)  treats the problem as a sequence of single document summarization tasks. 
Due to the complexity of the parameter optimization process,  our multi document summarization system has been more difficult to perfect. 
We have previously implemented both single document and multi document summarization systems built around this architecture. 
one treats the problem as a sequence of single document summarization tasks(  a technique we call individual message summarization,  or IMS) ,  and the other treats the problem as a variant of multi document summarization(  a technique we call collective message summarization,  or CMS). 
In one case,  each message can be considered a document in a multi document summarization task. 
Our basic summarization architecture is shown in Figure   1 – this describes both our previous single document and multi document summarization systems,  which we adapt for IMS and CMS. 
In other words,  treating email thread summarization as a multi document summarization problem leads to higher performance than treating the task as an independent series of single document summarization problems. 
As a point of reference,  we have previously found that Trimmer performs better than HMM for summarization of written news in both single document and multi document conditions(  Zajic et al,  2007). 
Our initial explorations have probed this large problem with existing single document and multi document summarization techniques. 
We present two approaches to email thread summarization. 
one based on linguistically motivated rules that operate on parse trees(  parseandtrim)  and the other based on a noisy channel model implementation using HMMs. 
It is currently a middle of the pack system based on recent DUC evaluations – not significantly better or worse than most systems(  Madnani et al,  2007,  Zajic et al,  2006). 
In (Baralis et-al. 2013), the authors present following contributions
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
This paper presents GraphSum(  Graph based Summarizer) ,  a new multi document summarizer that relies on a graph based summarization strategy. 
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
The multi document summarization task entails generating a summary of a collection of textual documents. 
Multi document summarization Text mining Association rule mining Graph ranking. 
To tackle the sentence based summarization problem,  several research efforts have been devoted to adopting information retrieval and or data mining techniques,  such as clustering. 
Specifically,  graph based summarization focuses on building a graph in which nodes represent either single terms or document sentences,  whereas an edge weighs the strength of the relationship between a pair of nodes. 
,  i e,  the English written benchmark collections that have been used for the Document Understanding Conference(  DUC)  contest on multi document summarization and(  ii)  five real life news article collections. 
• using association rules in graph based summarization to represent correlations among multiple terms,. 
is the latest Document Understanding Conference(  DUC)  contest on generic English written multi document summarization. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
)  to perform news document summarization,  and(  iii)  integrate ontology or dictionary based mining strategies(  e g,. 
In (Fung P, Ngai G 2006), the authors present following contributions
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
In this article,  we propose stochastic Hidden Markov Story Models(  HMSM)  for multi document extractive summarization. 
In extraction based summarization,  salient sentences are automatically extracted to form a sum mary directly Kupiec et al   1995; Myaeng and Jang 1999; Jing et al   2000; Nomoto and Matsumoto 2001,  2002; Zha 2002; Osborne 2002. 
One of the most robust and domain independent summarization approaches is extraction based or shallow summarization Mani 1999. 
In particular,  our proposed meta summarization method takes full advantage of the proposed HMSM for multi document summarization. 
The rst step in multi document summarization is to roughly classify documents that have a common theme or story before we proceed to model each story by our proposed stochastic method. 
We have presented a stochastic Hidden Markov Story Model for multilingual and multi document summarization. 
For multi document summarization,  we present a nal summary for each story by using the state labels in each individual summary. 
,  a theme based multi document sum marization system was proposed to model the story ow in each document and extract salient sentences from story states. 
,  a sim ilar stochastic content model is proposed for information ordering and single document summarization. 
In (Ouyang et-al. 2011), the authors present following contributions
RU is applicable in both single document and multi document summarization,  is extendable to arbitrary compression rates with no extra annotatione ort,  and takes into account both random system performance and inter judge agreement. 
We computed J(  inter judge agreement) ,  R(  random performance) ,  S(  system performance) ,  and D(  normalized system performance)  over 20 clusters(  total = 200 documents)  The results are presented in Table We explored di erent summarization technologies that work in both single and multi document mode. 
In this paper we will discuss Relative Utility(  RU) ,  a method for evaluating extractive summarizers,  both single document and multi document. 
One ma jor bottleneck in the development of text summarization systems is the absence of well de ned and standardized evaluation metrics. 
This annotation allows us to compile human generated ideal summaries at di erent target lengths,  and it is the basis for our di erent measures of sentence based agreement,  both between the human agreement and between the system and the human annotators. 
Even though the relative rankings of the sentences based on the judge assigned importance varies signi cantlyfrom judge to judge,  their absolute importance scores are highly correlated. 
We will present some experiments performed on alarge text corpus to discuss how RU is a ected by interjudgeagreement,  compression rate(  or summary length) ,  and summarization method The main problem with traditional co selection cos election metrics(  thus named because they measure the degree of overlap between the list of sentences selected by a judge and an automatically produced extract)  such as Precision,  Recall,  and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. 
Question Answering and Text Summarization. 
Forty topic based clusters we recreated were created,  consisting of 10 documents each Three human annotators from LDC assigned each sentence a score on a scale from 0 to 10,  expressing the im. 
Fourth,  for summary lengths of 80% and above,  R gets really close to J showing that reasonable summarization that signi cantly beats random at such summary lengths is quite di cult. 
In (Gupta and Lehal 2010), the authors present following contributions
It is an extraction based multi document summarization system. 
integrates multilingual summarization and multi document summarization capabilities using a multi engine,  core summarization system and provides fast,  interactive document access through hypertext summaries. 
is a novel multi document text summarization approach,  which aims to build systems for relation identification and characterization that can be transferred across domains and tasks without modification of model parameters. 
Text summarization based on fuzzy logic system architecture. 
Automatic text summarization based on fuzzy logic. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
Multi document extractive summarization. 
Multi document extractive summarization deals with extraction of summarized information from multiple texts written about the same topic. 
In query based text summarization. 
Query based extractive text summarization. 
In (Cao et-al. 2015c), the authors present following contributions
Experiments on the DUC generic multi document summarization task show that our proposed method outperforms stateoftheart approaches. 
We conduct extensive experiments on the DUC 2001,  2002 and 2004 generic multi document summarization data sets. 
In our work,  we focus on the generic multi document summarization task and carry out experiments on DUC 2001 2004 data sets. 
With respect to baselines,  PriorSum significantly outperforms Reg Manual which uses manually compiled features and the graph based summarization system LexRank. 
Meanwhile,  LexRank(  Erk an and Radev,  2004) ,  a commonly used graph based summarization model,  is introduced as an extra baseline. 
REGSUM is a word regression approach based on some advanced features such as word polarities(  Wiebe et al,  2005)  and categories(  Tausczik and Penne baker,  2010). 
ClusterCMRW incorporates the cluster level information into the graph based ranking algorithm. 
We also choose as baselines those stateoftheart summarization results on DUC(  2001,  2002,  and 2004)  data. 
From Table 2,  we can see that PriorSum can achieve a comparable performance to the stateoftheart summarization systems R2N2,  ClusterCMRW and REGSUM. 
In (Fattah and Ren 2009), the authors present following contributions
LexRank is used to compute sentence importance based on the concept of eigenvector centrality in a graph representation of sentences for multi document summarization task(  Erk an and Radev,  2004). 
In the future work,  we will extend this approach to multi document summarization by addressing some anti redundancy methods which are needed,  since the degree of redundancy is significantly higher in a group of topically related articles than in an individual article as each article tends to describe the main point as well as necessary shared background. 
Other research includes multi document summarization(  Vanderwende et al,  2007,  Harabagiu et al,  2007)  and summarization for specific domains(  Mo ens,  2007,  Reeve et al,  2007,  Ling et al,  2007). 
Unlike LexRank feature,  Bushy path is a simple and an effective text feature for single and multi document summarization task. 
Simpler approaches were then explored that consist of extracting representative text spans texts pans,  using statistical techniques and or techniques based on surface domain independent linguistic analyses. 
The different attempts in this field have shown that human quality text summarization was very complex since it encompasses discourse understanding,  abstraction,  and language generation(  Spar ck,  1993). 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
Table 3,  Table 4 show the summarization precision associated with lead approach and each feature for different compression rates for Arabic and English documents respectively. 
In (Khan et-al. 2015), the authors present following contributions
We propose a framework for abs tractive summarization of multi documents,  which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. 
A multi document summarization system,  GISTEXTER,  presented in. 
Table 2 presents the comparative evaluation of proposed framework and other summarization systems based on mean coverage score and precision measures,  in the context of multi document abs tractive summarization task in DUC 2002. 
The automatic multi document summarization oftextis a major task in the field of natural language processing(  NLP)  and has gained more consideration in recent years. 
In the current era of information overload,  multi document summarization is an essential tool that creates a condensed summary while preserving the important contents of the source documents. 
In order to investigate the impact of GA on summarization,  we dropped GA component from the proposed framework for multi document abs tractive summarization,  and called it AS SRL. 
In future,  we plan to integrate graph with SRL to build a semantic graph for multi document abs tractive summarization. 
Two approaches are employed to multi document summarization. 
In (Banerjee et-al. 2015), the authors present following contributions
Experimental results on the DUC 2004 and 2005 multi document summarization data sets show that our proposed approach outperforms all the baselines and stateoftheart extractive summarizers as measured by the ROUGE scores. 
Compared with their opinionated texts such as product reviews,  the target documents in multi document summarization do not contain such high level of redundancy. 
We performed the rest of our experiments only on the DUC 2004 data set as it has been widely used for multi document summarization. 
In multi document summarization,  all documents are not equally important some documents contain more information on the main topics in the document set. 
We evaluated our approach on the DUC 2004 and 2005 data sets on multi document summarization. 
Graph based techniques have also been very popular in summarization. 
Abs tractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries. 
In (Kedzie et-al. 2015), the authors present following contributions
Our system predicts sentence salience in the context of a large scale event,  such as a disaster,  and integrates these predictions into a clustering based multi document summarization system. 
Multi document summarization has the potential to assist the crisis informatics community. 
While we focus our system on the language of news and disaster,  we emphasize that the use of language modeling can be an effective feature for multi document summarization for other domains that have related text corpora. 
We begin with a review of related work in the information retrieval and multi document summarization literature. 
A principal concern in extractive multi document summarization is the selection of salient sentences for inclusion in summary output(  Nenkova and McKeown,  2012). 
The second category,  predictive approaches,  includes ranking and classification based methods. 
Sentences have been ranked by the average word probability,  average TF*IDF score,  and the number of topically related words(  topic signatures in the summarization literature) (  Nenkova and Vanderwende,  2005; Hovy and Lin,  1998; Lin and Hovy,  2000). 
In (Genest PE, Lapalme G 2011), the authors present following contributions
(  Siddharthan et al,  2004)  uses shallow techniques for syntactical simplification of text by removing relative clauses and appositives,  before running a sentence clustering algorithm for multi document summarization. 
Sentence fusion first identifies themes(  clusters of similar sentences)  from the source documents and selects which themes are important for the summary(  a process similar to the sentence selection of centroid based extractive summarization methods(  Radev et al,  2004) )  and then generates a representative sentence for each theme by sentence fusion. 
Section 3 describes and analyses our first attempt at using this framework,  for the TAC 2010 multi document news summarization task,  followed by the competition s results in section 4. 
Our first attempt at full abs tractive summarization took place in the context of the TAC 2010 multi document news summarization task. 
In (Barrera and Verma 2012), the authors present following contributions
Due to the difficulty that single document summarization has in beating a standard baseline,  especially for news articles,  most efforts are currently focused on multi document summarization. 
These results have implications not only for extractive and abs tractive single document summarization,  but could also be leveraged in multi document summarization. 
We have recently extended this approach to multi document summarization and are currently evaluating it. 
In (CR132), the authors present following contributions
The extrinsic evaluation,  also called task based evaluation,  has received more attention recently at the DARPASummarization Evaluation Conference Mani et al,  1998. 
MEAD is significantly different from previous work on multi document summarization Radev &McKeown,  1998; Carbon ell and Gold stein,  Mani and Bloedorn,  1999; McKeown et al,  1999. 
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015), the authors present following contributions
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates. 
Our system predicts sentence salience in the context of a large scale event,  such as a disaster,  and integrates these predictions into a clustering based multi document summarization system. 
4.23.11-tSummarizing multiple documents through system combination
In (Goldstein et-al. 2000), the authors present following contributions
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
Conventional IR systems find and rank documents based on maximizing relevance to the user query(  Salton,  1970; van Rijsbergen,  1979; Buckley,  1985; Salton,  1989). 
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015), the authors present following contributions
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
In total,  therefore,  when employing ROUGE for the evaluation of summarization systems,  there are 192(  8 x 2 x 2 x 3 x 2)  possible system level variants to choose from. 
We subsequently replicate a recent large scale evaluation that relied on,  what we now know to be,  suboptimal ROUGE variants revealing distinct conclusions about the relative performance of stateoftheart summarization systems. 
Our evaluation reveals for the first time which metric variants significantly outperform others,  optimal metric variants distinct from current recommended best variants,  as well as machine translation metric BLEU to have performance on par with ROUGE for the purpose of evaluation of summarization systems. 
Automatic metrics of summarization evaluation have their origins in machine translation(  MT) ,  with ROUGE(  Lin and Hovy,  2003) ,  the first and still most widely used automatic summarization metric,  comprising an adaption of the BLEU score(  Papineni et al,  2002). 
For example,  although the most commonly used metric ROUGE has a very large number of possible variants,  it is common to include only a small range of those in evaluations. 
Since distinct variants of ROUGE achieve significantly stronger correlation with human assessment than previous recommended best variants,  we subsequently replicate a recent evaluation of stateoftheart summarization systems revealing distinct conclusions about the relative performance of systems. 
Secondly,  were all variants of ROUGE to be included in evaluations,  would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings. 
Correlations with human assessment reveal an extremely wide range in performance among variants,  highlighting the importance of an optimal choice of ROUGE variant in system evaluations. 
Despite such limitations,  however,  subsequent evaluations of stateoftheart summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems(  Hong et al,  2014). 
Since we have established that the variants of ROUGE used to rank stateoftheart and baseline summarization systems in Hong et al  (  2014)  have significantly weaker correlations with human assessment than several other ROUGE variants,  this motivates our replication of the evaluation. 
Although the approach to evaluation of metrics provides insight into the accuracy of conclusions drawn from metric test combinations,  the evaluation is limited by inclusion of only six variants of ROUGE,  fewer than 4% of possible ROUGE variants. 
Despite such limitations,  however,  subsequent evaluations relied on recommended ROUGE variants to rank stateoftheart systems(  Hong et al,  2014). 
When ROUGE(  Lin and Hovy,  2003)  was first proposed,  the methodology applied to its evaluation,  in one respect,  was similar to that applied to metrics in MT,  as ROUGE variants were evaluated by correlation with a form of human assessment. 
Automatic evaluation in MT and summarization have much in common,  as both involve the automatic comparison of system generated texts with one or more human generated reference texts,  contrasting either system output translations or peer summaries with human reference translations or model summaries,  depending on the task. 
In both MT and summarization evaluation,  any newly proposed automatic metric must be assessed by the degree to which it provides a good substitute of human assessment,  and although there are obvious parallels between evaluation of systems in the two areas,  when it comes to evaluation of metrics,  summarization has diverged considerably from methodologies applied to evaluation of metrics in MT. 
Table 4 shows correlations of BLEU and the top ten performing variants of ROUGE when evaluated against the arithmetic(  mean) ,  harmonic and geometric mean of quality and coverage scores for summaries. 
The strength of this correlation,  directly between scores of pairs of summarization metrics,  should be taken into account using a significance test of the difference in correlation between r(  Mbase,  H)  and r(  Mnew,  H). 
Contrary to prior belief,  the vast majority of optimal ROUGE variants are precision based,  showing that the assumption that recall based metrics are superior for evaluation of summarization systems to be inaccurate,  and a likely presence of bias in favor of recall based metrics in evaluations by correlation with human coverage scores alone. 
Current recommended best variants of ROUGE are shown to be signifi cantly outperformed by several other ROUGE variants. 
Figure 3 shows pairwise Williams significance test outcomes for BLEU,  the top ten ROUGE variants,  as well as current recommended ROUGE variants(  Owczarzak et al  (  2012) )  used to compare systems in Hong et al  (  2014). 
Table 2 shows proportions of optimal ROUGE variants that can be attributed to each of ROUGE’s configuration options. 
In Table 1,  • identifies variants of ROUGE not significantly outperformed by any other variant. 
ROUGE includes a large number of distinct variants,  including eight choices of ngram counting method(  ROUGE1; 2; 3; 4; S SU4; W L) ,  binary settings such as word stemming of summaries and an option to remove or retain stop words. 
Somewhat surprisingly,  BLEU MT evaluation metric achieves strongest correlation with human assessment overall,  r = 0.797,  with performance of ROUGE variants ranging from r = 0.786,  just below that of BLEU,  to as low as r = 0.293. 
In MT,  recent work has identified the suitability of Williams significance test(  Williams,  1959)  for evaluation of automatic MT metrics(  Graham and Baldwin,  2014; Graham et al,  2015; Graham,  2015) ,  and,  for similar reasons,  Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. 
In contrast in summarization,  over the years since the introduction of ROUGE,  summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics. 
Besides moving away from well established methods such as correlation with human judgment,  previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants. 
For example,  were the original methodology,  by correlation with human assessment,  to be applied,  would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings. 
Since the inception of BLEU,  evaluation of automatic metrics in MT has been by correlation with human assessment. 
The fact that final overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries,  is,  again,  a divergence from MT evaluation,  as ngram counts used to compute BLEU scores are computed at the document as opposed to sentence level. 
ROUGE score distributions for systems were tested for normality using the ShapiroWilk test(  Royston,  1982)  where score distributions for none of the included systems were shown to be significantly non normal. 
We evaluate systems using the variant of ROUGE that achieves strongest correlation with human assessment,  average ROUGE2 precision with stemming and stop words removed. 
For this purpose,  differences in median ROUGE scores can be tested for statistical significance using,  for example,  Wilcox on signed rank test,  while paired ttest can be applied to difference of mean ROUGE scores for systems. 
(  1)  movement away from evaluation by correlation with human assessment(  2)  omission of important components of human assessment from evaluations,  in addition to large numbers of metric variants(  3)  absence of methods of significance testing improvements over a baseline. 
Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality(  Yannakoudakis et al,  2011; Yannakoudakis and Briscoe,  2012; Evanini et al,  2013). 
Figure 4 shows outcomes of paired ttests for summary score distributions of each pair of systems,  revealing three summarization systems not significantly outperformed by any other as DPP,  ICSISUMM and REGSUM. 
Evaluation of a given summarization metric,  Mnew,  by Pearson correlation takes the form of quantifying the correlation,  r(  Mnew,  H) ,  that exists between metric scores for systems and corresponding human assessment scores,  and contrasting this correlation with the correlation for some baseline metric,  r(  Mbase,  H). 
However,  in this respect,  ROUGE has a distinct advantage over BLEU,  as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system level ROUGE scores,  while BLEU is restricted to the application of randomized methods(  Koehn,  2004; Graham et al,  2014). 
For the purpose of metric evaluation,  we combine human coverage and linguistic quality scores using the average of the two scores,  and use this as a gold standard human score for evaluation of metrics. 
Results reveal superior variants of metrics distinct from previously best recommendations. 
In addition,  BLEU achieves strongest correlation with human assessment overall,  but does not significantly outperform the best performing ROUGE variant. 
This has the obvious disadvantage that superior variants may exist but remain unidentified due to their omission. 
For example,  in the evaluation of metrics in Section 3,  we combined linguistic quality and coverage into a single score using the mean of the two scores. 
We outline an evaluation methodology that overcomes all such challenges,  providing the first method of significance testing suitable for evaluation of summarization metrics. 
For example,  in Table 1 the difference in correlation with human assessment of BLEU and that of median ROUGEL precision with stemming and stop words retained,  0.141(  0.797 − 0.656) ,  is not significant,  while the smaller difference in correlation with human assessment between correlations of BLEU and average ROUGE3 recall with stemming and stop words removed,  0.067(  0.797 − 0.73)  is signifi cant,  since scores of the latter pair of metrics correlate with one another with more strength. 
We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern. 
with stemming and stop words removed is not significantly outperformed by BLEU or any other variant of ROUGE for any of the three combined mean human assessment scores. 
Table 3 shows ROUGE scores for summarization systems originally presented in Hong et al  (  2014). 
In addition,  we include in the evaluation of metrics,  an evaluation of BLEU for the purpose of summarization evaluation,  and contrary to common belief,  precision based BLEU is on par with recall based ROUGE for evaluation of summarization systems. 
BLEU MT metric achieves highest correlation across all human evaluation combinations and highest again when evaluated against human coverage scores alone,  and BLEU’s brevity penalty,  that like recall penalizes a system for too short output,  is a probable cause of the metric overcoming the recall based bias of an evaluation based on coverage scores alone. 
ROUGE2 prec. 
Since the best variant of ROUGE is based on average ROUGE scores as opposed to median ROUGE scores,  a difference of means significance test is appropriate provided the normality assumption of score distributions for systems is not violated. 
Moses(  Koehn et al,  2007)  multibleu was used to compute BLEU(  Papineni et al,  2002)  scores for summaries and prepare rouge applied to summaries before running ROUGE(  Lin and Hovy,  2003). 
An additional variation of human assessment scores is by combining coverage and quality with a variant of the arithmetic mean,  such as the harmonic or geometric mean. 
One possibility would be to search for optimal weights for combining quality and coverage,  but there is a risk with this approach that we will not find the most representative combination but simply the combination that best describes the metrics. 
Other combinations are of course possible,  but without any additional human evaluation data,  it is challenging to identify the combination that best represents an overall human assessment for a given summary. 
In addition,  Table 4 includes correlations of metric scores with coverage alone,  as well as linguistic quality scores alone to provide additional insight,  although linguistic quality scores alone do not provide a suffi cient evaluation of metrics – since it is possible to generate summaries with perfect linguistic quality without inclusion of any relevant content whatsoever. 
For many pairs of metrics,  differences in correlation with human judgment are small,  however,  and prior to concluding superiority in performance of one metric over another,  significance tests should be applied. 
Although BLEU achieves strongest correlation with human assessment overall,  Figure 3 reveals the difference between BLEU’s correlation with human assessment and that of the best performing ROUGE variant as not statistically significant,  and since ROUGE holds the distinct advantage over BLEU of facilitating standard methods of significance testing differences in scores for systems,  for this reason alone we recommend the use of the best performing ROUGE variant over BLEU,  average ROUGE2 precision with stemming and stop words removed. 
In order to evaluate metrics by correlation with human assessment,  it is necessary to obtain a single human evaluation score per system. 
In addition,  as expected,  all stateoftheart systems significantly outperform all baseline systems. 
Table 1 shows the Pearson correlation of each variant of ROUGE with human assessment,  in addition to BLEU’s correlation with the same human assessment of summaries from DUC2004. 
Finally,  options for computation of the overall score for a system is by computation of the mean or median of that system s summary score distribution. 
In DUC2004(  Over et al,  2007) ,  human annotations used to compute summary coverage are carried out by identification of matching peer units(  PUs) ,  the units in a peer summary that express content of the corresponding model summary. 
Additional configurations include the use of precision,  recall or fscore to compute individual summary scores. 
In addition,  our recommended variant,  ave. 
Figure 2 includes linguistic quality and coverage score distributions from DUC2004 human evaluation,  where each distribution is skewed in opposing directions,  in addition to the distribution of the average of the two scores for summaries. 
Detail of the first suitable summarization metric significance test,  Williams test,  was provided. 
More importantly,  however,  linguistic quality scores provide an additional dimension of human assessment,  allowing greater discriminatory power between the quality of summaries than was possible with coverage scores alone. 
Replication of a recent evaluation of stateoftheart summarization systems also revealed contrasting conclusions about the relative performance of systems. 
The location of all points almost exclusively within the upper left corner of the plot in Figure 1 indicates that the linguistic quality of almost all summaries reaches at least as high a level as its corresponding coverage score. 
We wish to thank the anonymous reviewers. 
Figure 1 is a scatter plot of human coverage scores and corresponding linguistic quality scores for all human assessed summaries from DUC- 2004,  where,  for the purpose of comparison,  each of the 7 linguistic quality ratings are converted to a corresponding percentage quality(  A 100%; B 75%; C 50%; D 25%; E 0%). 
In addition to annotations used to compute human coverage scores,  human assessors were asked to rate the linguistic quality of summaries under 7 different criteria,  providing ratings from A to E,  with A denoting highest and E least quality rating. 
Human coverage scores(  CS)  are computed by combining Matching PUs with coverage estimates as follows. 
This research is supported by Science Foundation Ireland through the CNGL Program me(  Grant 12/CE/I2267)  in the ADAPT Cent re(  www. 
adaptcentre ie)  at Trinity College Dublin. 
In addition,  an overall coverage estimate(  E)  is provided by the human annotator,  the proportion of the corresponding model summary or collective model units(  MUs)  expressed overall by a given peer summary. 
This follows the intuition that a summary is unlikely to obtain high coverage without sufficient linguistic quality,  while the same cannot be said for the converse,  that a high level of linguistic quality necessarily leads to high coverage. 
In (Lin 2004), the authors present following contributions
In this paper,  we introduced ROUGE,  an automatic evaluation package for summarization,  and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data. 
We found that(  1)  ROUGE2,  ROUGEL,  ROUGEW,  and ROUGES worked well in single document summarization tasks, (  2)  ROUGE1,  ROUGEL,  ROUGEW,  ROUGESU4,  and ROUGESU9 performed great in evalua ting very short summaries(  or headline like summaries) , (  3)  correlation of high 90% was hard to achieve for multi document summarization tasks but ROUGE1,  ROUGE2,  ROUGES4,  ROUGES9,  ROUGESU4,  and ROUGESU9 worked reasonably well when stop words were exc luded from match ing, (  4)  exclusion of stop words usually improved correlation,  and(  5)  correlations to human judgments were increased by using multiple references. 
Therefore,  how to evaluate summaries automatically has drawn a lot of at tent ion in the summarization research community in recent years. 
We found that correlations were not affected by stemming or removal of stop words in this data set,  ROUGE2 performed be tter among the ROUGEN variants,  ROUGEL,  ROUGEW,  and ROUGES were all performing well,  and using mu ltiple references improved pe rformance though not much. 
Saggion et al  (  2002)  used normalized pairwise LCS to compare simila rity between two texts in automatic summarization evaluation. 
This result is more intu itive than using BLEU -2 and ROUGEL. 
In Table 3 A,  A,  and A,  we show correlation analysis results on DUC 2001,  2002,  and 2003 100 words multi document summarization data. 
However,  how to achieve high correlation with human judgments in multi document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic. 
ROUGEN,  ROUGEL,  ROUGEW,  and ROUGES included in the ROUGE summarization evaluation package and their evaluatio ns. 
One advantage of using LCS is that it does not require consecutive matches but in sequence matches that reflect sentence level word order as ngrams. 
In summary,  we showed that the ROUGE package could be used effectively in automatic evalu at ion of summaries. 
The results indicated that using multiple references improved correlation and exclusion of stop words us ually improved performance. 
Melamed et al  (  2003)  used uni gram Fmeasure to estimate machine translation quality and showed that uni gram Fmeasure was as good as BLEU. 
To check the significance of the results,  we estimated confidence intervals of correlations using bootstrap re sampling. 
It includes measures to automatically determine the quality of a summary by comparing it to other(  ideal)  summaries created by humans. 
In (Zajic et-al. 2008), the authors present following contributions
Although this is generally accepted in the summarization literature,  and Rouge scores are widely reported in lieu of opinions from human assessors,  the extension of this automatic metric across domains has not been established. 
This represents among the first automatic summarization work of its type on this particular corpus As a first step,  we have adapted existing document summarization techniques to tackle this problem. 
Using a baseline retrieval engine built on Luce ne,  she manually searched for relevant threads and selected them for summarization. 
We explored the email thread summarization problem using messages from the Enron corpus,  which consists of approximately half a million emails from the folders of 151 Enron employees. 
System output was automatically evaluated using Rouge with the five reference summaries described in the previous section. 
In (Dunlavy et-al. 2007), the authors present following contributions
We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences(  DUC)  as measured by the best known automatic metric for summarization system evaluation,  ROUGE. 
The benefit of using QCS over such methods is that it is a fully automatic system for document retrieval,  organization,  and summarization. 
Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001–2006 Document Understanding Conferences The focus of these conferences is the evaluation and discussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. 
In (Riedhammer et-al. 2010), the authors present following contributions
However,  listening to the whole meeting is tedious and one should be able to directly access the relevant information Automatic meeting summarization is one step towards the development of e cient user interfaces for accessing meeting archives. 
This approach is based on the ICSI text summarization system(  Gil lick et al,  2008; Gil lick and Favre,  2009)  and was mo di ed for the fort he meeting domain in(  Gil lick et al,  2009)  . While most MMR implementations rely on words and their frequency throughout the data,  we could already show that using key phrases instead of words to model relevancy leads to better performance for meeting summarization(  Riedhammer et al,  a). 
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (CR100), the authors present following contributions
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed(  i e,  COMPENDIUM) ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted. 
This paper focuses on abs tractive text summarization. 
In light of this,  Text Summarization(  TS)  is of great help since its main aim is to produce a condensed new text containing a significant portion of the information in the original text(  s)  The process of summarization can be divided into three stages. 
In this section,  we explain previous work on recentabstractive summarization,  and we stress our novelty with respect to other similar approaches An approach for combining different fragments of information that have been extracted from one or more document sis documents is suggested in. 
In order to solve this limitation,  besides checking for the correctness of the sentences once they have been generated and filtering out those ones,  which do not satisfy the proposed constraints,  we would also need to apply some constraints based on the information sentences contain,  optimizing the set of generated sentences,  so that only the best ones with respect to their content are used With respect to the general results of the abstractiveapproaches,  since the length of the summaries is restricted to only 100 words,  when selecting the most important sentences before or after generating new sentences,  some of the oft he concepts may not be included. 
On the Ont he one hand,  we try to elucidate the reasons why the Graph COMPENDIUM approach performs worse than theCOMPENDIUM Graph,  and on the other hand,  we want to analyze the reasons of the low overall performance of theabstractive approaches Regarding the first type of analysis carried out,  if we use the word graph based method for generating new sentences first,  and use all of them as input for COMPENDIUM,  thisTS tool can have difficulties in selecting important content This occurs because many of the sentences will start with the same words(  e g,  if we take the top 10 words with highest tfidf) ,  so once COMPENDIUM detects a specific fragment of information as relevant,  sentences containing the same portion of information that have not been detected as redundant will be also selected,  leading to summaries that have not much variation in content. 
attempt to transform an extractive summarization into an abs tractive one in the context of meeting summarization by performing sentence compression. 
In these cases,  all the stages of the summarization process are taken into account Due to the difficulty associated to the generation of abstracts,  most approaches only focus on the first stage(  i e,  topic identification) ,  producing extracts as a result. 
In contrast,  abs tractive approaches require a more elaborate process,  involving sentence compression,  information fusion,  and or language generation. 
Consequently,  in order to solve these limitations,  research into abstractivemethods is needed. 
,  The aim of this paper is to conduct an analysis of the potentials and limitations of word graphs for generatingabstractive summaries. 
However,  preliminary experiments carried out prove that the combination of extractive and abs tractive information is a more suitable strategy to adopt towards the generation of abstracts.KeywordsHuman Language Technologies,  automated retrieval and mining,  automated content summarization,  abs tractive techniques,  graph based algorithms. 
This research has been supported by the FPI grant(  BES200716268)  from the Spanish Ministry of Science and Innovation,  under the project TEXTMESS(  TIN200615265C0601)  and project grant no   TIN200913391C0401,  both funded by the SpanishGovernment. 
Moreover,  we want to verify if the proposed strategy for generating new sentences taking into account the words with highest tfidf could be appropriate for query focused summarization. 
Concerning theCOMPENDIUM +GraphsALL and baselineALL approaches,  the results of the former increase by 80% and 72%,  for the fort he first words or the top 10 words with highest tfidf,  respectively. 
We perform a ttestto account for the significance of the results for a confidence interval(  results which are statistically significant are marked with a star)  Assuming this ideal case,  the results are improved by 25% on average,  with respect to the original approaches. 
Further on,  taking also into consideration theabstractive summaries produced,  we combined both types of summaries,  according to these rules. 
(  i e,  we produce generic single document summaries of 100 words each)  and we compare our abs tractive summaries to the existing model summaries In addition to the two approaches explained in Section IV. 
The results obtained give clear proof of the oft he difficulty of the task,  and the challenges it presents However,  in a preliminary experiment,  we show that a more appropriate strategy would be to combine extractive andabstractive information,  improving the performance of the resulting summaries considerably The remaining of the paper is structured as follows. 
The minimal length for a sentence must be 3 words(  i e,  subject verb object)  Every sentence must contain a verb The sentence should not end in an article(  e g,  a,  the) ,  apreposition(  e g,  of) ,  an interrogative word(  e g,  who) ,  nor a conjunction(  e g,  and)  The remaining sentences after applying the aforementioned constraints will be used for building the abstractivesummaries.IV. 
2 strategies for generating new sentences,  3 summarization approaches,  and 3 heuristics for selecting sentences with regard to their length,  resulting in 900 different summaries(  50 documentsx 18 types)  For assessing the appropriateness of the generated abs tractive summaries,  we compare them to the model summaries employing the evaluation tool ROUGE. 
Further on,  howabstractive summaries are produced is explained in SectionIV. 
Extractive summarization relies on the selection of the most important sentences in order to produce the summary. 
This is due to the fact that the summarization guidelines we followed together with the model summaries we had,  focused on generic summarization,  whereas our proposed strategy for generating new sentences using the top 10 words with highest tfidf may be more appropriate toqueryfocused summarization,  since this type of summary contains the most important information with regard toa specific topic,  and consequently,  the tfidf method can provide some clues about the relevant topics of a document Now,  by examining the content of the generated abstracts,  we mainly focus on two types of problems. 
In general,  results are lower when the new sentences are generated from the words with highest tfidf values. 
In theshortterm,  we plan to increase the corpus size and carry out the same experimentation with more documents,  improving also the word graph method. 
The same approach but in the int he case of the top 10 words with highest tfidf are used,  leads to an improvement of 40% compared to the results obtained for Graphs COMPENDIUM -ALL. 
A clear tendency is observed in the majority of the cases that the best results are obtained when the important information is first identified and extracted,  and then the new sentences are generated(  COMPENDIUM Graphs)  .ROUGE results for COMPENDIUM +GraphsALL improve on average 55% with respect to the Graphs COMPENDIUM -ALL approach when the first words of a sentence are used to generate the new sentences. 
The First International Conference on Advances in Information Mining and Management we need to face. 
They also help us to identify the limitations and the main challenges63IMMM 2011. 
DiscussionAs can be seen from both tables,  results are not very high,  though they are promising for further research,  since they quantify how far we are from producing abstracts. 
Tables I and II show the Fmeasure results for the abs tractive summaries C. 
ROUGE1,  ROUGE2 andROUGESU4,  which account for the number of commonunigrams,  bi grams big rams,  and skipbigrams with four words in between at most,  respectively. 
In particular,  we use the following metrics. 
28. 
The purpose of the method was to compress and merge information from sentences. 
only those sentences that are longer(  in number of words)  than the average length,  and SHORT. 
all generated sentences LONG. 
• ALL. 
Graphs COMPENDIUM and COMPENDIUM Graphs,  we define a baseline,  in which we generate the new sentences from the source document and select the first ones to build the abs tractive summary,  until the length of 100 words is reached Moreover,  in order to broaden this analysis,  we experiment with three heuristics concerning the length of the generated sentences. 
In (Kulkarni and Prasad 2010), the authors present following contributions
This study describes three approaches to Automated Text Summarization using connectionist statures based on. 
Authors have surveyed the current text summarization approaches(  Afantenos et al,  2005; Ledeneva et al,  2008)  their advantages and disadvantages and,  with the goal of identifying summarization techniques most suitable to generic text summarization. 
7-tMultilingual approaches for text summarization
In (Baralis et-al. 2013), the authors present following contributions
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
Multi document summarization Text mining Association rule mining Graph ranking. 
8-tSummary evaluation
8.1-tInformativeness evaluation
In (Patel et-al. 2007), the authors present following contributions
One of the measures in informativeness methodology for extractive summary evaluation is content based evaluation i e. 
So,  evaluation in terms of informativeness is usually preferred. 
The first is Quality evaluation and the second is an informativeness evaluation. 
Degree of Evaluation has been debatable for long time. 
Section gives details of material and evaluation strategies used. 
Through evaluations performed on a single document summarization for English,  Hindi,  Gujarati and Urdu documents,  we show that the method performs equally well regardless of the language. 
Our algorithm tags the sentences making references to previous sentences. 
19. 
This problem has been handled by analyzing the sentence for the presence of certain set of terms or phrases within positional constraints. 
This paper describes an efficient algorithm for language independent generic extractive summarization for single document. 
If a term is repeating more than once in a sentence,  only one instance is considered for computing intermediate sentence weight. 
α and β are constants whose values depend one the language being considered for the summarization. 
respectively. 
Where W(  s)  is an intermediate sentence weight,  WD and WT are the term weights of those sentence terms,  which belong to the document feature vector and the theme feature vector,. 
Degree of information content of a sentence is represented by sentence weight,  which is computed as follows. 
One of the problems of summary generation is the risk of extracting a sentence,  which is not complete by itself as it makes reference to previous sentence(  s). 
3.2 Sentence Reference Index. 
It thus. 
So,  the next step is to carry out sentence analysis and to determine sentence weight. 
Conference RIAO2007,  Pittsburgh PA,  U.S.A. 
May June 1,  2007 - Copyright C.I.D. 
Paris,  France. 
generates what we call as Sentence Reference Index. 
This index is used to enhance information content of a sentence. 
The intermediate sentence weight W(  s)  is enhanced for those sentences whose following sentences make reference to it. 
This is based on the following hypothesis. 
If more is said(  written)  in the following sentence(  s)  about the contents of the current sentence,  it implies higher importance(  richness)  of content of the current sentence. 
In order to calculate the Sentence Reference Index,  we are maintaining an external table for the words that give the indication for previous sentence reference. 
We have collected a list of such words for English,  Hindi,  Gujarati and Urdu languages. 
This process is repeated recursively updating the tag values appropriately. 
Thus,  after preprocessing we have made the text suitable for extracting important sentences. 
Theme Feature Vector(  For Hindi Documents). 
Title Feature Vector vs. 
Second,  check each word in the dictionary(  vocabulary)  of respective language. 
If word is not found in the dictionary then it is considered to be a name. 
Both of these approaches involve a lot of overhead and the execution of algorithm becomes slow. 
We have used a novel way to handle this issue with satisfactory results. 
We consider only those words as names which are within single double quotes. 
And we enhance the weight of sentence in as described in section. 
Using the threshold as 30% of the highest frequency,  the resulting vector is called the Document Feature Vector. 
In (Banerjee et-al. 2015), the authors present following contributions
Further,  manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness. 
In manual evaluation,  our approach also achieves promising results on informativeness and readability. 
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (He et-al. 2012), the authors present following contributions
We use the ROUGE(  RecallOriented Understudy for Gi sting Evaluation)  toolkit(  Lin 2004)  which has been widely adopted by DUC for automatic summarization evaluation. 
We choose two automatic evaluation methods ROUGEN and ROUGEL in our experiment. 
8.4-tText summarization evaluation programs
In (Mani and Maybury 1999), the authors present following contributions
This is a welcome volume for both researchers and teachers who are interested in extending the traditional boundaries of Information Retrieval to include related information access and analytic applications such as summarization,  extraction,  and question answering Text summarization,  which is de ned by the editors as the process of distilling the most important information from a source(  s)  to produce an abridged version for a particular user(  s)  and task(  s)  ” is a technology that has been worked on for more than forty years anda capability that users have long desired. 
Today s recognized information overload has exacerbated this need while advances in Natural Language Processing and statistical text processing appear ready to provide viable near term solutions The editors have compiled an excellent selection of papers on summarization,  with an even split between classical and contemporary papers thirteen of each,  with many of the contemporary contributions based on papers presented at the ACL/EACL Workshop on Intelligent Scalable Summarization,  held in Madrid,  Spain in The choice classical papers include the 1958 paper by Luhn on The AutomaticCreation of Literature Abstracts,  Edmund son s 1969 paper on New Methods in Automatic Extracting,  and the 1975 paper by Pollock and Zamora on Automatic AbstractingResearch at Chemical Abstracts readings which I always include in my course on Indexing and Abstracting. 
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193–197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764–7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675–1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514–14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196–206
0074 Amigo et-al. 2005 Amigó E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL ’05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280–289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584–599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260–273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553–563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208–1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SAC’12), pp 782–786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96–109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366–377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL ’05), pp 141–148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107–117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829–833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335–336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91–100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353–361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85–112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759–777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, O’Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588–1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613–1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755–5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780–5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1–16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340–348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64–73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1–39
0037 Glavaš G, Šnajder J 2014 Glavaš G, Šnajder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904–6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40–48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128–137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203–225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717–727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258–268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620–1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSP’06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33–64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212–225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515–1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107–117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81–94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382–386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591–594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319–322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608–1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737–747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61–66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255–262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695–1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366–1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788–791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462–471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887–902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490–500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778–787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74–81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912–920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168–178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61–66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159–165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404–411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132–138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41–55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319–324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42–54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227–237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190–198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210–218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298–1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123–132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79–88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1–4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919–938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801–815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419–430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206–213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177–184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862–2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1–8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224–233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112–9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341–359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513–523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948–961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37–50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643–1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75–95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600–1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35–41
