root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (Glavaš G, Šnajder J 2014):
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
We chose these particular kernels because their general forms have intuitive interpretations for event matching and can be easily adjusted to fit our needs for event centered text comparison. 
Furthermore,  considering that numerous textual sources provide information about the same real world events,  the need for aggregating and summarizing the most relevant information has become obvious. 
Nevertheless,  studies on event based text summarization are rare(  Daniel,  Radev,  & Allison,  2003; Filatova & Hatzivassiloglou,  2004; Li,  Wu,  Lu,  Xu,  & Yuan,  2006). 
The amount of textual data reporting on real world events(  e g,  breaking news,  police reports,  social media posts)  is increasing rapidly on a daily basis. 
With the number of documents describing real world events and event oriented information needs rapidly growing on a daily basis,  the need for efficient retrieval and concise presentation of event related information is becoming apparent. 
2-tVarious types of text Summarization
In (Giannakopoulos et-al. 2008):
There are various types of correlation measures,  called correlation coef cients,  depending on the context in which they can be applied. 
Although ROUGE takes into account contextual information,  it remains at the word level,  which means we either regard different types of the same word as different or we need to apply(  language dependent)  stemming or lemmatization to remove this effect. 
Trying to capture more than the simple co occurrence of words and in order to allow for different types of the same word,  our method uses character ngrams positioned within a context indicative graph. 
It would also be interesting to investigate different types of neighborhood and different functions of importance for neighborhood as well as different weighting functions for the importance of matching ngrams of speci c rank(  e g,  longer vs. 
In (0079):
The use of rare words or technical terminology for example can make text difficult to read for certain audience types(  CollinsThompson and Call an,  2004; Sch warm Schwa rm and Ostendorf,  2005; Elhadad and Sutaria,  2007). 
Coherent text is characterized by various types of cohesive links that facilitate text comprehension(  Halliday and Has an,  1976). 
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non discourse features(  Miltsakaki and Kukich,  2000). 
In (Carlson et-al. 2003):
Thus,  our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth,  such as anaphoric relations(  Gar side et al,  1997)  or style types(  Leech et al,  1997)  ; analysis of a single text from multiple perspectives(  Mann and Thompson,  1992)  ; or illustrations of a theoretical model on a single representative text(  Brit ton Britt on and Black,  1985; Van Dijk and Kintsch,  1983). 
The annotators hired to build the corpus were all professional language analysts with prior experience in other types of data annotation. 
textual organization,  span,  and same unit(  used to link parts of units separated by embedded units or spans). 
In (Barzilay and Lapata 2005):
Each text can thus be viewed as a distribution defined over transition types. 
Note that considerable latitude is available when specifying the transition types to be included in a feature vector. 
there is often no single coherent rendering of a given text but many different possibilities that can be partially ordered. 
Second,  we examined the effect of different types of summaries(  human vs. 
Participants were asked to use a seven point scale to rate how coherent the summaries were without having seen the source texts. 
This approach has been shown to be highly effective in various tasks ranging from collaborative filtering(  Joachims,  a)  to parsing(  Toutanova et al,  2004). 
In (Mihalcea and Tarau 2004):
systems(  4) ,  types(  3) ,  solutions(  3) ,  minimal(  3) ,  linear(  2) ,  in equations(  2) ,  algorithms(  2). 
For the same text,  a frequency approach provides the following top ranked lexical units. 
Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. 
These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. 
the text is tokenized,  and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic lters. 
In (Radev et-al. 2001):
• Di erent color schemes for di erent types of information Di erent colors were used for display of various types of information. 
WebInEssence uses two types of clustered input – either the set of hits that the user has selected or the output of our own clustering engine – CIDR(  Radevet al,  1999)  It uses an iterative algorithm that creates as a side product the so called document centroids. 
mining(  84.54) ,  data(  64.13) ,  knowledge(  14.25) ,  discovery(  11.98) ,  advertised(  11.20) ,  databases(  9.69) ,  information(  6.98) ,  research(  6.96) ,  analysis(  6.95) ,  text(  6.05) ,  patterns(  5.30) ,  algorithms(  4.39). 
In (Carenini et-al. 2007):
According to our preliminary study,  we found that the email threads could be divided into two types. 
In order to cover both structure in the context of email summarization,  we randomly select 4 single chains and 16 trees,  which is close to their ratio in the 38 conversations. 
In our experimentation,  we observe that stemming occurs the most frequently among the three types discussed above. 
The selected emails also need to represent different types of conversation structure. 
Several studies in the NLP literature have explored the reoccurrence of similar words within one document due to the text cohesion. 
In (Grosz et-al. 1995):
Our original paper(  Grosz,  Jos hi,  and Weinstein 1983)  on centering claimed that certain entities mentioned in an utterance were more central than others and that this property imposed constraints on a speaker'  s use of different types of referring expressions. 
We defined various centering constructs and proposed two centering rules in terms of these constructs. 
To describe these types,  we need to introduce two new relations,  realizes and directly realizes,  that relate centers to linguistic expressions. 
That is,  the centers of an utterance in general,  and the backward looking center specifically,  are determined on the basis of a combination of properties of the utterance,  the discourse segment in which it occurs,  and various aspects of the cognitive state of the participants of that discourse. 
In (Hearst 1997):
With the increase in importance of multimedia information,  especially in the context of Digital Library projects,  the need for segmentation and summarization of alternative media types is becoming increasingly important. 
coding discourse and dialogue phenomena,  and especially coding segment boundaries,  may be inherently more difficult than many previous types of content analysis(  for instance,  dividing newspaper articles based on subject matter) "  and so implies that the levels of agreement needed to indicate good reliability for TextTiling may be justified in being lower. 
Initial testing was done on the texts evaluated with several different sets of parameter settings and a default configuration that seems to cover many different text types was chosen. 
In (Harabagiu S, Lacatusu F 2005):
The recognition of discourse relations between text units can be performed by using a combination of features that represent(  a)  alignment between entities and predicates from each text that encode similar types of lexicosemantic information(  b)  alignment based on functional similarity or inclusion(  c)  alignment based on semantic information specific to each discourse relation(  e g. 
In the rest of this section,  we describe how we recognized both types of relations for use in a summarization system. 
We expect that by organizing topic themes using either(  or both)  cohesion and coherence relations,  we can generate more relevant MDS,  which better reflects the actual organization of topics in texts. 
In (Rush et-al. 2015):
6 overlapping word types between the headline and the input although only 2. 
The headline vocabulary consists of 31 million tokens and K word types with the average title of length 8. 
Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. 
We apply a minimal preprocessing step using PTB tokenization,  lower casing,  replacing all digit characters with #,  and replacing of word types seen less than 5 times with UNK. 
The expectation is for a summary of roughly 14 words,  based on the text of a complete article(  although we only make use of the first sentence). 
In (Kulesza and Taskar 2012):
Samples from the model,  like the one presented in the gure,  not only o er some immediate intuition about the types of papers contained in the collection,  but also,  upon examining individual threads,  provide a succinct illustration of the content and development of each area. 
We can then use the DPP content model wit ha size model of our choosing,  or simply set the desired size based on context. 
The average out degree is 3.26 citations per paper,  and 0.011% of the total possible edges are present in the graph To obtain useful threads,  we set edge weights to re ect the degree of textual similarity between the citing and the cited paper,  and node weights to correspond with a measure of paper importance. 
In (Dunlavy et-al. 2007):
Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001–2006 Document Understanding Conferences The focus of these conferences is the evaluation and discussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. 
We present a novel integrated information retrieval system the Query,  Cluster,  Summarize(  QCS)  system which is portable,  modular,  and permits experimentation with different instantiations of each of the constituent text analysis components. 
Most importantly,  the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic. 
In (Nobata et-al. 2001):
First,  we explain scoring functions used in the system,  and then mention the other parts,  such as threshold types,  patterns and parameters the system uses. 
Each evidence is integrated using parameters,  which are estimated using training data Suitable parameter sets can be selected at each section information and compression ratio In the following sections,  we explain methods used in our system,  then show and discuss the evaluation results on the TSC,  Text Summarization Challenge,  which was held by National Information Institute. 
Our system uses four types of metrics to estimate the importance of sentences,  such as sentence location,  sentence length,  TF/IDF values of words,  and similarity to the title In the task B of TSC,  summarization for information retrieval task,  the system also uses similarity to a given query The signi cance of sentences is given by the sum of the values of the above metrics with parameters. 
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (Zajic et-al. 2008):
a pure statistical approach based on HMMs. 
On the other hand,  we expect that the purely statistical HMMbased approach will be more robust to text from different genres. 
Specifically,  a sentence selector builds the final summary by choosing among the candidates,  based on features propagated from the sentence compression method,  features of the candidates themselves,  and features of the present summary state. 
It is currently a middle of the pack system based on recent DUC evaluations – not significantly better or worse than most systems(  Madnani et al,  2007,  Zajic et al,  2006). 
In published work,  we have examined two approaches to sentence compression. 
Since the trimming approach requires an accurate parse tree to work with,  we anticipate that parse errors will be a major source of concern because modern statistical parsers are generally trained on news wire text and perform poorly on outofgenre text. 
In this work,  we do not examine the filtering process in detail instead,  only very simple approaches are employed,  e g,  retain first n sentences. 
We adopt a weighted feature based approach where the parameters have been tuned on test data from previous DUC evaluations. 
one based on linguistically motivated rules that operate on parse trees(  parseandtrim)  and the other based on a noisy channel model implementation using HMMs. 
Prima facie,  both approaches have advantages and disadvantages. 
We have developed two different approaches to the problem of email thread summarization that leverage existing work. 
The recent work of Carenini et al  (  2007)  examines extractive approaches to summarization on Enron data that leverage graphs defined by quoted texts. 
The bar graphs in Figure   3,  Figure   4 present a different view of the results error bars denote the 95% confidence intervals,  which provide the reader a method for assessing the statistical significance of the differences. 
Experiments did not reveal any significant differences in performance between the two approaches,  suggesting that there is value in content both at the beginnings and ends of email threads. 
Instead of a purely extractive approach,  we employ linguistic and statistical methods to generate multiple compressions,  and then select from those candidates to produce a final summary. 
Both our linguistic and statistical sentence compression techniques did not appear to perform well on Enron data,  due to outofgenre issues. 
The matrix setup of the complete set of experimental conditions allowed to us answer two research questions independently. 
Note that additional truncation was not necessary with the CMS approach since summary length is directly controlled by the sentence selector,  which iteratively selected candidates until the desired length had been achieved. 
We explored both hypotheses experimentally. 
IMS without sentence compression,  initial variant IMS without sentence compression,  final variant CMS without sentence compression. 
3.2-tTopic based approaches
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Hennig and Labor(  2009)  proposed a multi document summarization method based on Probabilistic Latent Semantic Analysis(  PLSA) ,  which represents sentences and queries as probability distributions over latent topics. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
Wan(  2008)  also proposed a document summarization model using the Hypertext Induced Topic Search(  HITS)  algorithm. 
Traditionally,  research on extractive summarization is based on the position of a sentence in a document which measures the overall frequency of the words they contain such as the TFIDF technique(  Luhn,  1958; Edmund son,  1969; Bran dow Brand ow,  Mitze,  & Rau,  1995). 
3.3-tGraph based approaches
In (Glavaš G, Šnajder J 2014):
In this article,  we present event graphs,  a novel event based document representation model that filters and structures the information about events described in text. 
To construct the event graphs,  we combine machine learning and rule based models to extract sentence level event mentions and determine the temporal relations between them. 
To adequately capture the semantics of events,  we introduce event graphs,  a novel event centered document representation based on sentence level event mentions. 
We describe an NLP pipeline that combines supervised machine learning and rule based models for the extraction of event graphs from English text. 
To this end,  we employ a named entity recognition(  e g,  if an argument is a named entity of type LOCATION,  then the argument is declared to be locative) ,  temporal expression recognition(  if an argument is a part of a temporal expression,  it is considered to be temporal) ,  and a measure of WordNetbased semantic similarity with temporal and locative concepts(  e g,  location,  geographical area,  and facility for locations,  and,  e g,  time,  duration,  time period for temporal arguments). 
3.4-tDiscourse based approaches
In (Gupta et-al. 2011):
Based on this classification,  automatic summarizers can be characterized as approaching the problem at the surface,  entity,  or discourse level. 
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 
There are several approaches to Text Summarization,  such as those proposed by(  Hovy and Mar cu 1998; Mani and May bury 1999; Tucker 1999; Radev 2000; May bury and Mani 2001; Mani 2001; Alonso and Castellon 2001). 
In (Yeh et-al. 2005):
Entity level approaches model text entities and their relationships co occurrence,  co reference,  etc,  and determine salient information based on the text entity model(  Azzam et al,  1999; McKeown & Radev,  1995). 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
outperforms keyword based text summarization approaches. 
In (Pardo et-al. 2003b):
Indeed,  if we discourse analyze the sample text based on the RST Theory. 
Similarly to the other deep based work,  our system can also produce various summaries for the same source text,  for distinct strategies may be corresponding to varied choices of information units or discourse relations. 
Although GistSumm also determines the gist of a source text,  its approach is purely statistical,  avoiding the inherent complexity of those deep based ones. 
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005):
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 
However,  most IR techniques that have been exploited in text summarization focus on symbolic level analysis,  and they do not take into account semantics such as synonymy,  polysemy,  and term dependency(  Hovy & Lin,  1997). 
Latent semantic analysis(  LSA)  is a mathematical technique for extracting and inferring relations of expected contextual usage of words in passages of discourse(  Deer wester et al,  1990; Landau er et al,  1998). 
The second one exploits latent semantic analysis(  LSA) (  Deer wester,  Duma is,  Furn as,  Landau er,  & Harsh man,  1990; Landau er,  Foltz,  & La ham,  1998)  and a text relationship map(  T.R.M.) (  Salton et al,  1997)  to derive semantically salient structures from a document. 
The second uses latent semantic analysis(  LSA)  to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. 
one used relevance measure to rank sentence relevance,  and the other used latent semantic analysis to identify semantically important sentences. 
We conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Thus,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Hence,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
4.2-tInformation extraction using sentence based abstraction technique
In (Chan 2006):
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 
Simulation experiments suggest that this technique is useful because it moves away from a purely keyword based method of textual information extraction and its associated limitations. 
At the syntactic stage,  information extraction involves tokenization,  partsofspeech tagging,  syntactic parsing,  phrasal pattern matching,  and the transformation of each sentence into an intermediate structure using various templates. 
In this article,  a sentence based abstraction technique for information extraction is presented. 
The application of text based abstraction techniques in various disciplines has recently become more widespread. 
Although the demand for a sophisticated text abstraction technique is pervasive,  it is commonly accepted that the latest technology that has developed from current linguistic analysis,  which aims to completely simulate the human intellectual understanding,  is far from satisfactory. 
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines. 
Our abstraction technique can be used to detect the boundaries between groups of consecutive sentences that are highly relevant to each other. 
The usual approach to text classification is to reduce a text to a bag of words,  which throws away a lot of the linguistic information that is represented,  but the salient sentences that are extracted by this abstraction technique provide an alternative to text classification and indexing. 
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007):
Following our work in the Document Understanding Conference(  DUC)  2005 and 2006(  Ye et al,  2005,  Ye et al,  2006) ,  our summarization algorithm uses the DCL to select a globally optimum sentence set that represents as many concepts as possible with the fewest words. 
Text summarization Document concept lattice Concept Semantic. 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document(  s)  that can be preserved after summarization. 
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests. 
This task of document understanding(  Dang,  2005)  is a core issue in text summarization. 
• Motivated by our evaluation metric on answer loss,  we propose a novel document model,  the document concept lattice,  which indexes sentences with respect to their coverage of overlapping concepts. 
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 
Once words that represent unified concepts in the documents are linked,  we represent the sources as a document concept lattice(  DCL). 
In this paper,  we review and detail our approach to automatic,  multi document extractive summarization. 
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Giannakopoulos et-al. 2008):
The AutoSummENG method is based on the concept of using statistically ex tr acted tract ed textual information from summaries integrated into a rich representa tional equivalent of a text to measure similarity between generated summaries and a set of model summaries. 
evaluation frameworks uses statistical measures of similarity based on ngrams of words,  4 although it supports different kinds of analysis,  ranging from ngram to semantic Hovy et al   b. 
A number of different intermediate representations of summaries information have been introduced in existing summarization evaluation literature,  ranging from automatically extracted snippets to human decided sub sentential portions of text. 
The over information common ground of recent information retrieval efforts has created a serious motive for the design and implementation of summarization systems,  which are either based on existing information retrieval practices or provide a new pointofview on the retrieval process. 
In the domain of automatic summarization,  graphs have been used as a means to determine salient parts of text Mihalcea 2004; Erk an and Radev a,  b or determine query related sentences(  see Otterbacher et al. 
These categories of ngrams are based on statistical criteria and are used to describe how noise can deteriorate the performance of our method as a function of the methodology parameters. 
the type of statistical information extracted,  the representation chosen for the extracted information,  the method of similarity calculation. 
In multi document summarization,  graphs have also been used to detect differences and similarities between source texts Mani and Bloedorn 1997. 
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008):
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 
We integrate our best cohesion measure together with the subjective opinions. 
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones. 
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 
Subjective opinions are often critical in many conversations. 
Summarizing email conversations is challenging due to the characteristics of emails,  especially the conversational nature. 
Having built the sentence quotation graph with different measures of cohesion,  in this section,  we develop two summarization approaches. 
Third,  we did not consider subjective opinions. 
Moreover,  subjective words can significantly improve accuracy. 
Then we extend this structure into a sentence quotation graph,  which can allow us to capture the conversational relationship at the level of sentences. 
The empirical evaluation shows that the basic clue words have the highest accuracy among the three cohesion measures. 
Email summarization is also helpful for mobile email users on a small screen. 
In (Carenini et-al. 2007):
Since we are studying multiple email summarization in a conversational context,  we required that each conversation contained at least 4 emails. 
This clearly shows that the hidden emails do carry crucial information and have to be considered by the email summarization system. 
Not only does this study provide a gold standard to evaluate CWS and other summarization methods,  but it also sheds light on the importance of clue words and hidden emails to human summarizers. 
In (Zajic et-al. 2008):
Unlike news wire text,  which is meant for general consumption by a wide audience,  emails are only intended for their recipients. 
Email represents an instance of informal text – a broader genre that includes conversational speech,  blogs,  instant and SMS messages,  etc   Interest in automated processing techniques for informal media has been growing over the past few years for many reasons. 
Previous work has employed a corpus of emails sent among the board members of the ACM chapter at Columbia University(  Ram bow,  Shrestha,  Chen,  & Lauridsen,  2004). 
4.6-tSummarization of text through complex network approach
In (Antiqueira et-al. 2009):
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features. 
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
Automatic summarization Complex networks Network measurements Sentence extraction Summary informativeness. 
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary. 
The proposed method,  called CNSumm(  Complex Networks based Summarization) ,  consists of four steps. 
We have described the use of complex networks concepts for extractive summarization,  as well as its evaluation through standard informativeness scores and significance tests. 
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
This should perhaps be expected,  as the measurements of complex networks have already been shown to capture important features of texts. 
Complex networks concepts were considered potentially useful for the summarization task because they offer different,  often complementary,  views of a network,  and thus can be used to highlight a subset of its nodes. 
network measurements,  which are neither language nor domain dependent,  can be used for extractive summarization,  and can lead to informativeness scores close to the more linguistically complex and computationally costly systems. 
Complex networks have attracted a lot of attention. 
This type of complex network,  known as small world,  also shows high clustering,  i e. 
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Ferreira et-al. 2014):
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
The 2002 conference was the last one that proposed the contest to create generic multi document summaries. 
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
As already mentioned,  multi document summarization can be classified into generic and query based summarization. 
Alguliev and his collaborators(  Alguliev,  Aliguliyev,  & Mehdiyev,  2013)  propose a generic document summarization method which is based on sentence clustering. 
The same techniques used in single document summarization systems apply to multi document ones in multi document summarization some issues as the degree of redundancy and information diversity increase,  however. 
Other works in generic summarization apply clustering methods to achieve larger information diversity,  eliminating redundancy. 
Generic summarization systems extract the main ideas from a text collection,  while query based ones select sentences related to a specific query performed by the user. 
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
The system presented here generates a generic summarization and is completely unsupervised,  while in the other systems(  Alguliev et al,  b,  Atkinson and Munoz,  2013,  Radev et al,  2004)  the user has to provide an annotated corpus. 
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009):
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 
Section 2 presents the different text feature parameters,  Section 3 is about the proposed automatic summarization model,  Section 4 shows the experimental results and finally Section 5 presents conclusions and future work. 
With the huge amount of information available electronically,  there is an increasing demand for automatic text summarization systems. 
Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user(  Ye et al,  2007,  Steinberger et al,  2007,  Dorr and Gaasterland,  2007,  Diaz and Gerv s,  2007). 
Moreover,  we use all feature parameters to train feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  in order to construct a text summarizer for each model. 
In this paper,  we have investigated the use of genetic algorithm(  GA) ,  mathematical regression(  MR) ,  feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  for automatic text summarization task. 
Text summarization addresses both the problem of selecting the most important portions of text and the problem of generating coherent summaries. 
Unlike LexRank feature,  Bushy path is a simple and an effective text feature for single and multi document summarization task. 
The PNN approach performance evaluation based on precision(  English case). 
The PNN approach performance evaluation based on precision(  Arabic case). 
Sequential learning systems such as Hidden Markov Models have been exploited for the text summarization task,  but they cannot fully exploit the rich linguistic features since they have to assume independence among the features for tractability(  Conroy and O’Leary,  2001,  Jing,  2002). 
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Kulesza and Taskar 2012):
• An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;• A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;• A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
• The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
The DPP models outperform the baselines in most cases furthermore,  there is a signi cant boost in performance due to the use of DPP maximum likelihood training in place of logistic regression. 
introduced the task of temporal summarization,  which takes as input astream of news articles related to a particular topic,  and then seeks to extract sentences describing important events as they occur. 
In (Gupta and Lehal 2010):
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
Query based extractive text summarization. 
In query based text summarization. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
Multi document extractive summarization deals with extraction of summarized information from multiple texts written about the same topic. 
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011):
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 
Filatova and Hatzivassiloglou(  2004)  modeled extractive document summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
In particular,  we model text summarization as an integer linear programming problem. 
The diversity is very important evidence serving to control the redundancy in the summarized text and produce more appropriate summary. 
In automatic document summarization,  the selection process of the distinct ideas included in the document is called diversity. 
To tackle this pressing text information overload problem,  document clustering(  Aliguliyev,  2009,  Aliguliyev,  2009,  Wang et al,  2008)  and text summarization(  Aliguliyev,  2006,  Aliguliyev,  2010,  Alguliev and Alyguliev,  2008,  Alguliev and Aliguliyev,  2009,  Alguliev et al,  2005)  together have been used as a solution. 
For this reason,  document clustering and text summarization can be used for important components of information retrieval systems(  Yoo,  Hu,  & Song,  2007). 
► We model unsupervised generic text summarization as an optimization problem. 
The proposed model is quite general and can also be used for single and multi document summarization. 
That is why document clustering enables us to group similar text information and then text summarization provides condensed text information for the similar text by extracting the most important text content from a similar document set or a document cluster. 
summary should contain informative textual units that are relevant to the user(  2)  redundancy. 
Optimizing all three properties jointly is a challenging task and is an example of a global summarization problem. 
single and multi document summarization. 
Redundancy. 
redundancy in a summary will not be minimized. 
Formally we can formalize the document summarization problem as follows. 
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Ferreira et-al. 2013):
Extractive summarization Sentence scoring methods Summarization evaluation. 
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization. 
Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. 
In (Antiqueira et-al. 2009):
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
Although extractive summarization can produce texts that have cohesion and coherence problems,  many systems have been proven to yield summaries whose informative level is satisfactory. 
A graph,  or network,  is a representation that may capture text structure in various ways,  being therefore suitable for extractive summarization. 
In (Ferreira et-al. 2014):
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
Multi document summarization Extractive summarization Sentence clustering. 
The following sections detail the sentence scoring methods and the sentence clustering algorithm used here. 
In (Riedhammer et-al. 2010):
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain. 
The selected meeting extracts can then either be juxtaposed to form a short text summarizing a meeting or used as a starting point to enhance browsing experience Extractive summarization algorithms often rely on the measurement of two important aspects. 
We conclude on the be ne ts and drawbacks of the presented models and give an outlook on future aspects to improve extractive meeting summarization 2010 Else vier B.V. 
In (Gupta and Lehal 2010):
Text Summarization methods can be classified into extractive and abs tractive summarization. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
In (Azmi AM, Al-Thanyyan S 2012):
A comprehensive evaluation in Uz da et al  (  2008)  has concluded that automatic text summarization methods which are based on RST are better than extractive summarizers and those with hybrid methods produce worse summaries. 
In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. 
The Optimized Dual Classification System(  Sobh et al,  2006)  is an Arabic extractive text summarization system. 
In (Otterbacher et-al. 2009):
In the current paper,  we have also demonstrated the e ectiveness of our method as applied to two classical IR problems,  extractive text summarization and passage retrieval for question answering. 
An extractive summarization method that is almost equivalent to LexRank with cosine links was independently proposed in(  Mihalceaand Tarau,  2004). 
The ranked sentences were added to a summary one by one until the summary exceeded 250 words which was the limit in theDUC evaluations. 
In (Sipos et-al. 2012):
In this paper,  we present a supervised learning approach to training sub modular scoring functions for extractive multi document summarization. 
Work on extractive summarization spans a large range of approaches. 
In this way,  extractive summarization avoids the hard problem of generating well formed natural language sentences,  since only existing sentences from the input documents are presented as part of the summary. 
In (Ganesan et-al. 2010):
Extractive methods also tend to be verbose and this is especially problematic when the summaries need to be viewed on smaller screens like on a PDA. 
Graphs have been commonly used for extractive summarization(  e g,  LexRank(  Erk an and Radev,  2004)  and TextRank(  Mihalcea and Tarau,  2004) ) ,  but in these works the graph is often undirected with sentences as nodes and similarity as edges. 
Due to the subtle variations of redundant opinions,  typical extractive methods are often inadequate for summarizing such opinions. 
In (Alguliev et-al. 2013):
There are several most widely used extractive summarization methods as follows. 
The centroid based method,  MEAD,  is one of the popular extractive summarization methods(  Radev,  Jing,  Stys,  & Tam,  2004). 
In general,  document summarization can be divided into extractive summarization and abs tractive summarization. 
In (Rush et-al. 2015):
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. 
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 
The standard sentence summarization evaluation set is associated with the DUC2003 and DUC2004 shared tasks(  Over et al,  2007). 
In (Ye et-al. 2007):
In this paper,  we review and detail our approach to automatic,  multi document extractive summarization. 
Given the notion of semantic concepts,  the extractive summarization approach is equivalent to picking a set of sentences that represent as many salient concepts as possible. 
In both extractive and abs tractive summarization,  a key consideration is how to properly represent the knowledge contained in the input document. 
In (Hearst 1997):
All three scoring methods make use only of patterns of lexical co occurrence and distribution within texts,  eschewing other kinds of discourse cues. 
Some summarization algorithms extract sentences directly from the text. 
Boundary identification is done identically for all lexical scoring methods,  and assigns a depth score,  the depth of the valley(  if one occurs) ,  to each token sequence gap. 
In (Kulesza and Taskar 2012):
• The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
We employ this learning algorithm to perform toper form extractive summarization of news text. 
For instance,  consider extractive document summarization,  where the goal is to choose a subset of the sentences in a news article that together form a good summary of the entire article In this setting Y is the set of sentences in the news article being summarized,  thus Y is not xed in advance but instead depends on context. 
In (0035):
Asubset of these documents was randomly selected for the experiments to be reported in this section In the first experiment,  using automatically generated reference extractive summaries,  we employed four text summarization methods,  as follows. 
In this paper we address the automatic summarization task Recent research works on extractive summary generation employ some heuristics,  but few works indicate how to select the relevant features We will present a summarization procedure based on the application of trainable Machine Learning algorithms which employs a set of features extracted directly from the original text. 
Hence,  the notion of a collection of documents in IR can be replaced by the notion of a single document in text summarization. 
In (Harabagiu S, Lacatusu F 2005):
We have evaluated each of these summarization methods using a number of standard techniques,  including the ROUGE automatic scoring packages and the manual Pyramid evaluation method. 
In order to evaluate the quality of summaries with ROUGESU4,  we take into account that this scoring method is insensitive to the quality of ordering methods used in MDS. 
To evaluate the performance of clustering the theme sentences,  we used the same 10 topics as in the evaluation of the paraphrase discovery. 
In (Alguliev et-al. 2011):
A transductive approach(  Amini & Usunier,  2009)  for extractive multi document summarization identifies topic themes within a document collection,  which help to identify two sets of relevant and irrelevant sentences to a question. 
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013):
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
In fact,  on the one hand,  since they do not consider the underlying correlations among multiple terms,  some relevant facets of the analyzed data could be disregarded. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
Hence,  there is a need for novel graph based summarizers that also consider the correlations among multiple terms and the differences between positive and negative term correlations. 
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum(  Graph based Summarizer). 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
The correlations among multiple terms are extracted using an established data mining technique,  i e,  association rule mining. 
• using association rules in graph based summarization to represent correlations among multiple terms,. 
In (Kulesza and Taskar 2012):
In the rst,  we subtract a multiple of one of the vectors in V from all of the other vectors so that they are zero in the ithcomponent,  leaving us with a set of vectors spanning the subspace of V orthogonal to ei. 
We assume that the conditional DPP kernel L(  X ; θ)  is parameterized in terms ofa generic θ,  and let denote the conditional probability of an output Y given input X under parameter θ. 
Our input will be a still image depicting multiple people,  and our goal is to simultaneously identify the poses the positions of the torsos,  heads,  and left and right arms of all the people in the image. 
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Alguliev et-al. 2013):
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
DE is a simple yet efficient evolutionary algorithm,  which has been widely applied to solve continuous optimization problems(  Price,  Storn,  & Lampinen,  2005). 
(  1)  content coverage,  summary should contain salient sentences that cover the main content of the documents(  2)  diversity,  summaries should not contain multiple sentences that convey(  carry)  the same information and(  3)  length,  summary should be bounded in length. 
To solve the optimization problem has been created an improved differential evolution algorithm. 
According to Mani and May bury(  1999) ,  automatic text summarization takes a partially structured source text from multiple texts written about the same topic,  extracts information content from it,  and presents the most important content to the user in a manner sensitive to the user s needs. 
The multi document summarization task has turned out to be much more complex than summarizing a single document,  even a very large one. 
The algorithm can adjust crossover rate adaptively according to the fitness of individuals. 
Nowadays,  without browsing the large volume of documents,  search engines such as Google,  Yahoo. 
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Yang et-al. 2014):
Recall that our goal is to obtain more accurate sentence clusters and generate good summaries in theme based summarization. 
Thus,  good sentence clusters are the guarantee of good summaries in theme based summarization. 
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters. 
Ranking based clustering Sentence clustering Theme based summarization. 
Ranking based clustering framework shows the best performance,  it further presents ranking distribution of documents and terms can help generate more accurate sentence clusters. 
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information. 
Based on it,  a ranking based sentence clustering framework is developed. 
To help alleviate this problem,  we argue in this paper that a term can be deemed as an independent text object instead of a feature of a sentence. 
We contribute to ranking distributions of terms and documents can help generate more accurate sentence clusters. 
The advantages of the proposed ranking based clustering framework are clearly demonstrated in the above tables. 
During clustering,  sentences are the objects first to be clustered at each iteration,  and links to the documents and terms are used to help clustering sentence. 
In (Harabagiu S, Lacatusu F 2005):
Second,  we believe that our results represent a comprehensive and replicable study,  which demonstrates the effectiveness of a structured,  theme based approach to multi document summarization. 
We first discuss a total of six different sentence extraction methods(  EM1– EM6)  which correspond to the four baseline topic representation techniques(  TR1–TR4) ,  introduced in Section 2,  plus the two topic theme based representations based on TR5 introduced in Section 3.16 For ease of exposition,  we will refer to the graph based theme representation as TH1 and the linked list based theme representation as TH2. 
We believe that these results prove the benefits of using the theme based topic representation for multi document summarization. 
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010):
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
All features are computed over primitives,  syntactic,  linguistic,  or knowledge based information units extracted from the sentences. 
Text summarization based on fuzzy logic system architecture. 
If users create query words the way they create for information retrieval,  then the query based summary generation would become generic summarization. 
Automatic text summarization based on fuzzy logic. 
It is an extraction based multi document summarization system. 
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 
In query based text summarization. 
This approach is less expensive and more robust than a summarization technique based entirely on a single method. 
Query based extractive text summarization. 
In (Fung P, Ngai G 2006):
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
A nal meta summarizer is used to summarize multiple documents using the HMSM state labels. 
For multi document summa rization,  the state labels of individual documents are used to group multiple summaries together into a single meta summary,  greatly simplifying the pro cess. 
In (Harabagiu S, Lacatusu F 2005):
We expect that theme based representations can be used to organize topic relevant information from multiple sources,  extracted from either, (  1)  a single sentence, (  2)  a cluster of sentences, (  3)  a discourse fragment,  or even(  4)  a cluster of documents. 
In Section 2,  we considered five different baseline topic representations that use terms(  TR1) ,  relations(  TR2) ,  document segments(  TR3) ,  sentence clusters(  TR4) ,  or semantic frames generated from multiple different documents(  TR5). 
We separately evaluated the quality of compression when different topic representations were available to the summarization system. 
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015):
Many extraction based summarization methods have been proposed in the past years. 
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 
With the construction of feature groups,  a latent variable is utilized to control the selection of them. 
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
Therefore,  in this paper,  we attempt to devise an approach to the generation of the topic aspect oriented summarization according to topic factors,  we call this TAOS(  Topic AspectOriented Summarization). 
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 
abstract based and extraction based. 
Documents summarization can be generally categorized as two approaches. 
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 
Of particular interest to us in this paper is the extractive multi document summarization,  where the obtained final summary is a subset of the elements from multiple input documents. 
BagofVisualWord is a image representation based on BoW(  BagofWord)  model. 
For image summarization task,  we also extract three kinds of features for each image. 
A natural way to generate topic aspect oriented summarization is to assume that topic aspects can prefer some feature groups,  and the group sparsity can be introduced. 
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
Folksonomy systems are classification systems derived from social tags assigned to multimedia content by everyday users. 
Given multiple documents that need to be summarized,  we first perform a preprocessing step so that the documents can be analyzed at different granularities(  i e,  word level and sentence level). 
In this paper,  we propose FoDoSu which is a novel multi document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. 
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Khan et-al. 2015):
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling. 
First,  it employs semantic role labeling for semantic representation of text. 
Our work,  in contrast,  aims to treat this limitation by using semantic role labeling(  SRL)  technique to build semantic representation from the document text automatically. 
To the best of our knowledge,  semantic role labeling(  SRL)  technique,  which exploits semantic role parser,  has not been employed for the semantic representation oftextin multi document abs tractive summarization. 
First,  it employs semantic role labeling to extract predicate argument structure(  semantic representation)  from the contents of input documents. 
introduced a work that combined semantic role labeling with general statistic method(  GSM)  to determine important sentences for single document extractive summary. 
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015):
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization. 
In fact,  our model incorporate a two level sparse representation model. 
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (Fung P, Ngai G 2006):
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
A nal meta summarizer is used to summarize multiple documents using the HMSM state labels. 
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
For multi document summa rization,  the state labels of individual documents are used to group multiple summaries together into a single meta summary,  greatly simplifying the pro cess. 
An unsupervised segmental Kmeans method is used to iteratively cluster multiple documents into different topics(  stories)  and learn the parameters of parallel Hidden Markov Story Models(  HMSM) ,  one for each story. 
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a):
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 
We develop a Ranking framework upon Recursive Neural Networks(  R2N2)  to rank sentences for multi document summarization. 
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 
In this parsing tree,  recursive neural networks measure the importance of all these non terminal nodes. 
This process is modeled by recursive neural networks(  RNN). 
This paper presents a Ranking framework upon Recursive Neural Networks(  R2N2)  for the sentence ranking task of multi document summarization. 
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015):
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 
Therefore,  in our work,  computing importance,  non redundancy and coherence is tightly integrated and taken care of simultaneously Our graph based summarization technique has the further advantage of being completely unsupervised We apply our graph based summarization technique to scienti c articles from the journal PLOS Medicine,  a high impact open access journal from the medical domain. 
Hence,  we optimize importance,  non redundancy and coherence simultaneously. 
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015):
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 
Our approach is based on sub modular maximization and mixture learning,  which has been successfully used in applications such as document summarization(  Lin,  2012)  and image summarization(  Tschiatschek et al,  2014) ,  but has never been applied to topic identification tasks or,  more generally,  DAG summarization. 
Given a collection of articles spanning different topics,  but with similar titles,  automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy as a topic DAG. 
Furthermore,  our approach also generalizes these clustering approaches,  since one of the components in our mixture of sub modular functions is defined via these unsupervised approaches,  and maps a given clustering to a set of topics in the hierarchy. 
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015):
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates. 
Our system predicts sentence salience in the context of a large scale event,  such as a disaster,  and integrates these predictions into a clustering based multi document summarization system. 
The AP+SALIENCE model is better able to find salient updates earlier on for the disaster domain,  this is an especially important quality of the model. 
4.23.11-tSummarizing multiple documents through system combination
In (Goldstein et-al. 2000):
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
This is the focus of our work reported here,  including the necessity to eliminate redundancy among the information content of multiple related documents. 
Specifically,  we are constructing sets of 10 documents,  which either contain a snapshot of an event from multiple sources or the unfoldment of an event over time. 
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015):
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
In total,  therefore,  when employing ROUGE for the evaluation of summarization systems,  there are 192(  8 x 2 x 2 x 3 x 2)  possible system level variants to choose from. 
Automatic metrics of summarization evaluation have their origins in machine translation(  MT) ,  with ROUGE(  Lin and Hovy,  2003) ,  the first and still most widely used automatic summarization metric,  comprising an adaption of the BLEU score(  Papineni et al,  2002). 
Table 4 shows correlations of BLEU and the top ten performing variants of ROUGE when evaluated against the arithmetic(  mean) ,  harmonic and geometric mean of quality and coverage scores for summaries. 
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (CR100):
In light of this,  Text Summarization(  TS)  is of great help since its main aim is to produce a condensed new text containing a significant portion of the information in the original text(  s)  The process of summarization can be divided into three stages. 
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed(  i e,  COMPENDIUM) ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted. 
This paper focuses on abs tractive text summarization. 
attempt to transform an extractive summarization into an abs tractive one in the context of meeting summarization by performing sentence compression. 
In order to solve this limitation,  besides checking for the correctness of the sentences once they have been generated and filtering out those ones,  which do not satisfy the proposed constraints,  we would also need to apply some constraints based on the information sentences contain,  optimizing the set of generated sentences,  so that only the best ones with respect to their content are used With respect to the general results of the abstractiveapproaches,  since the length of the summaries is restricted to only 100 words,  when selecting the most important sentences before or after generating new sentences,  some of the oft he concepts may not be included. 
On the Ont he one hand,  we try to elucidate the reasons why the Graph COMPENDIUM approach performs worse than theCOMPENDIUM Graph,  and on the other hand,  we want to analyze the reasons of the low overall performance of theabstractive approaches Regarding the first type of analysis carried out,  if we use the word graph based method for generating new sentences first,  and use all of them as input for COMPENDIUM,  thisTS tool can have difficulties in selecting important content This occurs because many of the sentences will start with the same words(  e g,  if we take the top 10 words with highest tfidf) ,  so once COMPENDIUM detects a specific fragment of information as relevant,  sentences containing the same portion of information that have not been detected as redundant will be also selected,  leading to summaries that have not much variation in content. 
In these cases,  all the stages of the summarization process are taken into account Due to the difficulty associated to the generation of abstracts,  most approaches only focus on the first stage(  i e,  topic identification) ,  producing extracts as a result. 
In contrast,  abs tractive approaches require a more elaborate process,  involving sentence compression,  information fusion,  and or language generation. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
Knowledge poor approaches do not consider any knowledge pertaining to the domain to which text summarization is applied therefore,  knowledge poor approaches can be easily applied to any domain(  Abracos & Lopes,  1997; Gong & Liu,  2001; Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; May eng & Jang,  1999; Salton et al,  1997; Mitra,  Singh al,  & Buckley,  1997). 
In this paper,  we propose two text summarization approaches. 
outperforms keyword based text summarization approaches. 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
The foundation of knowledge rich approaches is the assumption that understanding of the meaning of a text can benefit the generation of a good summary(  Aone et al,  1997; Azzam et al,  1999; Barzilay & Elhadad,  1997; Hovy & Lin,  1997; McKeown & Radev,  1995). 
In (Glavaš G, Šnajder J 2014):
Furthermore,  the results suggest that similar event based approaches could be used to address closely related NLP tasks such as text simplification. 
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
We evaluate the performance using ROUGE1 and ROUGE2 evaluation metrics(  Lin,  2004) ,  the most commonly used automated evaluation metrics for text summarization. 
Nevertheless,  studies on event based text summarization are rare(  Daniel,  Radev,  & Allison,  2003; Filatova & Hatzivassiloglou,  2004; Li,  Wu,  Lu,  Xu,  & Yuan,  2006). 
Third,  we provide an overview of event based approaches to text summarization. 
In Section 2,  we provide an overview of work on event processing in NLP and TDT and its applications in IR and text summarization. 
In this work,  we aim to bridge that gap,  and we propose event oriented retrieval and summarization models based on sentence level event extraction. 
We argue that the most relevant information in event oriented texts is the event mentions and the relations in which they stand to each other,  while all other information is event unrelated and may be considered less relevant. 
In (Genest PE, Lapalme G 2011):
Most systems develop ped for the main international conference on text summarization,  the Text Analysis Conference(  TAC) (  Owczarzak and Dang,  2010) ,  predominantly use sentence extraction,  including all the top ranked systems,  which make only minor post editing of extracted sentences(  Conroy et al,  2010) (  Gil lick et al,  2009) (  Ge nest Gen est Gene st et al,  2008) (  Chen et al,  2008). 
This low linguistic score is understandable,  because this was our first try at text generation and abs tractive summarization,  whereas the other systems that year used sentence extraction,  with at most minor modifications made to the extracted sentences. 
The kind of texttotext generation involved in our work is related to approaches in paraphrasing(  Androutsopoulos and Malakasiotis,  2010). 
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text. 
Our first attempt at full abs tractive summarization took place in the context of the TAC 2010 multi document news summarization task. 
Summarization approaches can generally be categorized as extractive or abs tractive(  Mani,  2001). 
We believe that a fully abs tractive approach requires a separate process for the analysis of the text that serves as an intermediate step before the generation of sentences. 
In (Alguliev et-al. 2011):
Tao et al  (  2008)  have designed word based and sentence based association networks(  WAN and SAN for short,  respectively)  and proposed word and sentence weighting approaches based on how much co occurrence information they contain,  and applied to text summarization. 
Many approaches have been proposed for text summarization based on the diversity. 
In this paper,  we focus on the unsupervised generic text summarization,  which generates a summary by extracting key textual units in given document collection. 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
The proposed generic text summarization model is presented in Section 3. 
Though text summarization has drawn attention primarily after the information explosion on the Internet,  the first work has been done as early as in the s(  Luhn,  1958). 
For this reason,  document clustering and text summarization can be used for important components of information retrieval systems(  Yoo,  Hu,  & Song,  2007). 
That is why document clustering enables us to group similar text information and then text summarization provides condensed text information for the similar text by extracting the most important text content from a similar document set or a document cluster. 
In (Zajic et-al. 2008):
The recent work of Carenini et al  (  2007)  examines extractive approaches to summarization on Enron data that leverage graphs defined by quoted texts. 
Finally,  evaluation issues in general present challenges for text summarization. 
Section 3 describes our general framework for text summarization and specific approaches we have developed for email thread summarization. 
We attempted to present our summarization systems with text as clean as possible. 
These two compression techniques represent different trade offs that we think are particularly salient for informal text. 
We conducted a variety of experiments to explore the problem of email thread summarization. 
For Trimmer,  proper compression depends on correct parse trees,  and parsers trained on news wire text(  like the one we use)  are likely to make many errors. 
Repetitions of text from earlier messages(  quoted text)  were also eliminated. 
We apply both methods to the problem of email thread summarization. 
All email messages were preprocessed before they were presented to our summarization systems. 
7-tMultilingual approaches for text summarization
In (Baralis et-al. 2013):
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
To effectively address the summarization problem in a multilingual context in. 
Multi document summarization Text mining Association rule mining Graph ranking. 
The multi document summarization task entails generating a summary of a collection of textual documents. 
The raw textual content is commonly unsuitable for use in item set items et and association rule mining. 
GraphSum performs better than many stateoftheart approaches,  including those that heavily rely on advanced semantics based models(  e g,  ontologies)  or complex linguistic processing steps. 
As future work,  we plan to(  i)  adapt and evaluate the proposed summarizer into a multilingual contest(  e g,  the TAC’11 contest. 
)  to further improve the summarization performance. 
with language dependent text processing. 
to tackle the summarization problem by combining complex network analysis. 
To ease the knowledge discovery process,  a significant research effort has been devoted to studying and developing automated summarization tools,  which produce a succinct overview of the most relevant document content,  i e,  the summary. 
Nowadays Internet provides access to a huge number of electronic textual documents. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
Hence,  the proposed summarizer is potentially applicable to documents coming from rather different application contexts. 
) ,  the graph nodes could also represent a subset of terms with size higher than one(  e g,  Analysis,  Context). 
Unlike previous approaches(  e g,. 
The statistically relevant differences in the comparisons between GraphSum and the other approaches are starred in Table 5. 
Note that,  in our context,  assuming that a document is a representative summary of the rest of the collection is a good approximation,  because we specifically cope with documents that range over the same topic. 
Comparisons between GraphSum and the other approaches. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
Knowledge poor approaches do not consider any knowledge pertaining to the domain to which text summarization is applied therefore,  knowledge poor approaches can be easily applied to any domain(  Abracos & Lopes,  1997; Gong & Liu,  2001; Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; May eng & Jang,  1999; Salton et al,  1997; Mitra,  Singh al,  & Buckley,  1997). 
In this paper,  we propose two text summarization approaches. 
outperforms keyword based text summarization approaches. 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
The foundation of knowledge rich approaches is the assumption that understanding of the meaning of a text can benefit the generation of a good summary(  Aone et al,  1997; Azzam et al,  1999; Barzilay & Elhadad,  1997; Hovy & Lin,  1997; McKeown & Radev,  1995). 
In (Glavaš G, Šnajder J 2014):
Furthermore,  the results suggest that similar event based approaches could be used to address closely related NLP tasks such as text simplification. 
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
We evaluate the performance using ROUGE1 and ROUGE2 evaluation metrics(  Lin,  2004) ,  the most commonly used automated evaluation metrics for text summarization. 
Nevertheless,  studies on event based text summarization are rare(  Daniel,  Radev,  & Allison,  2003; Filatova & Hatzivassiloglou,  2004; Li,  Wu,  Lu,  Xu,  & Yuan,  2006). 
Third,  we provide an overview of event based approaches to text summarization. 
In Section 2,  we provide an overview of work on event processing in NLP and TDT and its applications in IR and text summarization. 
In this work,  we aim to bridge that gap,  and we propose event oriented retrieval and summarization models based on sentence level event extraction. 
We argue that the most relevant information in event oriented texts is the event mentions and the relations in which they stand to each other,  while all other information is event unrelated and may be considered less relevant. 
In (Genest PE, Lapalme G 2011):
Most systems develop ped for the main international conference on text summarization,  the Text Analysis Conference(  TAC) (  Owczarzak and Dang,  2010) ,  predominantly use sentence extraction,  including all the top ranked systems,  which make only minor post editing of extracted sentences(  Conroy et al,  2010) (  Gil lick et al,  2009) (  Ge nest Gen est Gene st et al,  2008) (  Chen et al,  2008). 
This low linguistic score is understandable,  because this was our first try at text generation and abs tractive summarization,  whereas the other systems that year used sentence extraction,  with at most minor modifications made to the extracted sentences. 
The kind of texttotext generation involved in our work is related to approaches in paraphrasing(  Androutsopoulos and Malakasiotis,  2010). 
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text. 
Our first attempt at full abs tractive summarization took place in the context of the TAC 2010 multi document news summarization task. 
Summarization approaches can generally be categorized as extractive or abs tractive(  Mani,  2001). 
In (Alguliev et-al. 2011):
Tao et al  (  2008)  have designed word based and sentence based association networks(  WAN and SAN for short,  respectively)  and proposed word and sentence weighting approaches based on how much co occurrence information they contain,  and applied to text summarization. 
Many approaches have been proposed for text summarization based on the diversity. 
In this paper,  we focus on the unsupervised generic text summarization,  which generates a summary by extracting key textual units in given document collection. 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
The proposed generic text summarization model is presented in Section 3. 
Though text summarization has drawn attention primarily after the information explosion on the Internet,  the first work has been done as early as in the s(  Luhn,  1958). 
For this reason,  document clustering and text summarization can be used for important components of information retrieval systems(  Yoo,  Hu,  & Song,  2007). 
8-tSummary evaluation
8.1-tInformativeness evaluation
In (Patel et-al. 2007):
So,  evaluation in terms of informativeness is usually preferred. 
The first is Quality evaluation and the second is an informativeness evaluation. 
One of the measures in informativeness methodology for extractive summary evaluation is content based evaluation i e. 
Thus,  after preprocessing we have made the text suitable for extracting important sentences. 
So,  the next step is to carry out sentence analysis and to determine sentence weight. 
Degree of information content of a sentence is represented by sentence weight,  which is computed as follows. 
Where W(  s)  is an intermediate sentence weight,  WD and WT are the term weights of those sentence terms,  which belong to the document feature vector and the theme feature vector,. 
respectively. 
α and β are constants whose values depend one the language being considered for the summarization. 
If a term is repeating more than once in a sentence,  only one instance is considered for computing intermediate sentence weight. 
3.2 Sentence Reference Index. 
One of the problems of summary generation is the risk of extracting a sentence,  which is not complete by itself as it makes reference to previous sentence(  s). 
This problem has been handled by analyzing the sentence for the presence of certain set of terms or phrases within positional constraints. 
19. 
Our algorithm tags the sentences making references to previous sentences. 
This paper describes an efficient algorithm for language independent generic extractive summarization for single document. 
This process is repeated recursively updating the tag values appropriately. 
It thus. 
Conference RIAO2007,  Pittsburgh PA,  U.S.A. 
May June 1,  2007 - Copyright C.I.D. 
Paris,  France. 
generates what we call as Sentence Reference Index. 
This index is used to enhance information content of a sentence. 
The intermediate sentence weight W(  s)  is enhanced for those sentences whose following sentences make reference to it. 
This is based on the following hypothesis. 
If more is said(  written)  in the following sentence(  s)  about the contents of the current sentence,  it implies higher importance(  richness)  of content of the current sentence. 
In order to calculate the Sentence Reference Index,  we are maintaining an external table for the words that give the indication for previous sentence reference. 
We have collected a list of such words for English,  Hindi,  Gujarati and Urdu languages. 
Theme Feature Vector(  For Hindi Documents). 
Title Feature Vector vs. 
Table. 
Some examples which support this approach are shown in following Tables. 
First,  maintaining a list of names and every time a word comes it is checked against this list. 
If it is there then this information can be used for increasing the weight of the word. 
Second,  check each word in the dictionary(  vocabulary)  of respective language. 
If word is not found in the dictionary then it is considered to be a name. 
Both of these approaches involve a lot of overhead and the execution of algorithm becomes slow. 
We have used a novel way to handle this issue with satisfactory results. 
We consider only those words as names which are within single double quotes. 
And we enhance the weight of sentence in as described in section. 
Using the threshold as 30% of the highest frequency,  the resulting vector is called the Document Feature Vector. 
For English. 
We have proposed an enhanced feature vector i e. 
Theme feature vector which tells more about the central theme of the document. 
In order to form this,  we extract frequent terms from the title and merge these with the frequent proper nouns vector. 
However,  we can provide this table for other languages as well without any change in algorithm. 
The resulting vector is termed as Theme Feature Vector. 
3,  5,  18. 
,  because in many cases the title feature vector is not richly suggestive of the core idea(  s)  of the document whereas the proposed Theme Feature Vector is rich in conveying the central idea(  s)  of the document. 
Examples to substantiate the above hypotheses are presented in Table below. 
Forming theme feature vector for documents which are not in English is somewhat challenging as we have no direct accessibility of proper nouns in the document. 
However,  as said earlier,  we are considering the names in quotes as special nouns and including them into theme feature vector. 
However,  only taking such special nouns as theme words may lead to following two problems. 
The documents not contain quoted names may have empty theme feature vector. 
Even theme feature vector of documents containing quoted names,  may not be. 
We have successfully handled the above problems by applying threshold α on document feature vector and including the top most frequent terms from it in our theme feature vector. 
The title words are included in theme feature vector only if title word appears in Document feature vector above threshold β. 
Initially,  we have considered following criteria. 
λ1 = 50% of highest term frequency λ2 = max of. 
25% of highest term frequency,  2. 
This is a significant improvement over title feature vector as used by most of the researchers. 
Two obvious approaches. 
The Sentence Reference Index,  as mentioned above,  helps in identifying the number of sentences,  which make reference(  cascaded reference)  to the current sentence. 
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (Baralis et-al. 2012):
toolkit,  which has been adopted as official DUC’04 tool for performance evaluation. 
toolkit. 
Others considered previously selected patterns in item set items et evaluation to reduce model redundancy. 
Experimental results,  performed on the DUC’04 document collection by means of ROUGE toolkit,  show that the proposed approach achieves better performance than a large set of competitors. 
Sentence evaluation and selection steps consider(  i)  a sentence relevance score that combines the tfidf statistics. 
trjk(  Ii)  =(  1 if Ii ⊆ trjk,  0 otherwise(  2)  The coverage of a sentence sjk with respect to the pattern based model is defined as the number of ones that occur in the corresponding coverage vector SCjk. 
We formalize the problem of selecting the most informative and not redundant sentences according to the model as a set covering problem. 
The set covering problem. 
The set covering optimization problem focuses on selecting the minimal set of sentences,  of arbitrary size l and maximal score,  whose logic OR of the corresponding coverage vectors,  i e,  SC∗=SC1 ∨. 
8.4-tText summarization evaluation programs
In (Neto et-al. 2000):
This paper describes a text mining tool that performs two tasks,  namely document clustering and text summarization. 
Our text summarization algorithm is based on computing the value of aTFISF(  term frequency – inverse sentence frequency)  measure for each word,  which is an adaptation of the conventional TFIDF(  term frequency – inverse document frequency)  measure of information retrieval. 
However,  the textual,  unstructured nature of documents makes these two text mining tasks considerably more difficult than their data mining counterparts. 
document clustering and text summarization. 
Section 6 introduces the method developed for text summarization,  and section 7 presents the results of experiments evaluating the performance of the system in this task. 
In addition,  we also weal so added to our system a text summarization algorithm,  which can be effectively used to summarize individual documents. 
We believe these two kinds of summarization are complementary to each other,  and together they significantly help the user to better understand the information stored in the documents being mined The system has been evaluated on real world documents and the results are satisfactory,  as will be described later This paper is organized as follows. 
The main differences between data mining and text mining is as follows. 
By contrast,  text mining deals with unstructured or semi structured data,  namely the text found in articles,  documents,  etc In addition to the availability of little(  if any)  structure in the text,  there are other reasons why text mining is so difficult. 
These keywords can be regarded as an ultra compact “ summarization of the contents of that cluster. 
Text mining is an emerging field at the intersection of several research areas,  including data mining,  natural language processing,  and information retrieval Feldman & Dagan 95. 
a document clustering algorithm or a text summarization one The document clustering algorithm uses the Auto class algorithm Cheese man et al   88. 
Sentences with high values of TFISF are selected to producea summary of the source text. 
The concepts contained in a text are usually rather abstract and can hardly be modeled by using conventional knowledge representation structures Furthermore,  the occurrence of synonyms(  different words with the same meaning)  and homonyms(  words with the same spelling but with distinct meanings)  makes it difficult to detect valid relationships between different parts of the text In this paper we describe a system that performs two important text mining tasks. 
The method is described in detail in section Note that the document clustering and the text summarization algorithms can be used together in a synergistic,  complementary way. 
In order to make our experiments more relevant,  we have used the evaluation thee valuation framework of a large scale project for evaluating text summarization algorithms. 
In this case it runs a summarization algorithm that extracts the most relevant sentences from a document. 
However,  the textual,  unstructured nature of documents makes these two text mining tasks considerably more difficult than their data mining counterparts Among the several obstacles to be faced in text mining,  two are the focus of this paper. 
In (Dunlavy et-al. 2007):
We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences(  DUC)  as measured by the best known automatic metric for summarization system evaluation,  ROUGE. 
Information retrieval Latent semantic indexingClusteringSummarizationText processing Sentence trimming. 
In many of the DUC evaluations,  similar lead sentence summaries have been used as baselines,  representing a summarization approach requiring minimal text and or natural language processing. 
Although the DUC data and evaluations were originally designed to test multi document summarization,  we developed a framework to extend it to the task of evaluation for each of the three components. 
Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001–2006 Document Understanding Conferences The focus of these conferences is the evaluation and discussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. 
Several tasks included in previous DUC evaluations focused on multi document summarization for clusters of documents,  and our participation in these evaluations led to the development of QCS. 
For each cluster,  query scores and document names are given,  with hyperlinks to the text of the documents in the Q subsection. 
The navigation bar contains links to the documents and is organized to reflect the output from the querying,  clustering and summarization modules. 
In (Hovy et-al. 2006):
The text summarization community has also searched for automatic summary evaluation methods that produce reliable scores that correlate well with human scoring. 
A good automatic summarization evaluation procedure should be able to differentiate good systems from bad ones. 
32 automatic summarization systems participated to create question focused summaries by answering a list of complicated questions from sets of 2550 texts. 
To examine the validity of our method,  we have tested the BE framework and its current implementation thoroughly using previous DUC evaluation results,  namely DUC2002 and 2003,  on single and multi document summarization tasks,  and very short headline generation task. 
Another important property that a good automated evaluation procedure possesses is that it must show good and consistent correlation across evaluations of different summarization tasks. 
Table 2 shows the correlation between BE and ROUGE,  a widely used and recognized automated summarization evaluation method(  Lin and Hovy,  2003). 
DUC2005 is the first time that query based summarization has been performed on a large scale. 
BE breakers(  that create individual BE units,  given a text) ,  BE matchers(  that rate the similarity of any two BE units) ,  and BE scorers(  that assign a score to each BE unit individually). 
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193–197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764–7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675–1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514–14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196–206
0074 Amigo et-al. 2005 Amigó E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL ’05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280–289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584–599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260–273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553–563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208–1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SAC’12), pp 782–786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96–109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366–377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL ’05), pp 141–148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107–117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829–833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335–336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91–100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353–361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85–112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759–777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, O’Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588–1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613–1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755–5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780–5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1–16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340–348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64–73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1–39
0037 Glavaš G, Šnajder J 2014 Glavaš G, Šnajder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904–6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40–48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128–137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203–225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717–727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258–268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620–1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSP’06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33–64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212–225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515–1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107–117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81–94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382–386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591–594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319–322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608–1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737–747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61–66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255–262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695–1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366–1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788–791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462–471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887–902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490–500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778–787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74–81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912–920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168–178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61–66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159–165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404–411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132–138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41–55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319–324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42–54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227–237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190–198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210–218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298–1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123–132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79–88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1–4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919–938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801–815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419–430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206–213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177–184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862–2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1–8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224–233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112–9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341–359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513–523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948–961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37–50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643–1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75–95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600–1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35–41
