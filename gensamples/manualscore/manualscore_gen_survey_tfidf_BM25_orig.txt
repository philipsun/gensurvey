root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (Leite and Rino 2006):
Need of a Corpus. 0 0
 Compression Rate = 30%(  extract length / source text length). 0 0
Two Main Approaches for Automatic Summarization. 0 0
John,  G. 0 0
3364. 0 0
Computational Linguistics,  23(  1) ,  pp. 0 0
Segmenting Text into MultiParagraph Subtopic Passages,. 0 0
TextTiling. 0 0
(  1997). 0 0
Hearst,  M. 0 0
Morgan Kaufmann Publishers. 0 0
359366,  San Francisco,  CA. 0 0
In Proceedings of the International Conference on Machine Learning,  pp. 0 0
Correlation based feature selection of discrete and numeric class machine learning. 0 0
(  2000). 0 0
Hall,  M. 0 0
In Proceedings of IJCAI’93. 0 0
for classification learning. 0 0
Multi interval discretization of continuous valued attributes. 0 0
(  1993). 0 0
Fayyad,  Usama ; Irani,  Keki. 0 0
111121,  1999. 0 0
MIT Press,  pp. 0 0
(  eds) ,  Advances in Automatic Text Summarization. 0 0
May bury. 0 0
Mani and M.T. 0 0
; Langley,  P. 0 0
(  1995). 0 0
Estimating continuous distributions in Bayesian classifiers. 0 0
Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence(  pp. 0 0
potencial e. 0 0
A migra o do SuPor para o ambiente WEKA. 0 0
2-tVarious types of text Summarization
In (Gupta V 2013):
Various features used for summarizing multilingual HindiPunjabi text are given below. 0 0
Various features used in this algorithm for summarizing multilingual Hindi and Punjabi text are. 0 0
Various automatic text summarization systems are commercially or non commercially available for most of the commonly used natural languages. 0 0
Various HindiPunjabi noun words which are in bold,  italics or underlined font or having high scores for TF(  TermFrequency). 0 0
Automatic text summarization. 0 0
has been used as model to estimate the text features weights for HindiPunjabi text summarization. 0 0
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 0 0
In (Nobata et-al. 2001):
Various clues have been used for sentence extraction. 0 0
Automated text summarization. 0 0
Our system can use three types of thresholds. 0 0
One of the oft he types of missing sentences are short sentences. 0 0
In recent years,  we can see a vast amount of machine readable documents on the web or machine readable text To obtain useful information from these documents ef ciently,  there are several ongoing research elds in natural language processing,  such as information retrieval,  information extraction,  automatic summarization Sentence extraction is one of useful methods for automatic text summarization. 1 0
However,  Sum has better results than the full text on both the average time and the Fmeasure,  which shows the effectiveness of Sum for the IR task. 0.5 0
In (Patel et-al. 2007):
Two types of intrinsic evaluations are generally carried out in summarization. 0 0
Various methods of scoring the relevance of sentences or passages and combining the scores are described in. 0 0
Two approaches are generally followed for automatic text summarization research. 0  0
Partition the text as suggested in the previous subsection(  i e. 0 0
The whole text is partitioned into number of parts as per following scheme. 0 0
Thus,  after preprocessing we have made the text suitable for extracting important sentences. 0 0
Location feature depends on the genre of the text document and author s style. 0 0
The need for text summarization methods that can handle multiple languages appear to be growing. 0 0
In (Gupta et-al. 2011):
Various frequency measures are also used by(  Edmund son 1969; Kupiec et al   1995; Teufel and Mo ens 1999; Hovy and Lin 1999). 0 0
We have used the General Architecture for Text Engineering(  GATE)  tool for POS tagging and searching the patterns of tags corresponding to these types of phrases e g. 0 0
Our approach for summarizing the text determines more significant and topically related sentences in the text and thus outputs a higher summary quality. 1 0.5
The summarizer can be also improved by utilizing linguistic information contained in text documents. 0 0
The future extension will concentrate on web text retrieval according to a given interest profile. 0 0.5
In (Antiqueira et-al. 2009):
Moreover,  Mihalcea defines three types of networks. 0 0
Various approaches exist for extractive summarization,  including the use of word frequency. 0 0
For each text there is a pair of reference summaries. 0 0
the sentence in the text that best expresses its main idea. 0 0
In Step 1,  the source text is preprocessed in a straightforward manner. 0 0
A simple network representation of texts was defined,  which requires only shallow text preprocessing. 0 0
the scores per text generated by PageRank Backward,  we could not include them all in the analysis. 0 0
uses a SOM(  SelfOrganizing Map)  neural network to classify text sentences as essential,  complementary or superfluous. 0 0
In (Fung P, Ngai G 2006):
The linearity of the text is hence preserved in our model. 0 0
Various similarity measures and metrics include the cosine measure,  Dice coef cient,  Jaccard coef cient,  inclusion coef cient,  Euclidean distance,  KL convergence,  information radius,  etc   Manning and Sch utze 1999; Dagan et al   1997; Salton and McGill 1983. 0 0
There is no regard for the ordering information between sentences or text cohesion. 0 0
The superior per for mance of our method shows that modeling text cohesion improves summariza ti on results. 0 0
The per for mance of our summarizer is superior to conventional methods that do not incorporate text cohesion information. 0 0.5
In (Aliguliyev 2009):
Similarity measures have been used in text related research and application such as text mining,  information retrieving,  text summarization,  and text clustering. 0 0.5
Various methods criteria have been proposed over the years from various perspectives and with various focuses(  Hammouda & Ka mel,  2004). 0 0.5
source text interpretation to obtain a source representation,  source representation transformation to summary representation,  and summary text generation from the summary representation. 0 0
Global optimization in the summarization of text documents. 0 0
In (0079):
Coherent text is characterized by various types of cohesive links that facilitate text comprehension(  Halliday and Has an,  1976). 0 0
We computed the probability of each of our articles according to a multinomial model,  where the proba bility of a text with n relation tokens and k relation types is. 0 0
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non discourse features(  Miltsakaki and Kukich,  2000). 0 0
In (Goldstein et-al. 2000):
These types of summaries include. 0 0
With text span texts pan deletion the system attempts to delete"  less important"  spans of text from the original document the text that remains is deemed a summary. 0 0
From these sets we are performing two types of experiments. 0 0
There are two types of situations in which multi document summarization would be useful. 1 0
In the previous sections we discussed the requirements and types of multi document summarization systems. 1 0
There are also much more complicated types of summary extracts which involve natural language processing and or understanding. 1 0
In (Giannakopoulos et-al. 2008):
Let us elaborate on these two types of character sequences. 0 0
Based on different types of windows,  we can use the following approaches. 0 0
These representation types factoids,  SCUs— represent elementary information units that map semantic information items to different surface appearances. 0 0
Three types of correlation will be brie y presented here since they are related to the task at hand. 0
We have a text T l. 0 0
There are various types of correlation measures,  called correlation coef cients,  depending on the context in which they can be applied. 0 0
In (Radev et-al. 2001):
• Di erent color schemes for di erent types of information Di erent colors were used for display of various types of information. 0 0
Recall that in weighted information retrieval,  di erent weights are assigned areas signed for di erent types of tags. 0 0
To illustrate the types of summaries produced byWebInEssence,  we enclose the rst four sentences of the summary for cluster 00018. 1 0
Ne to et al   describes a text mining tool that performs document clustering and text summarization They used the Auto class algorithm to perform document clustering and used TF*ISF(  an adaptation of TF*IDF)  to perform sentence ranking and generate the summarization output(  Ne to et al,  2000)  Our work is di erent from theirs in that we perform personalized summarization based on the retrieval result from a generic personalized web based search. 1 0
In (Hearst 1997):
Initial testing was done on the texts evaluated with several different sets of parameter settings and a default configuration that seems to cover many different text types was chosen. 0 0
Hinds(  1979,  137)  also suggests that different discourse types have different organizing principles. 1 0
The evaluation presented here shows the results for different setting types to give a feeling for the space of results. 0 0
This two level structure is chosen for reasons of computational feasibility and for the purposes of the application types described below. 0 0
In (Rush et-al. 2015):
6 overlapping word types between the headline and the input although only 2. 0 0
The headline vocabulary consists of 31 million tokens and K word types with the average title of length 8. 0 0
Our system does not require this alignment step but instead uses the text directly. 0 0
The complete input training vocabulary consists of 119 million word tokens and K unique word types with an average sentence size of 31. 0 0
Wubben et al  (  2012)  utilize MOSES directly as a method for text simplification. 0 0
By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation. 0 0
In (Kedzie et-al. 2015):
We consider two types of language model features. 0 0
Figure   2 lists the event types found in our data set. 0 0
The second model is estimated from text specific to our event types. 0 0
We developed novel features that capture the language typical of different event types and that identify sentences specific to the particular disaster based on location. 0.5 0
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (CR100):
Statistical differences according to a ttest are indicated with a star As it can be seen from the results obtained,  we can confirm that for generic summaries,  it is better to generate the new sentences from the first words of each original sentence. 0.5 0
In contrast,  abs tractive approaches require a more elaborate process,  involving sentence compression,  information fusion,  and or language generation. 0 0
Section III describes the word graph based method for compressing and merging sentences. 0 0
In this paper,  we analyzed a method based on word graphs for generating abs tractive summaries. 0 0
(  i e,  we produce generic single document summaries of 100 words each)  and we compare our abs tractive summaries to the existing model summaries In addition to the two approaches explained in Section IV. 0 0
S ENTENCESIn this section,  we explain the proposed algorithm base don based on word graphs for generating new sentences. 0 0
In particular,  graph based algorithms used for such purpose have been proven to be very successful for producing multi document summaries,. 0 0
In this section,  we explain previous work on recentabstractive summarization,  and we stress our novelty with respect to other similar approaches An approach for combining different fragments of information that have been extracted from one or more document sis documents is suggested in. 0 0
In these cases,  all the stages of the summarization process are taken into account Due to the difficulty associated to the generation of abstracts,  most approaches only focus on the first stage(  i e,  topic identification) ,  producing extracts as a result. 0 0
Our aim is to explore to what extent new sentences generated employing a word graph based method(  which either compress or merge information)  are suitable for producing abstracts. 0 0
We first propose a method for compressing and merging information based on word graphs,  and then we generate summaries from the resulting sentences This allows us to quantify how feasible it is to produce abstracts directly. 0 1
suggests a method based on word graphs,  where the shortest path is computed to obtaina very short summary(  only one sentence)  from a set of related sentences belonging to different documents Liu and Liu. 0 0
However,  preliminary experiments carried out prove that the combination of extractive and abs tractive information is a more suitable strategy to adopt towards the generation of abstracts.KeywordsHuman Language Technologies,  automated retrieval and mining,  automated content summarization,  abs tractive techniques,  graph based algorithms. 0 0.5
On the Ont he one hand,  we try to elucidate the reasons why the Graph COMPENDIUM approach performs worse than theCOMPENDIUM Graph,  and on the other hand,  we want to analyze the reasons of the low overall performance of theabstractive approaches Regarding the first type of analysis carried out,  if we use the word graph based method for generating new sentences first,  and use all of them as input for COMPENDIUM,  thisTS tool can have difficulties in selecting important content This occurs because many of the sentences will start with the same words(  e g,  if we take the top 10 words with highest tfidf) ,  so once COMPENDIUM detects a specific fragment of information as relevant,  sentences containing the same portion of information that have not been detected as redundant will be also selected,  leading to summaries that have not much variation in content. 0 0
3.2-tTopic based approaches
In (Bairi et-al. 2015):
Topic Specificity,  Topic Clarity,  and Topic Relevance. 0 0
Topic Coherence. 0 0
Topic Relevance. 0 0
Topic Specificity.0 0
Topic Clarity. 0 0
We show from our experiments that this approach performs better than all other approaches and baselines. 0
Penalty based diversity. 0 0
Feature based Functions. 0 0
Topic clarity is the fraction of descendant topics that cover one or more documents. 0 0
Coverage based functions have a time complexity of O(  n)  whereas similarity based functions are O n 2 . 0 0
When run on a large collection of documents,  these approaches generate enormous numbers of topics,  a problem our proposed approach addresses. 0 0
We show that the sub modular mixture learning and maximization approaches,  i e,  SMMLcov and SMMLcov+sim outperform other approaches in various metrics. 0 0
To alleviate this problem,  we adopt cluster based evaluation metrics. 0 0
However,  we exclude the similarity based functions described in section 3.1.2. 0 0
3.3-tGraph based approaches
In (Moawad IF, Aref M 2012):
There are many rules can be derived based on many factors. 0 0
The Rich Semantic Graph Reduction Phase. 0 0
The Rich Semantic Graph Creation Phase. 0 0
the Rich Semantic Graph Creation Phase,  the Rich Semantic Graph Reduction Phase,  and the Summarized Text Generation Phase. 0 0
3)  The Rich Semantic Graph Generation module. 0 0
The ranking method is based on deriving the average weight of each concept(  word sense)  and the average weight of the whole sentence concepts based on(  1)  and(  2)  respectively. 0 0
provided a semantic graph based approach to generate an extractive summary for a single input document. 0 0
It can be successfully combined with a number of intelligent systems based on Human Language Technologies(  e g. 0 0
Keywords Text Summarization Abs tractive Summary Semantic Representation Rich Semantic Graph Semantic Graph. 0.5 0
3.4-tDiscourse based approaches
In (Moawad IF, Aref M 2012):
x Discourse Structuring Process. 0 0
The exponential growth in data increases the need for intelligent filtering and knowledge based approaches to reduce the time needed to absorb the key facts in documents and to avoid drowning in it. 0 0
Lexicalization,  Discourse Structuring,  Aggregation,  and Referring Expression processes. 0.5 0
The module uses the PDTB(  Pann Discourse Tree Bank Model)  relations. 0 0
In (Ferreira et-al. 2014):
Discourse relations. 0 0
Discourse relations(  Wolf & Gibson,  2005). 0 0
The aforementioned approaches present valid contributions to the field and display good performance,  in general. 0 0
Massachusetts based Raytheon company. 0 0
In (0079):
P(  n)  is the probability of an article having length n,  x i is the number of times relation i appeared,  and p i is the probability of relation i based on the Penn Discourse Tree bank. 0.5 0
P(  n)  is the maximum like li hood estimation of an article having n discourse re lat ions based on the entire Penn Discourse Tree bank(  the number of articles with exactly n discourse re lat ions,  divided by the total number of articles)  The log likelihood of an article based on its dis course relations(  F 13)  feature is defined as. 0.5 0
Discourse relations are believed to be a major factor in text coherence. 0.5 0
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005):
We conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 1 0
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 1 0
Hence,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 1 0
Thus,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 1 0
one used relevance measure to rank sentence relevance,  and the other used latent semantic analysis to identify semantically important sentences. 0.5 0
To take into account the context of a link,  we integrate the map and the above mentioned semantic sentence representation to promote text summarization from keyword level analysis to semantic level analysis. 1 0
The second uses latent semantic analysis(  LSA)  to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. 1 0.5
The second uses latent semantic analysis(  LSA)  to derive the semantic matrix of a document(  in single document level)  or a corpus(  in corpus level) ,  and uses semantic sentence representation to construct a semantic text relationship map. 1 0
outperforms keyword based text summarization approaches. 0.5 0
The analysis phase analyzes the input text and selects a few salient features. 0.5 0
Text relationship map construction creates the text relationship map based on semantic sentence representation  derived from the semantic matrix. 1 0
4.2-tInformation extraction using sentence based abstraction technique
In (Chan 2006):
In this article,  a sentence based abstraction technique for information extraction is presented. 1 1
Information extraction Automatic summary Shallow text processingConnectionist model. 0 0
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 1 0
We have described a discourse network of sentence abstraction based on textual continuity that arises from a connectionist model. 0 0
Simulation experiments suggest that this technique is useful because it moves away from a purely keyword based method of textual information extraction and its associated limitations. 1 0.5
At the syntactic stage,  information extraction involves tokenization,  partsofspeech tagging,  syntactic parsing,  phrasal pattern matching,  and the transformation of each sentence into an intermediate structure using various templates. 1 0.5
The model is based on a shallow linguistic extraction technique. 1 1
The underlying abstraction technique is powerful in capturing and representing textual features in an effective and efficient way. 1 0
The main idea behind capturing the nonlinearity of the discourse structure during the sentence based abstraction can be interpreted as the iterative change of the activation level of the discourse segments based on the textual continuity,  which is encoded in the connection weights. 1 0
Our abstraction technique can be used to detect the boundaries between groups of consecutive sentences that are highly relevant to each other. 1 0
The experimental results for our prototype show that the discourse network and its abstraction technique are able to correlate semantically relevant sentences. 1 1
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007):
Text summarization Document concept lattice Concept Semantic. 1 0
Answers appear frequently in the document set might be considered to be more crucial in understanding the source texts. 1 0
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 1 1
Text summarization is the process of distilling the most important information from sources to produce an abridged version for a particular users and tasks(  Mani & May bury,  1999). 1 0
As a document concept model,  DCL has the following properties. 0 0
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests. 1 0.5
where N is the number of documents in the corpus,  is the frequency of concept Ci in the document cluster,  and is the number of documents containing concept Ci. 0 0
• Motivated by our evaluation metric on answer loss,  we propose a novel document model,  the document concept lattice,  which indexes sentences with respect to their coverage of overlapping concepts. 1 0
As such,  introduce a new data structure,  the document concept lattice,  that compactly represents such hanging structures. 1 0.5
This is because texts often exhibit such repetition on purpose,  in order to aid a reader s understanding of the text. 0 0
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Rush et-al. 2015):
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 0.5 1
To test the effectiveness of this approach we run extensive comparisons with multiple abs tractive and extractive baselines,  including traditional syntax based systems,  integer linear program constrained systems,  information retrieval style approaches,  as well as statistical phrase based machine translation. 0.5 0
A similar issue in machine translation inspired Bahdanau et al  (  2014)  to instead utilize an attention based contextual encoder that constructs a representation based on the generation context. 1 0
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. 0 0
Finally,  we use a phrase based statistical machine translation system trained on Gigaword to produce summaries,  MOSES+(  Koehn et al,  2007). 1 0.5
We experiment with our attention based sentence summarization model on the task of headline generation. 0 1
We follow the statistical machine translation setup and use minimum error rate training(  MERT)  to tune for the summarization metric on tuning data(  Och,  2003). 0 0
ABS often uses more interesting rewording,  for instance new nz pm after election in Sentence 4,  but this can also lead to attachment mistakes such a russian oil giant chevron in Sentence 11. 0 0
We first note that the baselines COMPRESS and IR do relatively poorly on both data sets,  indicating that neither just having article information or language model information alone is sufficient for the task. 0 0
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008):
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 1 1
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 1 0.5
Table 3 shows the result of using subjective opinions described in Section 5. 0 0.5
Moreover,  we study how to include subjective opinions to help identify important sentences for summarization. 1 0.5
We integrate our best cohesion measure together with the subjective opinions. 1 1
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones. 1 0.5
Subjective opinions are often critical in many conversations. 1 1
Other than the conversation structure,  the measures of cohesion and the graph based summarization methods we have proposed,  the importance of a sentence in emails can be captured from other aspects. 1 0
Summarizing email conversations is challenging due to the characteristics of emails,  especially the conversational nature. 1 0
We adopt three cohesion measures. 1 0
In (Zajic et-al. 2008):
Summarization technology might be especially attractive for display of email on mobile devices with limited screen area. 1 1
These two alternatives make opposite hypotheses about the conversational structure of email threads. 1 0
(  Probably a mixture of both)  How should the conversational nature of email threads be conveyed. 0 0
However,  the IMS approach is less sophisticated in controlling summary length – threads with few emails might result in a summary shorter than the desired length and threads with many emails might result in a summary that is arbitrarily truncated. 1 0
In (Carenini et-al. 2007):
As we all know,  summarization is a subjective activity. 0 1
Since we are studying multiple email summarization in a conversational context,  we required that each conversation contained at least 4 emails. 1 0.5
hidden emails and significance of clue words. 0 0
Recall that a folder may contain emails involved in multiple distinct conversations. 0 0
Many email programs provide features to group emails into threads using headers. 0 0
The selected emails also need to represent different types of conversation structure. 0 0
4.6-tSummarization of text through complex network approach
In (Kulkarni and Prasad 2010):
The configurations of the network used for our approach is shown in the Figure 5. 0 0
Authors describe here,  the connectionist model used in the proposed approach for automatic text summarization. 0 0
Summarization algorithm module. 0 0
The occurrence summarization system based on statistical approach count of all the individual words in the sentence is using fuzzy logic over some significant text features. 0 0
The computed feature score is applied to the trained network that returns the final score of every sentence presented in the input text document. 0 0
The performance of the proposed approach is evaluated using precision,  recall and Fmeasure. 0 0.5
In the testing phase,  the features extracted from the input text document are given to the trained network that provides score for every sentence in the input document. 0 0
This study describes three approaches to Automated Text Summarization using connectionist statures based on. 0 0
Then,  the chromosomes with their fuzzy score are fed to the neural network for training. 0 0
This study intends to investigate Connectionist architecture for the Text Summarization system,  taking into account of existing new developments in adaptive evolving systems. 0 0
In this study,  we consider the system of an Automatic Text Summarization as Evolving system which learns incrementally through experience in the environment. 0 0
The neural network used in the proposed system is configured with a nine input,  hidden and one output layer. 0 0
The proposed automated text summarization system consists of five components. 0 0
The significant text features considered in the proposed system are. 0 0
The sentence score summary provided in the data set using the evaluation obtained from the neural network for the input document measures such as,  precision,  recall and Fmeasure. 0 0
A neural network MLP couples,  through functions and weights,  certain variables(  called inputs)  with certain other variables(  called outputs) (  Lahoz and Miguel,  2006). 0 0
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Ferreira et-al. 2014):
the creation of summaries with 200 and 400 words. 0 0
It combines single document summaries using sentence clustering techniques to generate multi document summaries. 0
Edge creation select. 0 0
As already mentioned,  multi document summarization can be classified into generic and query based summarization. 0 0
The 2002 conference was the last one that proposed the contest to create generic multi document summaries. 0 0
The edge creation services selected to the application. 0 0
Automatic methods are needed to process the Internet data efficiently,  scavenging useful information from it. 0 0
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 0 1
Alguliev and his collaborators(  Alguliev,  Aliguliyev,  & Mehdiyev,  2013)  propose a generic document summarization method which is based on sentence clustering. 0 1
In the context of generic summarization,  some systems must be highlighted. 0 0
The main contribution of the proposed algorithm is the creation of an unsupervised generic 0summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 0 0.5
Other works in generic summarization apply clustering methods to achieve larger information diversity,  eliminating redundancy. 0 0.5
The same techniques used in single document summarization systems apply to multi document ones in multi document summarization some issues as the degree of redundancy and information diversity increase,  however. 0 0.5
Multi document summarization Extractive summarization Sentence clustering. 0 0
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009):
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 1 0
Automatic summarization Genetic algorithm Mathematical regression Feed forward neural network Probabilistic neural network Gaussian mixture model. 1 0
After that we used all models for the summarization task using the maximum likelihood of each category as follows. 0 0
The PNN approach performance evaluation based on precision(  Arabic case). 0 0
The PNN approach performance evaluation based on precision(  English case). 0 0
The GMM approach performance evaluation based on precision(  Arabic case). 0 0
The GMM approach performance evaluation based on precision(  English case). 0 0
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 1 0
Simpler approaches were then explored that consist of extracting representative text spans texts pans,  using statistical techniques and or techniques based on surface domain independent linguistic analyses. 0 0.5
All models performance evaluation based on precision(  English testing data). 0 0
All models performance evaluation based on precision(  Arabic testing data). 0 0
Recently many experiments have been conducted for the text summarization task. 0 0 
The process of text summarization can be decomposed into three phases. 0 0
The structure of PNN implementation. 0 0
All models performance evaluation based on precision(  DUC 2001 testing data). 0 0
All models performance evaluation based on the average Rouge score(  DUC 2001 testing data). 0 0
It is clear from the figures that GMM approach gives the best results since GMM has a good capability to model arbitrary densities. 1 1
Furthermore,  we use trained models by one language to test summarization performance in the other language. 0 1
In this section,  we investigate the effect of each feature parameter on summarization by using Eq. 0 0
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Cao et-al. 2015a):
Graph based models play a leading role in the summarization area. 0 1
As for multiple references,  we choose the maximal value. 0 0
The documents are all from the news domain and are grouped into various thematic clusters. 0 0
Briefly,  RNN processes structured inputs(  usually a binary tree)  by repeatedly applying the same neural network at each node. 0 0
In this section,  we present the results of our models and compare them with other systems,  mainly measured by ROUGE2. 0 0
First,  we are interested in applying R2N2 for query focused summarization,  which can be achieved by introducing query related features to the input of the neural networks. 0 0
Extractive summarization(  Over and Yen 2004)  aims to generate a short text summary for a document or a set of documents through selecting salient sentences in the document(  s). 0 0
Experiments on the DUC 2001,  2002 and 2004 multi document summarization data sets show that R2N2 outperforms stateoftheart extractive summarization approaches. 0 0
However,  our model deals with word regression and sentence regression simultaneously and consistently. 1 0
In (Fattah and Ren 2009): 
Automatic summarization Genetic algorithm Mathematical regression Feed forward neural network Probabilistic neural network Gaussian mixture model. 1 0
A suitable combination of feature weights is found by applying GA. 1 0
In matrix notation,  we can represent regression as follow. 0.5 0
Furthermore,  we use trained models by one language to test summarization performance in the other language.  0 0.5
After that we used all models for the summarization task using the maximum likelihood of each category as follows.  0 0
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011):
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 0.5 0
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 1 0.5
The diversity is very important evidence serving to control the redundancy in the summarized text and produce more appropriate summary. 1 0
where vmin,  vmax are the allowable minimum and maximum velocity values,  pm in,  pm ax are the allowable minimum position and maximum position of particles. 0 0
redundancy in a summary will not be minimized. 0 0
► We model unsupervised generic text summarization as an optimization problem. 1 0
The proposed generic text summarization model is presented in Section 3. 0 0
That is why document clustering enables us to group similar text information and then text summarization provides condensed text information for the similar text by extracting the most important text content from a similar document set or a document cluster. 0 0
Many approaches have been proposed for text summarization based on the diversity. 0.5 0
In particular,  we model text summarization as an integer linear programming problem. 1 1 
redundancy in the summary will be reduced(  this provides the third term). 0.5 0
In Takamura and Okumura(  2009) ,  text summarization formalized as a budgeted median problem. 0.5 0
In particular,  we model text summarization as an integer linear programming(  ILP)  problem. 1 0.5
Filatova and Hatzivassiloglou(  2004)  modeled extractive document summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. 1 0
We represent generic text summarization model as an optimization problem and attempt to globally solve the problem. 1 0.5
the swarm size,  Nsw = 30; the number of iteration,  tmax = 500; the allowable minimum position and maximum position of particles,  pm ax = 50; pm in = −50; the allowable minimum and maximum velocity values,  vmax = 5; vmin = −5. 0 0
McDonald(  2007)  formalized text summarization as a knapsack problem and obtained the global solution and its approximate solutions. 0.5 0
In these tables through MCMR(  Maximum Coverage and Minimum Redundant)  denoted our model with the objective function f. 0 0
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Ganesan et-al. 2010):
Figure 2 shows ROUGE scores of these scoring methods at varying levels of gap. 0 0
This is because extractive methods that just select sentences tend to be much longer resulting in higher recall. 0
Due to the subtle variations of redundant opinions,  typical extractive methods are often inadequate for summarizing such opinions. 0 0.5
In (Ferreira et-al. 2013):
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization. 1 1
Extractive summarization Sentence scoring methods Summarization evaluation. 0 0
• In general,  sentence scoring methods are faster. 0 0.5
In (0035):
TIPSTER Text Summarization Evaluation Conference(  SUMMAC). 0 0
146)  s WS is a text summarizer which is part of Microsoft Word,  and it has been used for comparison with other summarization methods by several authors. 0 0
In text summarization we can employ the same idea. 0 0
In (Patel et-al. 2007):
The need for text summarization methods that can handle multiple languages appear to be growing. 0 1
Various methods of scoring the relevance of sentences or passages and combining the scores are described in. 1 0.5
Two approaches are generally followed for automatic text summarization research. 0 0
In (Riedhammer et-al. 2010):
Speech summarization originated from the porting of methods developed for text summarization. 0 1
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain. 0 1
sentence based scoring of relevance and redundancy,  and sub sentence based scoring with implicit redundancy. 0 0
In (Gupta and Lehal 2010):
This paper focuses on extractive text summarization methods. 0.5 1
These features are important as,  a number of methods of text summarization are using them. 0
Query based extractive text summarization. 0 0
In (Kulkarni and Prasad 2010):
Evaluation measure. 0 0
The purpose was to implement and evaluate existing connectionist methods and adopt the best suited for the domain of text summarization process. 0 0
The proposed automated text summarization system consists of five components. 0 0
In (Parveen and Strube 2015):
We focus on extractive summarization of scienti c articles Extractive summarization involves computing the importance of sentences which is used for deciding whether to includea sentence in the summary. 1 1
Evaluation metrics are ROUGESU4 and ROUGE2 Lin,  2004. 0 0
has proposed an optimization method based on integer linear programming(  ILP)  which considers text summarization as a knapsack problem Similarly,. 0 0
In (Aliguliyev 2009):
Generally speaking,  the methods can be either extractive summarization or abs tractive summarization. 0 0
In the past,  extractive summarizers have been mostly based on scoring sentences in the source document. 1 1
Sentence based extractive summarization techniques are commonly used in automatic summarization to produce extractive summaries. 1  0
In (Carenini et-al. 2008):
Evaluation of summarization is believed to be a difficult problem in general. 0 0
The extractive summarization problem can be viewed as a node ranking problem. 0.5 0
Note that when each distinct sentence in an email conversation is represented as one node in the sentence quotation graph,  the extractive email summarization problem is transformed into a standard node ranking problem within the sentence quotation graph. 0.5 0
In (Neto et-al. 2000):
“ Text summarization is still an emerging field,  and serious questions remain concerning the appropriate methods and types of evaluation. 0.5 0
theTIPSTER Text Summarization Evaluation Conference(  SUMMAC)  – hereafter referred to as the SUMMAC project for short Mani et al   98. 0 0
In practice,  the automatic construction of phrases isa quite difficult task and there is no guarantee that the new phrases will be really meaningful for the user Hence,  in the current version of our system we have opted to develop a safer extractive summarization algorithm. 0 0
In (Alguliev et-al. 2013):
There are several most widely used extractive summarization methods as follows. 0
The LexRank defines sentence salience based on graph based centrality scoring of sentences. 0.5
The work(  Sarkar,  2010)  presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system. 0.5 0
In (Azmi AM, Al-Thanyyan S 2012):
A comprehensive evaluation in Uz da et al  (  2008)  has concluded that automatic text summarization methods which are based on RST are better than extractive summarizers and those with hybrid methods produce worse summaries. 0.5 0.5
This automatic Arabic summarization system integrates an RSTbased system and a sentence scoring scheme. 0 1
The Optimized Dual Classification System(  Sobh et al,  2006)  is an Arabic extractive text summarization system. 0 0.5
In (Harabagiu S, Lacatusu F 2005):
ComponentBased Evaluation 1; sentence extraction. 0 0
ComponentBased Evaluation 3; sentence ordering. 0 0
We have evaluated each of these summarization methods using a number of standard techniques,  including the ROUGE automatic scoring packages and the manual Pyramid evaluation method. 0.5 0
In (Fang et-al. 2015):
Several stateoftheart methods including supervised and unsupervised are compared with the proposed TAOS for text and image summarization tasks respectively. 0 0
for extractive multi document summarization. 0 0
For text summarization task,  since the summaries are consist of individual sentences,  we totally extract three kinds of features of each sentence from documents. 0 0
In (Rush et-al. 2015):
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. 0 1
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 0 1
We begin by defining the sentence summarization task. 0 0
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013):
The correlations among multiple terms are extracted using an established data mining technique,  i e,  association rule mining. 1 0
To represent the most significant correlations among multiple terms a graph based model,  named correlation graph,  is generated. 1 0
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 1 1
• using association rules in graph based summarization to represent correlations among multiple terms,. 1 0
Hence,  there is a need for novel graph based summarizers that also consider the correlations among multiple terms and the differences between positive and negative term correlations. 1 1
Unlike all of the above mentioned approaches,  our summarizer discovers association rules from the analyzed document to also represent the correlations among multiple terms in the graph based model. 1 1
to discover relevant correlations among data. 0 0
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 1 1
,  GraphSum is a graph based approach that discovers and exploits association rules to also consider the high order correlations among multiple terms. 1 1
In (Baralis et-al. 2012):
However,  previous approaches typically focus on single word significance while do not effectively capture correlations among multiple words at the same time. 0.5 0.5
proposed to represent correlations among sentences by means of a graph based model. 1 0.5
Frequent item set items et mining is a well established data mining technique to discover correlations among data. 0 0
From a transactional representation of the document collection,  an highly informative and not redundant itemsetbased model is extracted to represent significant higher order correlations among document terms. 0 0
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Kulkarni and Prasad 2010):
Summarization algorithm module. 0 0
Evolutionary Programming(  EP)  model. 0 0
A separate algorithm is developed to determine which modifiers apply to a nucleus. 0.5 0
Precision recall schemes,  as well as summary accuracy measures which incorporate weightings based on multiple human decisions,  are suggested as particularly suitable in evaluating generic summaries. 1 0
Preprocessing Feature extraction Fuzzy model Evolutionary Programming(  EP)  model • Connectionist model • Sentence selection and assembly. 0 0
Two random integers are generated within documents on different categories and extractive the size of the population. 0 0
word net,  an online dictionary based on Semantic Nets(  SN)  • Fuzzy logic • Evolutionary connectionist and fuzzy techniques. 0 0
No of cue words in the sentence Total no   of cue phrases in the documents Term weight. 0 0
Then,  we generate the single document summary for these four documents using obtained for the input document(  Document No   the proposed system. 0 0
Training phase The back propagation algorithm can be utilized successfully to train neural networks it is extensively accepted for applications to layered feed forward networks,  or multilayer perceptrons(  Aliruliyev,  2009). 1 0
In many of the documents the importance of the sentences or headings is indicated by expressing the text in different text format e g,  Italics,  Bold,  underlined,  big font size and more. 0 0
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Almeida and Martins 2013):
Improving summarization performance by sentence compressiona pilot study. 0.5 0
Improving joint parsing and named entity recognition with non jointly labeled data. 0 0
For the multitask experiments,  we also used the data set of BergKirkpatrick et al  (  2011) ,  but we augmented the training data with extractive summarization and sentence compression data sets,  to help train the compressive summarizer. 0 0
Statistics based summarization step one. 0 0
Centroid based summarization of multiple documents. 0 0
Multiple aspect summarization using integer linear programming. 0 0
The icsi summarization system at tac 2008. 0 0
• For the compressive summarization task,  the. 0 0
Large margin learning of sub modular summarization models. 0 0
Multi document summarization by maximizing informative content words. 0 0
• For the extractive summarization task,  there are. 0 0
Coverage based extractive summarization can be formalized as follows. 0 0
Multi document summarization via budgeted maximization of sub modular functions. 0 0
Sentence compression as a component of a multi document summarization system. 0 0
Prior work in compressive summarization has followed one of two strategies. 0 0
In addition,  we propose a multitask learning framework to take advantage of existing data for extractive summarization and sentence compression. 0 0
For extractive summarization,  we used the DUC 2003 and 2004 data sets(  a total of 80 multi document summarization problems). 0 0.5
In (Leite and Rino 2006):
Improving Features Representation. 0 0
Improving Features Representation. 0 0
Improving Features Representation. 0 0
Two Main Approaches for Automatic Summarization. 0 0
3364. 0 0
Computational Linguistics,  23(  1) ,  pp. 0 0
Segmenting Text into MultiParagraph Subtopic Passages,. 0 0
TextTiling. 0 0
(  1997). 0 0
Hearst,  M. 0 0
Morgan Kaufmann Publishers. 0 0
359366,  San Francisco,  CA. 0 0
In Proceedings of the International Conference on Machine Learning,  pp. 0 0
Correlation based feature selection of discrete and numeric class machine learning. 0 0
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010):
Text summarization based on fuzzy logic system architecture. 0 0
Statistical selection of signature words. 0 0
BAYESUM leverages the common case in which multiple documents are relevant to a single query. 0 0
Automatic text summarization based on fuzzy logic. 0 0
is a multi document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order. 0 0
The importance of sentences is decided based on statistical and linguistic features of sentences. 0 0
The importance of sentences is decided based on statistical and linguistic features of sentences. 0 0
The importance of sentences is decided based on statistical and linguistic features of sentences. 0 0
Multi document extractive summarization deals with extraction of summarized information from multiple texts written about the same topic. 0 0
In (Ko et-al. 2003):
DOCUSUM is our summarization system based on new topic keyword identification method. 1 1
DOCUSUM is a text summarization system based on IR techniques using semantic and statistical methods. 1 1
However,  the summarization of documents with multiple topics is also handled importantly,  because a document with a large topic can consist of a number of smaller topics. 1 0
A Query based summarization makes a summary by extracting relevant sentences from a document. 1 0
Statistical features of this test set are as the following Table 1. 0 0
DOCUSUM identifies topic keywords without other linguistic resources such as the WordNet. 1 0
In (Ferreira et-al. 2014):
Statistical similarity. 0 0
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 1 1
The proposed system uses a graph model based on statistic similarities and linguistic treatment to represent the collection of input documents(  differently from Canhasi and Kononenko,  2014,  Chen et al,  2014). 1 1
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015):
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 1 0
Many extraction based summarization methods have been proposed in the past years. 0 0
Thus,  we totally define 13 feature groups for the text summarization task. 0.5 0
abstract based and extraction based. 0 0
Experiment results on DUC2003 and DUC2004 data sets for text summarization and NUSWide data set for image summarization demonstrate that the introduction of group selection can indeed improve summary performance. 1 0
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 0 0
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 0 0
In our TAOS,  is divided into G groups and the overlapping between groups is admissible,  therefore. 0.5 1
BagofVisualWord is a image representation based on BoW(  BagofWord)  model. 0 0
With the construction of feature groups,  a latent variable is utilized to control the selection of them. 1 0.5
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 1 0.5
For different topics set,  different feature groups are selected. 0.5 0
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 0 0
Here,  we can define the groups of feature according to the user defined priors. 1 0
The motivation of TAOS is to identify which feature groups are most relevant for different topics. 1 0
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 0 0.5
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 1 0
Folksonomy systems are classification systems derived from social tags assigned to multimedia content by everyday users. 1 0
Folksonomies became popular on Web 2.0 as part of social tagging applications,  such as social bookmarking and image annotations uploaded by users. 1 1
Finally,  our system generated a final summary of the given documents based on the top n scored sentences in the Score Table. 1 0
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 1 0.5
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 1 0
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 1 0
Given multiple documents that need to be summarized, we first perform a preprocessing step so that the documents can be analyzed at different granularities(  i e,  word level and sentence level). 0 1
To overcome these drawbacks,  we propose a novel multi document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. 1 0.5
In this paper,  we propose FoDoSu which is a novel multi document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. 1 0
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Shen et-al. 2007):
in this paper,  which is a stateoftheart sequence labeling method. 0 0
,  the authors observed that hidden topics can be discovered in a document as well as the projection of each sentence on each topic through Latent Semantic Analysis Deer wester et al,  1990. 0 0
The methods based on the last two features require less extra resources and efforts while still achieve better performances compared to other methods,  as shown in Gong and Liu,  2001. 0 0
In this paper,  we use CRF as a tool to model this sequence labeling problem. 1 0
The key idea of our approach is to treat the summarization task as a sequence labeling problem. 1 1
On the other side,  HMM,  as a generative model,  spends a lot of resources on modeling the generative models which are not particularly relevant to the task of inferring the class labels. 1 0
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015):
In fact,  our model incorporate a two level sparse representation model. 1 0.5
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 1 1
Then atwolevel sparse representation model is devised to extract all the salient sentences. 1 1
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 1 0
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 1 1
The summary set is a sparse representation of the original document set Level. 1 0
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 0 1
Finally,  our system generated a final summary of the given documents based on the top n scored sentences in the Score Table. 0 0
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 0 0.5
To overcome these drawbacks,  we propose a novel multi document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. 0 1
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a):
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 1 0
In this parsing tree,  recursive neural networks measure the importance of all these non terminal nodes. 1 0
It transforms the ranking task into a hierarchical regression process which is modeled by recursive neural networks. 1 0.5
This process is modeled by recursive neural networks(  RNN). 1 0
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 1 0.5
As for multiple references,  we choose the maximal value. 0.5 0
It uses a greedy approach to select sentences and considers the trade off between relevance and redundancy. 1 0
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015):
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 1 1
importance,  coherence value and redundancy Variables. 1 0
Hence,  we optimize importance,  non redundancy and coherence simultaneously. 1 0
apply their local coherence model,  the entity grid,  to summary coherence evaluation However,  to our knowledge,  the entity grid has not been used directly in extractive summarization to ensure summary coherence. 1 0.5
We focus on extractive summarization of scienti c articles Extractive summarization involves computing the importance of sentences which is used for deciding whether to includea sentence in the summary. 1 0.5
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015):
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 0 0
Our problem is different from traditional summarization tasks since we have an underlying DAG as a topic hierarchy that we wish to summarize in response to a subset of documents. 0 1
Figure 1 describes the topic summarization process for creation of the disambiguation page for Apple. 0 0
In this paper,  we investigate structured prediction methods for learning weighted mixtures of sub modular functions to summarize topics for a collection of objects using DAGstructured topic hierarchies. 1 0.5
The facility location function,  defined as f(  S)  = P i V maxj S sij,  is a natural model for kmedoids and exemplar based clustering,  and has been used in several summarization problems(  Tschiatschek et al,  2014; Wei et al,  a). 0 0
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015):
However,  we need to reconcile these sentences with updates from the previous hour to ensure that the most salient and least redundant updates are selected. 0 0
Automatic summarization could deliver relevant and salient information at regular intervals,  even when human volunteers are unable to. 0 0
The AP+SALIENCE model is better able to find salient updates earlier on for the disaster domain,  this is an especially important quality of the model. 1 0.5
A principal concern in extractive multi document summarization is the selection of salient sentences for inclusion in summary output(  Nenkova and McKeown,  2012). 0 0
4.23.11-tSummarizing multiple documents through system combination
In (Carenini et-al. 2008):
Those methods built a complete graph for all sentences in one or multiple documents and measure the similarity between every pair of sentences. 0 0
Summarizing email conversations is challenging due to the characteristics of emails,  especially the conversational nature. 0 0.5
This is different from other documents such as newspaper articles and formal reports. 0 1
In this paper,  we use two metrics to measure the accuracy of a system generated summary. 0 1
In this way,  the ROUGE metric measures the similarity of a system generated summary to a gold standard summary that is considered important by most human summarizers. 0 0.5
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015):
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 1 1
Our evaluation reveals for the first time which metric variants significantly outperform others,  optimal metric variants distinct from current recommended best variants,  as well as machine translation metric BLEU to have performance on par with ROUGE for the purpose of evaluation of summarization systems. 1 1
Automatic metrics of summarization evaluation have their origins in machine translation(  MT) ,  with ROUGE(  Lin and Hovy,  2003) ,  the first and still most widely used automatic summarization metric,  comprising an adaption of the BLEU score(  Papineni et al,  2002). 1 1
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (Yeh et-al. 2005):
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 0 0
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 0 0.5
This paper proposes two approaches to address text summarization. 0.5 1
outperforms keyword based text summarization approaches. 0 0
Both approaches concentrate on single document summarization and generate indicative,  2 extract based summaries. 0 0
In this paper,  we propose two text summarization approaches. 0 0
The format of summaries is another criterion to differentiate text summarization approaches. 0 0
Text search and text summarization are two essential technologies to address this problem. 0 0
In this experiment,  the feasibility of applying LSA to text summarization is evaluated. 0 0
In recent years,  a variety of text summarization methods has been proposed and evaluated. 0 0
The effect of LSA in text summarization is illustrated with an example shown in Table 17. 0 0
We conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 0 1
Knowledge poor approaches do not consider any knowledge pertaining to the domain to which text summarization is applied therefore,  knowledge poor approaches can be easily applied to any domain(  Abracos & Lopes,  1997; Gong & Liu,  2001; Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; May eng & Jang,  1999; Salton et al,  1997; Mitra,  Singh al,  & Buckley,  1997). 0 0
Hence,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 0 0
Thus,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 0 0
In (Kallimani et-al. 2011):
In this paper we discussed the different statistical approaches for abs tractive summarization in Telugu language. 1 1
Extractive and abs tractive summarization Stemming Information retrieval Automatic text summarization Word count frequency. 1 0
Automatic text summarization has been in existence since 1950. 0 0.5
We modeled the problem of text summarization as an IR problem. 1 1
Most of these summarization approaches aim for selecting the most informative sentences,  while less attempt has been made to generate abs tractive summaries or compress the extracted sentences and merge them into a concise summary. 1 0
The merits of automatic summarization of text are to control the size,  and prediction of content,  which could identify the relation between summarized text and the main text. 1 1
So,  different approaches can be used for the development of efficient tagger. 0 0
Further,  machine learning based approaches gives somewhat better results as compared to other approaches. 1 0
Different machine learning approaches to be addressed to get more appropriate summaries for the given document. 0.5 0.5
The previous work in automatic summarization has completely focused on extractive summarization,  in which key sentences are identified from the source text and extracted to form the output. 0.5 0
The many other uses of summarization are.  0 0
An alternative solution is abs tractive summarization in which the information from the source text is first extracted into the form of abstract data which is then post processed to infer the most important message from the original text. 1 0.5
It uses linguistic methods to examine and interpret the text and then to find the new concepts and expressions to best describe it by generating a new shorter text that conveys the most important information from the original text document. 1 0
In (Gupta et-al. 2011):
Document summarization methods have been in notice for a long time and new approaches are coming up very sporadically. 0 1
Automatic summarization one such reductive technique allowing the Computer to summa rise the longer text to shorter non redundant form. 0 1
Thus,  there is a need for automatic text summarization for the languages in order to subdue this constantly increasing amount of electronically produced text. 0 1
Shallow linguistic features used in surface level approaches can be classified as follows. 0 1
The text will be retrieved for the common search engine and then the text will be analyzed to find out if they are really relevant according to user s interested profile in order to provide concise summary of the retrieved subset of the relevant documents using the multi document summarization facilities. 0 0
The following phases of the summarization process can be identified. 0 0
Surface level approaches represent information in terms of shallow linguistic features that are then selectively combined together to yield a salience function used to extract information. 0 0
Topic focused summarization tasks are related with a specific information need expressed by the user. 0 1
Sentence Scoring Each sentence is assigned a numerical score depending on the summarization method being used. 0 0
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 0 0
In (Zajic et-al. 2008):
Section 3 describes our general framework for text summarization and specific approaches we have developed for email thread summarization. 0 0.5
We attempted to present our summarization systems with text as clean as possible. 0 1
We describe two separate approaches to email thread summarization that adapt existing techniques. 0 1
We have developed two different approaches to the problem of email thread summarization that leverage existing work. 0 0
We present two approaches to email thread summarization. 0 0
The recent work of Carenini et al  (  2007)  examines extractive approaches to summarization on Enron data that leverage graphs defined by quoted texts. 0 1
That is,  text from earlier messages is dropped. 0 0
That is,  text from later messages is dropped. 0 0
Prima facie,  both approaches have advantages and disadvantages. 0 0
Finally,  evaluation issues in general present challenges for text summarization. 0 0
Both approaches are implemented in our general framework driven by sentence compression. 0 0
In published work,  we have examined two approaches to sentence compression. 0
Repetitions of text from earlier messages(  quoted text)  were also eliminated.0 
Both approaches involve selecting important sentences from email messages and compressing them(  i e,  removing unimportant fragments). 0 0.5
On the other hand,  we expect that the purely statistical HMMbased approach will be more robust to text from different genres. 0 0
In this work,  we do not examine the filtering process in detail instead,  only very simple approaches are employed,  e g,  retain first n sentences. 0 0
In (Liu et-al. 2009):
Also,  summarization and several other NLP tasks(  e g,  information extraction and question answering)  can reciprocally boost each other s performance With text as the default material medium,  summarization is de ned as ar eductive transformation of source text to summary text through content condensation by selection and or generalization of what is considered important in the int he source. 0 0.5
Automatic text summarization can save people time and e ort in acquiring knowledge from a single document or a collection of texts. 0 1
Document summarization can be viewed as a reductive distilling of source text through content condensation,  while words with high quantities of information are believed to carry more content and thereby importance. 0 1
In order to make the performance comparison with other existing approaches more feasible,  our evaluations are carried out by following the tasks given in DUC. 0 0
In this paper,  we propose a new qu anti cation measure for word signi cance used in natural language processing(  NLP)  tasks,  and successfully apply it to an extractive text summarization approach. 0 0
Document Understanding Conference(  DUC)  3 is aimed at providing a text summarization and evaluation platform with large scale document sets available and enabling more researchers to participate in and to make further progress in this area. 0 0
In (Khan et-al. 2015):
These approaches employ syntactic parser to represent the source text syntactically. 0 1
Our proposed framework differs from other abs tractive summarization approaches in a few aspects. 1 1
The framework for multi document abs tractive summarization presented in this study,  is different from previous abs tractive summarization approaches in a few aspects. 1 0
On other hand,  a few semantic based approaches have also been proposed for abs tractive summarization and are briefly discussed as follows. 1 0
The major limitation of all the semantic based approaches for abs tractive summarization is that they are mostly dependent on human expert to construct domain ontology and rules which is a drawback for an automatic summarization system. 1 1
Two approaches are employed to multi document summarization. 0 0
Since abs tractive summarization requires deep analysis of text,  therefore,  semantic representation of source text will be a more suitable representation. 1 0.5
SRL has been widely applied in text content analysis tasks such as text retrieval. 0
The Document Understanding Conference(  DUC)  is a standard corpus used in text summarization research,  which contains documents along with their human model summaries. 0 0
7-tMultilingual approaches for text summarization
In (Gupta and Lehal 2010):
Multilingual text summarization is to summarize the source text in different language to the target language final summary. 1 1
Multilingual Extractive Text summarization. 0 0
Automatic text summarization system. 0 0
Extractive text summarization process. 0 0
Automatic text summarization based on fuzzy logic. 0 0
This paper focuses on extractive text summarization methods. 1 0
If one approaches the task of text abstracting from such a probabilistic modeling perspective,  it might well be possible that HMMs could be employed for this purpose,  as well. 0 1
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 0 0.5
The text summarization software should produce the effective summary in less time and with least redundancy. 0 0
These features are important as,  a number of methods of text summarization are using them. 0 0
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 0.5 0
Core summarization problem of MINDS is taking a single text and producing a shorter text in the same language that contains all the main points in the input text. 0 0.5
Thirdly,  It has used the Markov Model to order the subtopics that the final summarization should contain and output the text summarization according to the sentence ranking score of all sentences within one subtopic as user'  requirement. 0 0
integrates multilingual summarization and multi document summarization capabilities using a multi engine,  core summarization system and provides fast,  interactive document access through hypertext summaries. 1 0
In (Fung P, Ngai G 2006):
One of the most robust and domain independent summarization approaches is extraction based or shallow summarization Mani 1999. 0 1
Multilingual document summarization,  hidden Markov models. 0 0
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 1 0.5
Conventional extractive summarization methods picks out salient sentences according to either their positions in the text and or closeness to the centroid sentence to be included in a summary. 0 1
The growing interest in multilingual summarization Mani 1999. 0.5 0
Existing multilingual summarization systems(  e g,  Radev. 0.5 0
Multidocu ment summarization is an extension of single document summarization. 0 0
Other researchers have used Text Tiling in extraction based summarization as well. 0 0
We compare single document summarization results on unknown documents to two other baseline systems. 0 0
The linearity of the text is hence preserved in our model. 0 0
Our proposed Na ve Bayes salient sentence selection algorithm for summarization is described in Section 5. 0 1
There is no regard for the ordering information between sentences or text cohesion. 0 1
In particular,  our proposed meta summarization method takes full advantage of the proposed HMSM for multi document summarization. 0 1
In addition to address ing this issue,  this article uses a summarization method that is distinct from both Fung et al. 0 1
The superior per for mance of our method shows that modeling text cohesion improves summariza ti on results. 1 1
In (Gupta V 2013):
Hybrid Multilingual Summarizer,  Multilingual Hindi Punjabi Summarizer,  Hindi Extractive Summarization,  Punjabi Extractive Summarizer. 0 0
function is a simple example of text summarization system for English. 0 0
Most of these text summarization systems are for English and other foreign languages. 1 0
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 0 1
When it comes to Indian languages,  automatic text summarization systems are still lacking. 1 1
In some of summarization systems,  users can specify percentage of total source text in final summary. 0 0
From the above equation,  weights of each of nine features of HindiPunjabi text summarization have been calculated. 0.5 0
Automatic text summarization is one of the widely used applications in the field of natural language processing(  NLP). 0 0
Various automatic text summarization systems are commercially or non commercially available for most of the commonly used natural languages. 0 0
It is first time that this hybrid algorithm for multilingual text summarization has been proposed which supports both Hindi and Punjabi text. 1 1
An extractive summarization method. 0 0
Automatic text summarization. 0 0
has been used as model to estimate the text features weights for HindiPunjabi text summarization. 1 0
From results of weight learning in Table 1,  we can conclude that two most important features of hybrid algorithm for multilingual summarization of Hindi and Punjabi text are font feature and position feature. 1 0.5
In (Yeh et-al. 2005):
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 0 0
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 0 0
This paper proposes two approaches to address text summarization. 0 0
outperforms keyword based text summarization approaches. 0 0
Both approaches concentrate on single document summarization and generate indicative,  2 extract based summaries. 0 0.5
In this paper,  we propose two text summarization approaches. 0 0
The format of summaries is another criterion to differentiate text summarization approaches. 0 0
Text search and text summarization are two essential technologies to address this problem. 0 0
In this experiment,  the feasibility of applying LSA to text summarization is evaluated. 0 0
In recent years,  a variety of text summarization methods has been proposed and evaluated. 0 0
The effect of LSA in text summarization is illustrated with an example shown in Table 17.  0 0
We conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 0 1
In (Kallimani et-al. 2011):
In this paper we discussed the different statistical approaches for abs tractive summarization in Telugu language. 0 1
Extractive and abs tractive summarization Stemming Information retrieval Automatic text summarization Word count frequency. 0 0
Automatic text summarization has been in existence since 1950. 0 0
We modeled the problem of text summarization as an IR problem. 0 1
Most of these summarization approaches aim for selecting the most informative sentences,  while less attempt has been made to generate abs tractive summaries or compress the extracted sentences and merge them into a concise summary. 0 0
The merits of automatic summarization of text are to control the size,  and prediction of content,  which could identify the relation between summarized text and the main text. 0 1
So,  different approaches can be used for the development of efficient tagger. 0 0
Further,  machine learning based approaches gives somewhat better results as compared to other approaches. 0 0
8-tSummary evaluation
8.1-tInformativeness evaluation
In (0104):
Informativeness is rendered by the methods of selecting the information from documents to incorporate it into the summary. 0 0
ROUGE includes four automatic evaluation methods that measure the similarity between summaries. 1 0
Following the recent adoption of automatic evaluation techniques(  such as BLEU/NIST)  by the machine translation community,  a similar set of evaluation metrics – known as ROUGE 10 – were introduced for both single and multi document summarization. 1 1
Recently,  a series of government sponsored evaluation efforts in text summarization have taken place in both the United States and Japan. 1 0
Our evaluation results show a significant improvement in the quality of summaries based on topic themes over MDS methods that use other alternative topic representations. 1 1
The problem of using topic representations for multi document summarization(  MDS)  has received considerable attention recently. 0 0
For this supervised learning paradigm,  the abstracts created by humans are also considered. 0 0
We have implemented the classification with binary trees,  considering for each candidate theme the features represented in Table 1. 0 0
The selection is cast as a binary classification problem that can be solved through inductive methods. 0 0
Both these topic representations correspond to the notions of theme and theme change. 0 0
Selection of the candidate themes is made by considering the mappings of the clusters into(  1)  the topic representation T R and(  2)  the topic representation T R. 0 0
Step 4. 0 0
(  a)  the sentence position and document number where the argument was recognized,  followed by(  b)  the word number within the sentence where the argument starts,  and(  c)  the word number where the argument ends. 0 0
The format of the triplets is. 0 0
the number of times the lexeme arrest is used as a predicate in the cluster,  and the total number of times the same lexeme is recognized throughout the entire collection. 0 0
Two statistics modeling the coverage of the predicate argument representation are collected. 0 0
For example,  for the theme illustrated in Figures 2 and 4,  the cluster centered around the predicate ARREST would consider arguments Arg,  Arg,  Arg(  semantically consistent) ,  as well as ArgMLOC,  ArgMTMP,  and ArgMADV,  as illustrated in Figure 5. 0 0
The representation consists of(  1)  the predicate, (  2)  the semantically consistent argument,  and(  3)  arguments that anchor the predicate in time and location,  or describe the cause,  manner or effect of the predicate. 0 0
These conceptual representations are similar to the dependency based representation employed by. 0 0
Conceptual representations for each cluster are generated. 0 0
Step 3. 0 0
Spanish citizens,  consistent with Spaniards,  due to mappings to the same concept of the ontology employed in T R. 0 0
he referring to Pinochet,  and the patient. 0 0
the agent. 0 0
For each argument,  a list of triplets is recorded as well as a text snippet. 0 0
For training purposes,  only themes that are manifested in the human created abstracts are considered as positive examples. 0 0
ARREST,  WARRANT,  CHARGE,  PROTEST,  TRIAL,  REACT. 0 0
in both cases the predicate MURDER is used,  for which we have two associated thematic roles. 0 0
The themes are structured into a graph. 0 0
Step 6. 0 0
13. 0 0
The weights are given by the method presented in. 0 0
The discourse relations are modeled by a three valued feature. 0 0
Whenever we recognized any of these two discourse relations between a text unit containing one of our selected themes and any other text unit,  we would select the relation for inclusion in the theme representation,  and later in the summary. 0 0
13. 0 0
These two relations were recognized by the same naive Bayes classifiers as the one reported in. 0 0
To study the interaction of discourse relations on our theme representation,  we have considered only the CONTRAST and CAUSEEXPLANATION relations. 0 0
argues that for some summaries,  the structure of discourse helps in selecting better textual units. 
For example,  for topic T,  the themes that were selected were. 0 0
13. 0 0
Cohesive relations in the same sentence receive a weight a = 10,  in successive sentences a weight b = 5,  and in the same segment a weight c = 1. 0 0
Cohesive relations between themes belonging to the same segment are identified similarly to the cohesive relations between succeeding sentences. 0 1
Themes co occurring in successive sentences are recognized when pairs of theme relevant predicates are recognized each in one of the sentences. 0 1
For example,  in Figure 4,  both predicates ARREST and MURDER co occur both in S and S. 0 0
Themes co occur in the same sentence because their representative predicates share an argument or one of the predicates belongs to an argument of the other. 0 0
Often themes co occur(  1)  in the same sentence, (  2)  in successive sentences,  or(  3)  in the same text segment. 0 0
(  1)  Cohesion relations. 0 0
We have considered two forms of such relations. 0 0
In a topic,  there are meaningful relations between the themes. 0 0
Step 5. 0 0
(  2)  Discourse relations. 0 0
The message is semantically consistent. 0 0
In Figure 4,  semantic consistency is sought only for Arg,  Arg,  and Arg. 0 0
To find semantic consistency between Arg of predicate ARREST in S and Arg of the same predicate in S the frame semantics of the noun allegation are used. 0 0
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (Li et-al. 2013):
We train the CRF model with the Pocket CRF toolkit 5 using the guided compression corpus collected in Section 3. 0 0
We employ thePenn2Malt toolkit 4 to convert the parse result from the Berkeley parser to the dependency parsing tree,  and use these dependency features. 0 0
In future,  we would like to further explore the reinforcement relationship between keywords and summaries(  Wan et al,  2007) ,  improve the readability of the sentences generated from the guided compression system,  and report results using multiple evaluation metrics(  Nenkova et al,  2007; Louis and Nenkova,  2012)  as well as performing human evaluations.AcknowledgmentsPart of this work was done during the first author s internship in Bosch Research and Technology Center. 0 1
this is the current word s document frequency based on the 10 documents associated with each topic Bi gram Big ram document frequency. 0 0
In this paper,  we propose a pipeline summarization approach that combines a novel guided compression model with ILPbased summary sentence selection sentences election. 0 0
8.4-tText summarization evaluation programs 
In (Neto et-al. 2000):
“ Text summarization is still an emerging field,  and serious questions remain concerning the appropriate methods and types of evaluation. 1 1
Here the topic is provided as an input to the summarization system,  and the evaluation seeks to determine whether the ... 0.5 0
In order to make our experiments more relevant,  we have used the evaluation thee valuation framework of a large scale project for evaluating text summarization algorithms. 0.5 0
It should be noted that this definition implies that,  in this task,  summary evaluation is both extrinsic and subjective. 0.5 1
theTIPSTER Text Summarization Evaluation Conference(  SUMMAC)  – hereafter referred to as the SUMMAC project for short Mani et al   98. 1 0
Text mining is an emerging field at the intersection of several research areas,  including data mining,  natural language processing,  and information retrieval Feldman & Dagan 95. 0 0
5 He said he will send Congress a list of $20 billion in specific federal grants to states from which the administration and Congress would jointly select the grant programs to be killed. 0 0
This system was chosen mainly because,  overall,  it produced the best results of the SUMMAC evaluation in theabovedescribed adhoc task We have compared the summaries produced by the two systems – our system and theCGI/CMU system - for several source texts. 0.5 0
However,  the president did revive proposals to foster longtermgrowth,  such as lower capital gains tax rates and tax incentives for personal savings and research and development He said he will send Congress a list of $20 billion in specific federal grants to states from which the administration and Congress would jointly select the grant programs to be killed. 0 0
document clustering and text summarization Document clustering is performed by using the Auto class data mining algorithm. 0 0
In this case it runs a summarization algorithm that extracts the most relevant sentences from a document. 0.5 0
These keywords can be regarded as an ultra compact “ summarization of the contents of that cluster. 0 0
In this section we describe the results of an experiment carr ied out to evaluate our text summarization algorithm. 0 0
In theory,  this functionality makes the summarization algorithm more powerful and improves the comprehensibility of the output summary. 0.5 0
As mentioned before,  the summarization algorithm developed in our system extracts the most relevant sentences from a document. 0.5 0
In this case our algorithm could be directly compared against the 16 summarization algorithms that participated in the adhoc task of that project. 0.5 1
a document clustering algorithm or a text summarization one The document clustering algorithm uses the Auto class algorithm Cheese man et al   88. 1.5 0
In (Radev et-al. 2004a):
evaluate an existing summarizer,  test a summarization feature,  test a new evaluation metric,  test a shortquerymachine translation system. 1 0
evaluate an existing summarizer,  test a summarization feature,  test a new evaluation metric,  test a short query machine translation system.1 0
Co selection Cos election metrics include pregeneral classes of evaluation metrics. 0 0
www summarization com mead. 0 0
The MEAD evaluation toolkit(  MEADEval) ,  previously into MEAD as of version 3.07. 0.5 0
MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection. 0.5 1
The collectionmer workshop on Text Summarization(  Radev et al,  2002)  the University of Michigan for CST(  Crossdocumentclude subsumption,  identity,  ful llment,  paraphrase,  Structure Theory)  relationships. 0
Finally,  MEADhas been used in numerous applications,  ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection. 0 0
MEAD is the most elaborate publicly available platform for multilingual summarization and evaluation. 
a description of all related documents that will repeated sentences,  chronological ordering,  source preferences,  etc)  In addition to a number of command line utilities,  MEAD provides a Perl API which lets external programs access its internal libraries. 0.5 0.5
plements multiple summarization algorithms(  at arbitrary compression rates)  such as position based,  centroid based,  largest common subsequence,  and keywords. 0.5 0
Several systems have been built on top of MEAD,  speciﬁcallyNewsInEssence(  Radev et al,  c Radev et al,  b) (  online news tracking and summarization) ,  WebInEssence(  Radev et al,  d) (  clustering and summarization of Web hits) ,  and WAPMead(  in progress) (  wireless access to summarization for email access). 0 0.5
Several systems have been built on top of MEAD,  specifically NewsInEssence(  Radev et al,  c Radev et al,  b) (  online news tracking and summarization) ,  WebInEssence(  Radev et al,  d) (  clustering and summarization of Web hits) ,  and WAPMead(  in progress) (  wireless access to summarization for email access). 0 0
In (Gupta and Lehal 2010):
is the multilingual summarization and evaluation method. 0 0
Text summarization with neural networks. 0 0
Text summarization using regression for estimating feature weights. 0 0
Text summarization based on fuzzy logic system architecture. 0 0
Text summarization. 0 0
Multilingual Extractive Text summarization. 0 0
Then testing data are introduced to the system model for evaluation of its efficiency. 0 0
Text Summarization methods can be classified into extractive and abs tractive summarization. 0 0
Text Summarization methods can be classified into extractive and abs tractive summarization. 0 0
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 0 0
While intrinsic methods attempt to measure summary quality using human evaluation and extrinsic methods measure the same through a task based. 1 0
Extractive text summarization process. 0 0
Automatic text summarization system. 0 0
integrates multilingual summarization and multi document summarization capabilities using a multi engine,  core summarization system and provides fast,  interactive document access through hypertext summaries. 0 0
While intrinsic methods attempt to measure summary quality using human evaluation and extrinsic methods measure the same through a task based performance measure. 1 0
In query specific opinion summarization system. 0 0
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 0 0
9-tEvaluation results
10-Future directions in text summarization
