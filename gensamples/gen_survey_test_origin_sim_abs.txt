root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In  Harabagiu S, Lacatusu F 2005: 
The problem of using topic representations for multi document summarization   MDS   has received considerable attention recently
Several topic representations have been employed for producing informative and coherent summaries
In this article,  we describe five previously known topic representations and introduce two novel representations of topics based on topic themes
2-tVarious types of text Summarization
In  Zajic et-al. 2008: 
We present two approaches to email thread summarization collective message summarization   CMS   applies a multi document summarization approach,  while individual message summarization   IMS   treats the problem as a sequence of single document summarization tasks
Experimental results point to two findings that CMS represents a better approach to email thread summarization,  and that current sentence compression techniques do not improve summarization performance in this genre
Instead of a purely extractive approach,  we employ linguistic and statistical methods to generate multiple compressions,  and then select from those candidates to produce a final summary
In  Fattah and Ren 2009: 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
First,  we investigate the effect of each sentence feature on the summarization task
Furthermore,  we use trained models by one language to test summarization performance in the other language
The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
In  Sarkar 2010: 
This paper presents a sentence compression based summarization technique that uses a number of local and global sentence trimming rules to improve the performance of an extractive multi document summarization system
In effect,  the trimming process makes rooms for more diverse and salient units to appear in a summary
The trimming component eliminates the low content and redundant elements from the sentences in the draft summaries using a number of local and global sentence trimming rules without hampering the grammaticality and the fluency of the summaries
For managing a vast hoard of online or offline information,  summarization can be the useful means because the users can decide about the relevance of an individual document or a document cluster using just summary information
In  Dunlavy et-al. 2007: 
Although the DUC data and evaluations were originally designed to test multi document summarization,  we developed a framework to extend it to the task of evaluation for each of the three components query,  clustering,  and summarization
We demonstrate the improved performance by a series of experiments using standard test sets from the Document Understanding Conferences   DUC   as measured by the best known automatic metric for summarization system evaluation,  ROUGE
The user interface is designed to provide access to detailed information in a compact and useful format
We present a novel integrated information retrieval system the Query,  Cluster,  Summarize   QCS   system which is portable,  modular,  and permits experimentation with different instantiations of each of the constituent text analysis components
In the current implementation,  Latent Semantic Indexing is used for retrieval,  generalized spherical kmeans is used for the document clustering,  and a method coupling sentence trimming and a hidden Markov model,  followed by a pivoted QR decomposition,  is used to create a single extract summary for each cluster
Research and development of such systems is often hampered by the difficulty in evaluating how each particular component would behave across multiple systems
In  Riedhammer et-al. 2010: 
Following the idea that information is spread over the utterances in form of concepts,  we describe a system which nds an optimal selection of utterances covering as many unique important concepts as possible
We conclude on the be ne ts and drawbacks of the presented models and give an outlook on future aspects to improve extractive meeting summarization 2010 Else vier B.V
We analyze and discuss the performance of both approaches using various evaluation setups on two well studied meeting corpora
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain
Based on utterance comparison,  we introduce an optimal formulation for the widely used greedy maximum marginal relevance   MMR   algorithm
In  Song et-al. 2011: 
We demonstrate the improved performance by a series of experiments using standard test sets,  e g
It selects the most important sentence from each cluster to represent the overall meaning of document
Modern information retrieval   IR   systems consist of many challenging components,  e g clustering,  summarization,  etc   Nowadays,  without browsing the whole volume of data sets,  IR systems present users with clusters of documents they are interested in,  and summarize each document briefly which facilitates the task of finding the desired documents
The experimental results show that FEOM achieves performance as good as or better than state of arts of clustering and summarizing systems
In view of the nature of biological evolution,  we take advantage of several fuzzy control parameters to adaptively regulate the behaviors of the evolutionary optimization,  which can effectively prevent premature convergence to a local optimal solution
In order to further investigate its effectiveness as a generic data clustering model,  FEOM is then applied to sentence clustering based extractive document summarization
In  Radev et-al. 2001: 
In this paper,  we present our recent work on the development of a scalable personalized webbasedmultidocument summarization and recommendation system
We address some of the design issues to improve the scalability and readability of our multi document summarizer included in WebInEssence
WebInEssence is designed to help end users e ectively search for useful information and automatically summarize selected documents based on the users personal pro les
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In  Fattah and Ren 2009: 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
First,  we investigate the effect of each sentence feature on the summarization task
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
Moreover,  we use all feature parameters to train feed forward neural network   FFNN  ,  probabilistic neural network   PNN   and Gaussian mixture model   GMM   in order to construct a text summarizer for each model
3.2-tTopic based approaches
In  Hearst 1997: 
TextTiling is a technique for subdividing texts into multi paragraph multipara graph units that represent passages,  or subtopics
The discourse cues for identifying major subtopic shifts are patterns of lexical co occurrence and distribution
The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts
3.3-tGraph based approaches
In  Baralis et-al. 2013: 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis
This paper presents a novel and general purpose graph based summarizer,  namely GraphSum   Graph based Summarizer
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches
3.4-tDiscourse based approaches
In  Fattah and Ren 2009: 
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles
First,  we investigate the effect of each sentence feature on the summarization task
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
In  Yang et-al. 2014: 
The effectiveness of the proposed framework is demonstrated by both the cluster quality analysis and the summarization evaluation conducted on the DUC 2004 and DUC2007 data sets
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In  Yeh et-al. 2005: 
This paper proposes two approaches to address text summarization modified corpus based approach   MCBA   and LSAbased T.R.M approach   LSA + T.R.M
The first is a trainable summarizer,  which takes into account several features,  including position,  positive keyword,  negative keyword,  centrality,  and the resemblance to the title,  to generate summaries
Two new ideas are exploited
1   sentence positions are ranked to emphasize the significances of different sentence positions,  and   2   the score function is trained by the genetic algorithm   GA   to obtain a suitable combination of feature weights
The second uses latent semantic analysis   LSA   to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map
4.2-tInformation extraction using sentence based abstraction technique
In  Chan 2006: 
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines
In this paper,  we provide a novel quantitative model for the creation of a summary by extracting a set of sentences that represent the most salient content of a text
The model is based on a shallow linguistic extraction technique
What distinguishes it from previous research is that it does not work on the detection of specific keywords or cue phrases to evaluate the relevance of the sentence concerned
4.3-tText understanding and summarization through document concept lattice
In  Ye et-al. 2007: 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document   s   that can be preserved after summarization
Here,  a concept refers to an abstract or concrete entity or its action often expressed by diverse terms in text
Summary generation can thus be considered as an optimization problem of selecting a set of sentences with minimal answer loss
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics
4.4-tSentence extraction through contextual information and statistical based summarization of text
In  Ko and Seo 2004: 
A wide range of supervised learning algorithms has been applied to Text Categorization
However,  the supervised learning approaches have some problems
One of them is that they require a large,  often prohibitive,  number of labeled training documents for accurate learning
Generally,  acquiring class labels for training data is costly,  while gathering a large quantity of unlabeled data is cheap
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In  Carenini et-al. 2008: 
We first build a sentence quotation graph that captures the conversation structure among emails
We adopt three cohesion measures clue words,  semantic similarity and cosine similarity as the weight of the edges
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones
Second,  we use two graph based summarization approaches,  Generalized ClueWordSummarizer and PageRank,  to extract sentences as summaries
In  Kim and Hovy 2005: 
Then we describe recognizing opinion bearing sentences using these words We test the system on 3 different test sets
We show that our automatic method for obtaining opinion bearing words can be used effectively to identify opinion bearing sentences
We first define what an opinion means in our research and introduce an effective method for obtaining opinion bearing and nonopinionbearing words
4.6-tSummarization of text through complex network approach
In  Antiqueira et-al. 2009: 
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary
The graph or network representing one piece of text consists of nodes corresponding to sentences,  while edges connect sentences that share common meaningful nouns
Because various metrics could be used,  we developed a set of 14 summarizers,  generically referred to as CNSumm,  employing network concepts such as node degree,  length of shortest paths,  drings and kc ores
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In  Fattah and Ren 2009: 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
First,  we investigate the effect of each sentence feature on the summarization task
Then we use all features in combination to train genetic algorithm   GA   and mathematical regression   MR   models to obtain a suitable combination of feature weights
Moreover,  we use all feature parameters to train feed forward neural network   FFNN  ,  probabilistic neural network   PNN   and Gaussian mixture model   GMM   in order to construct a text summarizer for each model
4.9-tQuery-based summarization of multiple documents by applying regression models
In  Carbonell and Goldstein 1998: 
This paper presents a method for combining query relevance with information novelty in the context of text retrieval and summarization
The Maximal Marginal Relevance   MMR   criterion strives to reduce redundancy while maintaining query relevance in re ranking reran king retrieved documents and in selecting appropriate passages for text summarization
Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization
4.10-tMaximum coverage and minimum redundancy in summarization of text
In  Alguliev et-al. 2011: 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document   s
In particular,  we model text summarization as an integer linear programming problem
One of the advantages of this model is that it can directly discover key sentences in the given document   s   and cover the main content of the original document   s
This model also guarantees that in the summary can not be multiple sentences that convey the same information
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In  Ferreira et-al. 2013: 
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization
This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature
Abs tractive summaries attempt to improve the coherence among sentences by eliminating redundancies and clarifying the contest of sentences
Text summarization is the process of automatically creating a shorter version of one or more text documents
In addition,  directions to improve the sentence extraction results obtained are suggested
In  Lin 2004: 
Three of them have been used in the Document Unde rstanding Conference   DUC   2004,  a large scale summarization evaluation sponsored by NIST
The measures count the number of ove rlapping units such as ngram,  word sequences,  and word pairs between the computer generated summary to be evaluated and the ideal summaries created by humans
It includes measures to automatically determine the quality of a summary by comparing it to other   ideal   summaries created by humans
ROUGEN,  ROUGEL,  ROUGEW,  and ROUGES included in the ROUGE summarization evaluation package and their evaluatio ns
In  Fattah and Ren 2009: 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries
Furthermore,  we use trained models by one language to test summarization performance in the other language
First,  we investigate the effect of each sentence feature on the summarization task
The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles
In  Gupta et-al. 2011: 
Our approach for summarizing the text determines more significant and topically related sentences in the text and thus outputs a higher summary quality
Thus,  the paper aims at highlighting the effect of lexical chain scoring after the nouns and compound nouns are chained by searching for lexical cohesive relationships between words in the text using WordNet and using lexicographical relationships such as synonymy and hyponyms
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity   measuring chain score,  word score and finally sentence score   for ranking the text units
Note that the value of threshold is predetermined based on the percentage of output summary required to be generated
We find that the top ranked sentences are most of the time the most important ones that human subjects extract in our experiment
The deviation in percentage for precision and recall as obtained from human generated summary and our system generated summary is 12% and 8 % respectively
We present an approach of identifying the most prominent text sentences using various shallow linguistic features,  taking degree of connective ness among the text units into consideration so as to minimize the poorly linked sentences in the resulting summary
In  Barrera and Verma 2012: 
The goal of this study is to reconsider the importance of single document summarization by introducing a new approach and its implementation
The goal of automated summarization is to tackle the information overload problem by extracting and perhaps compressing the most important content of a document
Due to the difficulty that single document summarization has in beating a standard baseline,  especially for news articles,  most efforts are currently focused on multi document summarization
Successful summary evaluation results and baseline out performance are demonstrated when our system is executed on two separate data sets the Document Understanding Conference   DUC   2002 data set and a scientific magazine article set
In  Nobata et-al. 2001: 
We have developed a sentence extraction system,  which estimates the signi cance of sentences by integrating four scoring fours coring functions that use evidence such as sentence location,  sentence length,  TF/IDF values of words,  and similarity to the title
Similarity to a given query is also added to the tot he system on the summarization task for Information Retrieval
Parameters for scoring functions were optimized by an experiment using dry run data of the TSC
In  Mihalcea and Tarau 2004: 
In particular,  we propose two innovative unsupervised methods for keyword and sentence extraction,  and show that the results obtained compare favorably with previously published results on established benchmarks
In this paper,  we introduce TextRank – a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In  Baralis et-al. 2013: 
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms
The experiments performed on benchmark and real life documents demonstrate the effectiveness of the proposed approach compared to many stateoftheart summarizers
The graph nodes,  which represent combinations of two or more terms,  are first ranked by means of a PageRank strategy that discriminates between positive and negative term correlations
Then,  the produced node ranking is used to drive the sentence selection process
In  Brin and Page 1998: 
This paper addresses this question of how to build a practical large scale system which can exploit the additional information present in hypertext
Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems
The prototype with a full text and hyperlink database of at least 24 million pages is available at http google stanford edu
Search engines index tens to hundreds of millions of Web pages involving a comparable number of distinct terms
In this paper,  we present Google,  a prototype of a large scale search engine which makes heavy use of the structure present in hypertext
This paper provides an in depth description of our large scale Web search engine — the first such detailed public description we know of to date
Apart from the problems of scaling traditional search techniques to data of this magnitude,  there are new technical challenges involved with using the additional information present in hypertext to produce better search results
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In  Alguliev et-al. 2013: 
This paper proposes an optimization based model for generic document summarization
The model generates a summary by extracting salient sentences from documents
This approach uses the sentencetodocument collection,  the summarytodocument collection and the sentencetosentence relations to select salient sentences from given document collection and reduce redundancy in the summary
To solve the optimization problem has been created an improved differential evolution algorithm
The algorithm can adjust crossover rate adaptively according to the fitness of individuals
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In  Yang et-al. 2014: 
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information
As the length of sentences is short and the content it contains is limited,  the bagofwords cosine similarity traditionally used for document clustering is no longer reasonably suitable
Special treatment for measuring sentence similarity is necessary
4.18-tStatistical and linguistic based summarization system for multiple documents
In  Ferreira et-al. 2014: 
Text summarization,  the process of automatically creating a shorter version of one or more text documents,  is an important way of finding relevant information in large text libraries or in the Internet
One solution to this problem is offered by using text summarization techniques
This paper presents a multi document summarization system that concisely extracts the main aspects of a set of documents,  trying to avoid the typical problems of this type of summarization information redundancy and diversity
The DUC 2002 data set was used to assess the performance of the proposed system,  surpassing DUC competitors by a 50% margin of fmeasure,  in the best case
In  Ferreira et-al. 2013: 
Text summarization is the process of automatically creating a shorter version of one or more text documents
It is an important way of finding relevant information in large text libraries or in the Internet
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization
This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature
Extractive techniques perform text summarization by selecting sentences of documents according to some criteria
In  Mihalcea and Tarau 2004: 
In this paper,  we introduce TextRank – a graphbasedranking model for text processing,  and show how this model can be successfully used in natural language applications
In particular,  we propose two innovative unsupervised methods for keyword and sentence extraction,  and show that the results obtained compare favorably with previously published results on established benchmarks
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In  Fang et-al. 2015: 
The summarization is desirable to efficiently apprehend the gist of the huge amount of data and becomes a significant challenge in many applications such as news article summarization and social media mining
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization   abbreviated as TAOS
The underlying intuition of the proposed TAOS is that different topics can prefer different aspects and the different aspects can be represented by different preference of features   e g,  technical topic may prefer proper noun than sports topic
In order to materialize the intuition of TAOS,  we first extract several groups of features according to topic factors,  and then a group norm penalty   i e,  norm   and latent variables are utilized to select overlapping groups of features
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In  Heu et-al. 2015: 
Multi document summarization techniques aim to reduce documents into a small set of words or paragraphs that convey the main meaning of the original document
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic
However,  these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an outofdate dictionary or thesaurus
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents
We first create a word frequency table for analyzing the semantics and contributions of words using the HITS algorithm
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In  Kaljahi et-al. 2014: 
This paper describes a series of French semantic role la belling label ling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment
Using universal partofspeech tags and dependencies makes little difference over the original fine grained tag set tags et and dependency scheme
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In  Liu et-al. 2015: 
Multi document summarization is of great value to many tom any real world applications since it can help people get the main ideas within a short time
In this paper,  we tackle the problem of extracting summary sentences from multi document sets by applying sparse coding techniques and present a novel framework to this challenging problem
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization
Three requisite properties is proposed to form an ideal re constructable reconstruct able sum
mary
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In  Li et-al. 2015a: 
whether or not a sentence is in the summary,  or unsupervised methods such as graph based approaches to rank the sentences
Recently global optimization methods such as integer linear programming   ILP   have been shown to be quite powerful for this task
First we use syntactic information to select more important bi grams big rams
Second,  to estimate the importance of the oft he bi grams big rams,  in addition to the internal features based on the test documents   e g,  document frequency,  bi gram big ram positions  ,  we propose to extract features by leveraging multiple external resources   such as word embedding from additional corpus,  Wikipedia,  Dbpedia,  WordNet,  SentiWordNet
The bi gram big ram weights are then trained discriminatively in a joint learning model that predicts the bi gram big ram weight sand weights and selects the summary sentences in the ILPframework at the same time
We demonstrate that our system consistently outperforms the prior ILP method on different TAC data sets,  and performs competitively compared to other previously reported best results
We also conducted various analyses to show the contributions of different components
For example,  Gil lick et al     2009   used ILPto achieve the best result in the TAC 09 summarization task
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In  Cao et-al. 2015a: 
We develop a Ranking framework upon Recursive Neural Networks   R2N2   to rank sentences for multi document summarization
It formulates the sentence ranking task as a hierarchical regression process,  which simultaneously measures the salience of a sentence and its constituents   e g,  phrases   in the parsing tree
This enables us to draw on word level to sentence level supervisions derived from reference summaries
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In  Parveen and Strube 2015: 
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously
We represent input documents by means of a bipartite graph consisting of sentence and entity nodes
We rank sentences on the basis of importance by applying a graph based ranking algorithm to this graph and ensure nonredundancyand local coherence of the summary by means of an optimization step
Our graph based method is applied to scienti c articles from the journal PLOSMedicine
We use human judgements to evaluate the coherence of our summaries
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In  Bairi et-al. 2015: 
We study the problem of summarizing DAGstructured topic hierarchies over a given set of documents
Example applications include automatically generating Wikipedia disambiguation pages for a set of articles,  and generating candidate multi labels for preparing machine learning data sets   e g,  for text classification,  functional genomics,  and image classification
Unlike previous work,  which focuses on clustering the set of documents using the topic hierarchy as features,  we directly pose the problem as a sub modular optimization problem on a topic hierarchy using the documents as features
Desirable properties of the chosen topics include document coverage,  specificity,  topic diversity,  and topic homogeneity,  each of which,  we show,  is naturally modeled by a sub modular function
Other information,  provided say by unsupervised approaches such as LDA and its variants,  can also be utilized by defining a sub modular function that expresses coherence between the chosen topics and this information
4.23.10-tDisaster summarization through prediction of salient updates
In  Kedzie et-al. 2015: 
During crises such as natural disasters or other human tragedies,  information needs of both civilians and responders often require urgent,  specialized treatment
Monitoring and summarizing a text stream during such an event remains a difficult problem
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates
4.23.11-tSummarizing multiple documents through system combination
In  Hong et-al. 2015: 
We present a novel framework of system combination for multi document summarization
For each input set   input  ,  we generate candidate summaries by combining whole sentences from the summaries generated by different systems
We show that the oracle among these candidates is much better than the summaries that we have combined
We then present a supervised model to select among the candidates
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In  Graham 2015: 
We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern
1   movement away from evaluation by correlation with human assessment   2   omission of important components of human assessment from evaluations,  in addition to large numbers of metric variants   3   absence of methods of significance testing improvements over a baseline
We outline an evaluation methodology that overcomes all such challenges,  providing the first method of significance testing suitable for evaluation of summarization metrics
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In  Ganesan et-al. 2010: 
We present a novel graph based summarization framework   Opinosis   that generates concise abs tractive summaries of highly redundant opinions
Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method
In  Kallimani et-al. 2011: 
This work is built upon past work of extractive summarization methods to create abs tractive summaries by creating new sentences in it
An alternative solution is abs tractive summarization in which the information from the source text is first extracted into the form of abstract data which is then post processed to infer the most important message from the original text
The previous work in automatic summarization has completely focused on extractive summarization,  in which key sentences are identified from the source text and extracted to form the output
This paper conveys the methodology for the abs tractive summarization process and its evaluation considering Telugu,  a south Indian regional language,  as the language of study
In  Khan et-al. 2015: 
We propose a framework for abs tractive summarization of multi documents,  which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents
Our proposed framework differs from other abs tractive summarization approaches in a few aspects
Content selection for summary is made by ranking the predicate argument structures based on optimized features,  and using language generation for generating sentences from predicate argument structures
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling
Secondly,  it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text and finally it ranks the predicate argument structures based on features weighted by genetic algorithm   GA
In  Banerjee et-al. 2015: 
Abs tractive summarization is an ideal form of summarization since it can synthesize information from multiple documents to create concise informative summaries
The sentences in the most important document are aligned to sentences in other documents to generate clusters of similar sentences
Experimental results on the DUC 2004 and 2005 multi document summarization data sets show that our proposed approach outperforms all the baselines and stateoftheart extractive summarizers as measured by the ROUGE scores
Finally,  we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming   ILP   model with the objective of maximizing information content and readability of the final summary
Our method also outperforms a recent abs tractive summarization technique
Second,  we generate Kshortest paths from the sentences in each cluster using a word graph structure
In  Bing et-al. 2015: 
Different from existing abstraction based approaches,  our method first constructs a pool of concepts and facts represented by phrases from the input documents
We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary
Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints
In  Rush et-al. 2015: 
In this work,  we propose a fully data driven approach to abs tractive sentence summarization
Our method utilizes a local attention based model that generates each word of the summary conditioned on the input sentence
While the model is structurally simple,  it can easily be trained endtoend and scales to a large amount of training data
7-tMultilingual approaches for text summarization
In  Radev et-al. 2004a: 
MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection
This paper describes the functionality of MEAD,  a comprehensive,  public domain,  open source,  multi document multilingual summarization environment that has been thus far downloaded by more than 500 organizations
In  Patel et-al. 2007: 
Through evaluations performed on a single document summarization for English,  Hindi,  Gujarati and Urdu documents,  we show that the method performs equally well regardless of the language
The algorithm is based on structural and statistical   rather than semantic   factors
For other languages,  the degree of representativeness we get is highly encouraging
The results of summarization have been compared with DUC 2002 data using degree of representativeness
The algorithm has been applied on DUC data for English documents and various newspaper articles for other languages with corresponding stop words list and modified st emmer
In  Kabadjov et-al. 2010: 
We discuss the context and motivation for developing the system and provide an overview of its architecture
The summarization method employed yielded stateoftheart performance for English at the Update Summarization task of the last Text Analysis Conference   TAC   2009 and integrated with EMM represents the first online summarization system able to produce summaries for so many languages
The paper is intended to serve as accompaniment of a live demo of the system,  which can be of interest to researchers and engineers working on multilingual open source news analysis and mining
In  Gupta and Lehal 2010: 
The importance of sentences is decided based on statistical and linguistic features of sentences
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words
In this paper,  a Survey of Text Summarization Extractive techniques has been presented
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning
An extractive summarization method consists of selecting important sentences,  paragraphs etc   from the original document and concatenating them into shorter form
In  Giannakopoulos et-al. 2008: 
The presented approach is language neutral,  due to its statistical nature,  and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods
This article presents a novel automatic method   AutoSummENG   for the evaluation of summa rization systems,  based on comparing the character ngram graphs representation of the extracted summaries and a number of model summaries
A theory for the a priori determination of the methods parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process
Within this study,  we measure the effectiveness of different representation methods,  namely,  word and character ngram graph and histogram,  dif ferent ngram neighborhood indication methods as well as different comparison methods between the supplied representations
Additional Key Words and Phrases
Categories and Subject Descriptors
8-tSummary evaluation
8.1-tInformativeness evaluation
In  Amigo et-al. 2005: 
This paper presents a probabilistic framework,  QARLA,  for the evaluation of text summarisation systems
The input of the framework is a set of manual   reference   summaries,  a set of baseline   automatic   summaries and a set of similarity metrics between summaries
It provides i   a measure to evaluate the quality of any set of similarity metrics,  ii   a measure to evaluate the quality of a summary using an optimal set of similarity metrics,  and iii   a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In  Amigo et-al. 2005: 
This paper presents a probabilistic framework,  QARLA,  for the evaluation of text summarisation systems
The input of the framework is a set of manual   reference   summaries,  a set of baseline   automatic   summaries and a set of similarity metrics between summaries
It provides i   a measure to evaluate the quality of any set of similarity metrics,  ii   a measure to evaluate the quality of a summary using an optimal set of similarity metrics,  and iii   a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results
8.4-tText summarization evaluation programs
In  Lin 2004: 
It includes measures to automatically determine the quality of a summary by comparing it to other   ideal   summaries created by humans
The measures count the number of ove rlapping units such as ngram,  word sequences,  and word pairs between the computer generated summary to be evaluated and the ideal summaries created by humans
Three of them have been used in the Document Unde rstanding Conference   DUC   2004,  a large scale summarization evaluation sponsored by NIST
ROUGEN,  ROUGEL,  ROUGEW,  and ROUGES included in the ROUGE summarization evaluation package and their evaluatio ns
In  Hovy et-al. 2006: 
As part of evaluating a summary automatically,  it is usual to determine how much of the contents of one or more human produced ideal summaries it contains
In this paper we describe a framework in which summary evaluation measures can be instantiated and compared,  and we implement a specific evaluation method using very small units of content,  called Basic Elements,  that address some of the shortcomings of ngrams
Past automated methods such as ROUGE compare using fixed word ngrams,  which are not ideal for a variety of reasons
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193–197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764–7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675–1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514–14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196–206
0074 Amigo et-al. 2005 Amigó E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL ’05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280–289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584–599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260–273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553–563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208–1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SAC’12), pp 782–786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96–109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366–377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL ’05), pp 141–148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107–117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829–833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335–336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91–100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353–361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85–112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759–777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, O’Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588–1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613–1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755–5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780–5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1–16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340–348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64–73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1–39
0037 Glavaš G, Šnajder J 2014 Glavaš G, Šnajder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904–6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40–48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128–137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203–225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717–727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258–268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620–1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSP’06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33–64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212–225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515–1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107–117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81–94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382–386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591–594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319–322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608–1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737–747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61–66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255–262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695–1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366–1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788–791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462–471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887–902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490–500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778–787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74–81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912–920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168–178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61–66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159–165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404–411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132–138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41–55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319–324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42–54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227–237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190–198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210–218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298–1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123–132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79–88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1–4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919–938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801–815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419–430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206–213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177–184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862–2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1–8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224–233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112–9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341–359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513–523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948–961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37–50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643–1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75–95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600–1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35–41
