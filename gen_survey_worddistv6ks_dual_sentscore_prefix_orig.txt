root
Recent automatic text summarization techniques: a survey
1-tIntroduction
1.1-tNeed of text summarization
In (Glavaš G, Šnajder J 2014):
Furthermore,  considering that numerous textual sources provide information about the same real world events,  the need for aggregating and summarizing the most relevant information has become obvious. 
We chose these particular kernels because their general forms have intuitive interpretations for event matching and can be easily adjusted to fit our needs for event centered text comparison. 
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
However,  this is a rather narrow and imprecise account of an event mention,  as information ally important event mentions need not necessarily involve named entities(  e g,  The rebels then detonated the bomb,  wounding at least dozen people). 
This effectively filters out all event unrelated content,  leaving event mentions as the only pieces of information relevant for event oriented information needs. 
However,  the need for a more structured representation of events has been recognized within the TDT community(  Makkonen,  2003). 
2-tVarious types of text Summarization
In (Giannakopoulos et-al. 2008):
There are various types of correlation measures,  called correlation coef cients,  depending on the context in which they can be applied. 
Even though a series of numerous experiments has already been conducted,  we feel that the presented method should be further investigated as it may hold implications concerning the potential for using the graph representation in a language neutral way for various NLP tasks. 
Trying to capture more than the simple co occurrence of words and in order to allow for different types of the same word,  our method uses character ngrams positioned within a context indicative graph. 
In (0079):
This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non discourse features(  Miltsakaki and Kukich,  2000). 
Coherent text is characterized by various types of cohesive links that facilitate text comprehension(  Halliday and Has an,  1976). 
The use of rare words or technical terminology for example can make text difficult to read for certain audience types(  CollinsThompson and Call an,  2004; Sch warm Schwa rm and Ostendorf,  2005; Elhadad and Sutaria,  2007). 
In (Carlson et-al. 2003):
Thus,  our approach contributes to the community quite differently from detailed analyses of specific discourse phenomena in depth,  such as anaphoric relations(  Gar side et al,  1997)  or style types(  Leech et al,  1997)  ; analysis of a single text from multiple perspectives(  Mann and Thompson,  1992)  ; or illustrations of a theoretical model on a single representative text(  Brit ton Britt on and Black,  1985; Van Dijk and Kintsch,  1983). 
While other discourse theoretical approaches posit distinctly different treatments for various levels of the discourse(  Van Dijk and Kintsch,  1983; Meyer,  1985) ,  RST relies on a standard methodology to analyze the document at all levels. 
Our team was able to reach a significant level of consistency,  even though they faced a number of challenges which reflect differences in the agreement scores at the various levels. 
In (Barzilay and Lapata 2005):
This approach has been shown to be highly effective in various tasks ranging from collaborative filtering(  Joachims,  a)  to parsing(  Toutanova et al,  2004). 
Table 4 summarizes the accuracy of various configurations of our model for the ordering and coherence assessment tasks. 
Each text can thus be viewed as a distribution defined over transition types. 
Our work,  however,  not only validates these findings,  but also quantitatively measures the predictive power of various linguistic features for the task of coherence assessment. 
We conjecture that this difference in performance stems from the ability of our model to discriminate between various patterns of local sentence transitions. 
In (Mihalcea and Tarau 2004):
We experimented with various syntactic lters,  including. 
A larger window does not seem to help – on the contrary,  the larger the window,  the lower the precision,  probably explained by the fact that a relation between words that are further apart is not strong enough to de ne a connection in the int he text graph Experiments were performed with various syntactic lters,  including. 
Intuitively,  TextRank works well because it does not only rely on the local context of a text unit(  vertex) ,  but rather it takes into account information recursively drawn from the entire text(  graph)  Through the graphs it builds on texts,  TextRankidentiﬁesconnections between various entities in at ext,  and implements the concept of recommendation. 
In (Carenini et-al. 2007):
We provide a comprehensive comparison of CWS with various existing methods on the Enron data set. 
Below we compare CWS and MEAD at various lengths. 
Their majors covered various disciplines including Arts,  Law,  Science and Engineering. 
Among the various sets of features explored,  their experiments show that a centroid based method is effective. 
In our earlier studies,  we focus on the reconstruction of hidden emails. 
All 25 human summarizers were undergraduate or graduate students in University of British Columbia. 
The selected emails also need to represent different types of conversation structure. 
In (Grosz et-al. 1995):
We defined various centering constructs and proposed two centering rules in terms of these constructs. 
That is,  the centers of an utterance in general,  and the backward looking center specifically,  are determined on the basis of a combination of properties of the utterance,  the discourse segment in which it occurs,  and various aspects of the cognitive state of the participants of that discourse. 
Discourse(  1)  centers around a single individual,  describing various actions he took and his reactions to them. 
Grosz and Sidner were concerned with the inferences needed to interpret anaphoric expressions of various sorts(  e g. 
In (Rush et-al. 2015):
Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. 
While much work on this task has looked at deletion based sentence compression techniques(  Knight and Mar cu(  2002) ,  among many others) ,  studies of human summarizers show that it is common to apply various other operations while condensing,  such as paraphrasing,  generalization,  and reordering(  Jing,  2002). 
Unlike BLEU which interpolates various ngram matches,  there are several versions of ROUGE for different match lengths. 
In (Kulesza and Taskar 2012):
By encouraging the model to choose a spatially diverse set of poses,  we hope to improve the chance that the model predicts a single pose fo reach for each person Our data set consists of 73 still frames taken from various TV shows,  each approximately by 540 pixels in size Sapp et al,  2010. 
In this section we brie y survey the wider world of point processes and discuss the computational properties of alternative models we will focus on point processes that lead to what is variously described as diversity,  repulsion, (  over)  dispersion,  regularity,  order,  and inhibition. 
ROUGE measures ngramoverlap statistics between the human references and the summary being scored,  and combines them to produce various sub metrics. 
In (Gupta et-al. 2011):
We present an approach of identifying the most prominent text sentences using various shallow linguistic features,  taking degree of connective ness among the text units into consideration so as to minimize the poorly linked sentences in the resulting summary. 
Our research work presents the concept of topic word similarity with the high ranking sentences obtained after applying various heuristics,  lexical chaining and the Vector Space approaches. 
In this paper,  various shallow linguistic techniques are mentioned that rank the sentences in the text. 
In (Chan 2006):
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines. 
In sum,  the discourse coherence in our model is modeled by the process of matching text against a small set of relation types,  and then using predefined ranks to organize the instances of the event state concepts that appear in the text. 
Several major types of relationships give a text cohesiveness by relating lexical items to one another. 
In (Dunlavy et-al. 2007):
Much of the recent research on automatic text summarization systems is available in the proceedings of the 2001–2006 Document Understanding Conferences The focus of these conferences is the evaluation and discussion of summarization algorithms and systems in performing sets of tasks on several types of document collections. 
In this section,  we describe the results of two sets of experiments performed to test QCS on various document collections. 
Most importantly,  the combination of the three types of methods in the QCS design improves retrievals by providing users more focused information organized by topic. 
In (Radev et-al. 2001):
• Di erent color schemes for di erent types of information Di erent colors were used for display of various types of information. 
For example,  the total number of clusters generated for those 369 URLs for the query intelligent & agents is only 73,  much smaller than the total number of To help users quickly identify clusters that are more relevant to them,  various contextual information is provided for each cluster as shown in Figure These information include. 
returned for this option is normally very low,  varying from 1 to 4 clusters for a result set of 10 URLs.For the rest of the users,  option 2 is provided to dramatically reduce the navigation space for the entire search result. 
In (Yang et-al. 2013):
Firstly,  from our description of the accumulated appearance probability of various visual patterns,  we know that each image may contain certain types of visual patterns(  positive coefficients)  or do not contain these visual patterns(  zero coefficients). 
By treating the problem of automatic image summarization as the issue of dictionary learning,  each image in the original image set can be approximately reconstructed by a nonnegative weighted linear combination of the summary images,  or in other words,  represented by accumulated probability of the appearances of various visual words(  visual patterns)  as shown in Figure   1. 
The algorithms for automatic image summarization should work on image collections with various sizes and visual variety,  so we have integrated the images from ImageNet to construct a new image category called mix by mixing the images from multiple object categories to strengthen the visual diversity and enlarge the size of image category. 
3-tClassification of extractive approaches for summary generation
3.1-tStatistical based approaches
In (Fattah and Ren 2009):
Simpler approaches were then explored that consist of extracting representative text spans texts pans,  using statistical techniques and or techniques based on surface domain independent linguistic analyses. 
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 
Our approaches have been used the feature extraction criteria which gives researchers opportunity to use many varieties of these features based on the used language and the text type. 
where Y is the output vector,  X is the input matrix(  feature parameters) ,  W is the linear statistical model of the system(  the weights w,  w,  w in Eq. 
Others were about single and multiple sentence compression using parse and trim approach and a statistical noisy channel approach(  Zajic et al,  2007)  and conditional random fields(  Nomoto,  2007). 
Moreover,  a 95% confidence interval was considered for all tests to estimate the statistical uncertainty. 
We use an intrinsic evaluation to judge the quality of a summary based on the coverage between it and the manual summary. 
Moreover,  to investigate the proposed approaches performance on news wire data,  we have exploited DUC 2001 for single document test. 
We are going to exploit the MCBA + GA approach of Yeh et al,  for summarization and use it as a baseline approach. 
We have applied our new approaches on a sample of 100 Arabic political articles and 100 English religious articles. 
However,  abs tractive approaches require deep natural language processing such as semantic representation,  inference and natural language generation,  which have yet to reach a mature stage nowadays(  Ye et al,  2007). 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
LexRank is used to compute sentence importance based on the concept of eigenvector centrality in a graph representation of sentences for multi document summarization task(  Erk an and Radev,  2004). 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
This approach is a trainable summarizer,  which takes into account several features,  including sentence position,  positive keyword,  negative keyword,  sentence centrality,  sentence resemblance to the title,  sentence inclusion of name entity,  sentence inclusion of numerical data,  sentence relative length,  Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. 
First,  we investigate the effect of each sentence feature on the summarization task. 
A set of highest score sentences are chronologically specified as a document summary based on the compression rate. 
Then we use all features in combination to train genetic algorithm(  GA)  and mathematical regression(  MR)  models to obtain a suitable combination of feature weights. 
3.2-tTopic based approaches
In (Heu et-al. 2015):
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
Hennig and Labor(  2009)  proposed a multi document summarization method based on Probabilistic Latent Semantic Analysis(  PLSA) ,  which represents sentences and queries as probability distributions over latent topics. 
This technique allows users to easily understand a topic by looking at a short summary. 
Furthermore,  WordNetbased approaches fail to analyze proper nouns(  e g,  a person s name or the name of a product or firm)  and newly coined words because these words are not present in WordNet. 
To calculate the contribution of each word in the WFT’,  we apply the Hypertext Induced Topic Search(  HITS)  algorithm to our system. 
However,  these techniques fail to semantically analyze proper nouns and newly coined words because most depend on an outofdate dictionary or thesaurus. 
3.3-tGraph based approaches
In (Glavaš G, Šnajder J 2014):
In this article,  we present event graphs,  a novel event based document representation model that filters and structures the information about events described in text. 
To construct the event graphs,  we combine machine learning and rule based models to extract sentence level event mentions and determine the temporal relations between them. 
The information retrieval model measures the similarity between queries and documents by computing graph kernels over event graphs. 
To adequately capture the semantics of events,  we introduce event graphs,  a novel event centered document representation based on sentence level event mentions. 
Building on event graphs,  we present novel models for information retrieval and multi document summarization. 
3.4-tDiscourse based approaches
In (Gupta et-al. 2011):
Based on this classification,  automatic summarizers can be characterized as approaching the problem at the surface,  entity,  or discourse level. 
In this paper,  our algorithm ranks sentences based on the sum of the scores of the words in each sentence involving approaches like term frequencies,  location of sentence in the text,  cue words and phrases,  word occurrences,  and measuring lexical similarity(  measuring chain score,  word score and finally sentence score)  for ranking the text units. 
Cohesion is thus a surface indicator of the discourse structure of a document. 
In (Yeh et-al. 2005):
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
Entity level approaches model text entities and their relationships co occurrence,  co reference,  etc,  and determine salient information based on the text entity model(  Azzam et al,  1999; McKeown & Radev,  1995). 
Sections 3 Modified corpus based approach,  4 LSA give a detail description of our proposed approaches. 
In (Pardo et-al. 2003b):
Indeed,  if we discourse analyze the sample text based on the RST Theory. 
Similarly to the other deep based work,  our system can also produce various summaries for the same source text,  for distinct strategies may be corresponding to varied choices of information units or discourse relations. 
Deep based approaches,  e g,  those based on the rhetorical structuring of source texts,  address nuclearity to select relevant information. 
3.5-tApproaches based on machine learning
4-t Recent automatic text summarization extractive approaches
4.1-tTrained summarizer and latent semantic analysis for summarization of text
In (Yeh et-al. 2005):
Text summarization Corpus based approach Latent semantic analysis Text relationship map. 
The second uses latent semantic analysis(  LSA)  to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. 
However,  most IR techniques that have been exploited in text summarization focus on symbolic level analysis,  and they do not take into account semantics such as synonymy,  polysemy,  and term dependency(  Hovy & Lin,  1997). 
Thus,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Hence,  we conclude that LSA can be employed to promote text summarization from keyword level analysis to semantic level analysis. 
Latent semantic analysis(  LSA)  is a mathematical technique for extracting and inferring relations of expected contextual usage of words in passages of discourse(  Deer wester et al,  1990; Landau er et al,  1998). 
The second one exploits latent semantic analysis(  LSA) (  Deer wester,  Duma is,  Furn as,  Landau er,  & Harsh man,  1990; Landau er,  Foltz,  & La ham,  1998)  and a text relationship map(  T.R.M.) (  Salton et al,  1997)  to derive semantically salient structures from a document. 
one used relevance measure to rank sentence relevance,  and the other used latent semantic analysis to identify semantically important sentences. 
This paper proposes two approaches to address text summarization. 
4.2-tInformation extraction using sentence based abstraction technique
In (Chan 2006):
In this article,  a sentence based abstraction technique for information extraction is presented. 
Section V explains a sentence based abstraction technique,  which is a strategy that captures the dynamic influences of context and preference using an associative memory model. 
With the explosion in the quantity of online text and multimedia information in recent years,  there has been a renewed interest in the automated extraction of knowledge and information in various disciplines. 
At the syntactic stage,  information extraction involves tokenization,  partsofspeech tagging,  syntactic parsing,  phrasal pattern matching,  and the transformation of each sentence into an intermediate structure using various templates. 
Simulation experiments suggest that this technique is useful because it moves away from a purely keyword based method of textual information extraction and its associated limitations. 
The usual approach to text classification is to reduce a text to a bag of words,  which throws away a lot of the linguistic information that is represented,  but the salient sentences that are extracted by this abstraction technique provide an alternative to text classification and indexing. 
However,  at the discourse stage,  domain knowledge is used to integrate all of the intermediate templates that are generated to produce a large scale knowledge structure that serves to provide coherent information for extraction. 
The early approaches that came out of the Message Understanding Conferences(  MUC)  have demonstrated their capabilities at this stage of information extraction. 
The application of text based abstraction techniques in various disciplines has recently become more widespread. 
4.3-tText understanding and summarization through document concept lattice
In (Ye et-al. 2007):
This task of document understanding(  Dang,  2005)  is a core issue in text summarization. 
We argue that the quality of a summary can be evaluated based on how many concepts in the original document(  s)  that can be preserved after summarization. 
Text summarization Document concept lattice Concept Semantic. 
Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow on tests. 
Following our work in the Document Understanding Conference(  DUC)  2005 and 2006(  Ye et al,  2005,  Ye et al,  2006) ,  our summarization algorithm uses the DCL to select a globally optimum sentence set that represents as many concepts as possible with the fewest words. 
In this paper,  we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. 
• Motivated by our evaluation metric on answer loss,  we propose a novel document model,  the document concept lattice,  which indexes sentences with respect to their coverage of overlapping concepts. 
Once words that represent unified concepts in the documents are linked,  we represent the sources as a document concept lattice(  DCL). 
While an RST approach to summarization is well motivated,  it is difficult to build such trees for single documents without explicit textual cues,  and even more problematic to build a tree for a set of multiple source documents. 
4.4-tSentence extraction through contextual information and statistical based summarization of text
In (Giannakopoulos et-al. 2008):
The AutoSummENG method is based on the concept of using statistically ex tr acted tract ed textual information from summaries integrated into a rich representa tional equivalent of a text to measure similarity between generated summaries and a set of model summaries. 
the type of statistical information extracted,  the representation chosen for the extracted information,  the method of similarity calculation. 
The AutoSummENG method for summarization evaluation is a promising method based on language neutral analysis of texts and comparison to gold standard summaries. 
The method presented herein matches and even exceeds the correlation of the aforementioned methodologies on the newer,  DUC 20065 data in a language neutral,  statistical manner,  while taking into account contextual information. 
The over information common ground of recent information retrieval efforts has created a serious motive for the design and implementation of summarization systems,  which are either based on existing information retrieval practices or provide a new pointofview on the retrieval process. 
These categories of ngrams are based on statistical criteria and are used to describe how noise can deteriorate the performance of our method as a function of the methodology parameters. 
A number of different intermediate representations of summaries information have been introduced in existing summarization evaluation literature,  ranging from automatically extracted snippets to human decided sub sentential portions of text. 
evaluation frameworks uses statistical measures of similarity based on ngrams of words,  4 although it supports different kinds of analysis,  ranging from ngram to semantic Hovy et al   b. 
4.5-tSummarization of emails through conversational cohesion and subjective opinions
In (Carenini et-al. 2008):
As a third contribution of this paper,  we study how to make use of the subjective opinions expressed in emails to support the summarization task. 
Third,  we propose a summarization approach based on subjective opinions and integrate it with the graph based ones. 
We study how to summarize email conversations based on the conversational cohesion and the subjective opinions. 
We integrate our best cohesion measure together with the subjective opinions. 
Other than the conversation structure,  the measures of cohesion and the graph based summarization methods we have proposed,  the importance of a sentence in emails can be captured from other aspects. 
Moreover,  we study how to include subjective opinions to help identify important sentences for summarization. 
In Section 5,  we study summarization approaches with subjective opinions. 
Their experiments showed that features about emails and the email thread could significantly improve the accuracy of summarization. 
Subjective opinions are often critical in many conversations. 
Having built the sentence quotation graph with different measures of cohesion,  in this section,  we develop two summarization approaches. 
Summarizing email conversations is challenging due to the characteristics of emails,  especially the conversational nature. 
In (Zajic et-al. 2008):
We explored the email thread summarization problem using messages from the Enron corpus,  which consists of approximately half a million emails from the folders of 151 Enron employees. 
We present two approaches to email thread summarization. 
that CMS represents a better approach to email thread summarization,  and that current sentence compression techniques do not improve summarization performance in this genre. 
one treats the problem as a sequence of single document summarization tasks(  a technique we call individual message summarization,  or IMS) ,  and the other treats the problem as a variant of multi document summarization(  a technique we call collective message summarization,  or CMS). 
In (Carenini et-al. 2007):
Since we are studying multiple email summarization in a conversational context,  we required that each conversation contained at least 4 emails. 
Email summarization is a promising way to solve this problem. 
In this paper,  we discuss a different form of support - email summarization. 
Not only does this study provide a gold standard to evaluate CWS and other summarization methods,  but it also sheds light on the importance of clue words and hidden emails to human summarizers. 
4.6-tSummarization of text through complex network approach
In (Antiqueira et-al. 2009):
In this paper,  we investigate a graph based,  language independent approach to extractive text summarization inspired by recent developments in the area of complex networks. 
The use of complex networks to represent texts appears therefore as suitable for automatic summarization,  consistent with the belief that the metrics of such networks may capture important text features. 
In this paper,  we employ concepts and metrics of complex networks to select sentences for an extractive summary. 
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
The proposed method,  called CNSumm(  Complex Networks based Summarization) ,  consists of four steps. 
We have described the use of complex networks concepts for extractive summarization,  as well as its evaluation through standard informativeness scores and significance tests. 
Automatic summarization Complex networks Network measurements Sentence extraction Summary informativeness. 
Thus,  the potential of our approach could be assessed by maintaining the focus on the summarization algorithms rather than on the construction of the networks. 
Automatic text summarization is a well established subfield of natural language processing,  which is relevant for a number of scenarios. 
The underlying assumption is that each measurement selected reflects some features of the source text that might be interesting for summarization. 
Complex networks concepts were considered potentially useful for the summarization task because they offer different,  often complementary,  views of a network,  and thus can be used to highlight a subset of its nodes. 
For the purpose of summarization,  communities supposedly represent the topic structure of the source text. 
4.7-tAutomatic creation of generic document summaries through non-negative matrix factorization
In (Ferreira et-al. 2014):
The 2002 conference was the last one that proposed the contest to create generic multi document summaries. 
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
In general,  multi document summarization is either generic(  also termed extractive) (  Alguliev,  Aliguliyev,  & Hajirahimova,  a)  or query based(  Luo,  Zhuang,  He,  & Shi,  2013). 
As already mentioned,  multi document summarization can be classified into generic and query based summarization. 
for each collection of documents,  summaries with 200 words(  first task)  and 400 words(  second task)  were generated. 
It combines single document summaries using sentence clustering techniques to generate multi document summaries. 
Text summarization,  the process of automatically creating a shorter version of one or more text documents,  is an important way of finding relevant information in large text libraries or in the Internet. 
One solution to this problem is offered by using text summarization techniques. 
Since 2004,  ROUGE has become of widespread use for the automatic evaluation of summaries(  Das and Martins,  2007,  Wei et al,  2010). 
the creation of summaries with 200 and 400 words. 
Alguliev and his collaborators(  Alguliev,  Aliguliyev,  & Mehdiyev,  2013)  propose a generic document summarization method which is based on sentence clustering. 
4.8-tAutomatic text summarization using MR, GA, FFNN, GMM and PNN based models
In (Fattah and Ren 2009):
This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. 
Training mode where features are extracted from 100 manually summarized Arabic documents and 50 manually summarized English documents(  50 not 100 to investigate the training data set size effect on summarization performance)  and used to train GA,  MR,  FFNN,  PNN and GMM models. 
With the huge amount of information available electronically,  there is an increasing demand for automatic text summarization systems. 
Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user(  Ye et al,  2007,  Steinberger et al,  2007,  Dorr and Gaasterland,  2007,  Diaz and Gerv s,  2007). 
In this paper,  we have investigated the use of genetic algorithm(  GA) ,  mathematical regression(  MR) ,  feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  for automatic text summarization task. 
Moreover,  we use all feature parameters to train feed forward neural network(  FFNN) ,  probabilistic neural network(  PNN)  and Gaussian mixture model(  GMM)  in order to construct a text summarizer for each model. 
Section 2 presents the different text feature parameters,  Section 3 is about the proposed automatic summarization model,  Section 4 shows the experimental results and finally Section 5 presents conclusions and future work. 
Text summarization addresses both the problem of selecting the most important portions of text and the problem of generating coherent summaries. 
The different attempts in this field have shown that human quality text summarization was very complex since it encompasses discourse understanding,  abstraction,  and language generation(  Spar ck,  1993). 
Some were about evaluation of summarization using relevance prediction(  Hob son Hobs on et al,  2007) ,  ROUGEeval package(  Sj bergh,  2007) ,  SUMMAC,  NTCIR,  and DUC(  Over et al,  2007)  and voted regression model(  Hirao et al,  2007). 
4.9-tQuery-based summarization of multiple documents by applying regression models
In (Kulesza and Taskar 2012):
In contrast to tools like search,  which require the user to specify a query based on prior knowledge,  a set of threads provides an immediate,  concise,  high level summary of the collection,  not just identifying a set of important ob jects but also conveying the relationships between them. 
• The DUC 2003/2004 text summarization task,  where we form extractive summaries of news articles by choosing diverse subsets of sentences(  Section 4.2.1)  ;. 
• An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;• A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;• A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
We demonstrate structured DPPs on a toy geographical paths problem,  a still image multiple pose estimation task,  and two high dimensional text threading tasks. 
In (Otterbacher et-al. 2009):
In this paper,  we focus on the query based or focused summarization problem where we seek to generate a summary of a set of related documents given a speci c aspect of their oft heir common topic formulated as a natural language query. 
Our method is a query based extension of the LexRanksummarization method introduced in(  Erk an and Radev,  2004). 
A key challenge for passage retrieval for QA is that,  when attempting to retrieve answers to questions from a set of documents published by multiple sources over time(  e g. 
4.10-tMaximum coverage and minimum redundancy in summarization of text
In (Alguliev et-al. 2011):
Text summarization Maximum coverage Less redundancy Integer linear programming Particle swarm optimizationBranchandbound. 
In Takamura and Okumura(  2009) ,  Takamura and Okamura represented text summarization as maximum coverage problem with knapsack constraint. 
Filatova and Hatzivassiloglou(  2004)  modeled extractive document summarization as a maximum coverage problem that aims at covering as many conceptual units as possible by selecting some sentences. 
In Table 2,  Table 3 MCMR(  Maximum Coverage and Minimum Redundant)  denotes the proposed method(  in brackets by the B&B and PSO respectively denoted the branchandbound and particle swarm optimization algorithms which have been used for solving the optimization problem). 
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
In these tables through MCMR(  Maximum Coverage and Minimum Redundant)  denoted our model with the objective function f. 
In particular,  we model text summarization as an integer linear programming problem. 
Tao et al  (  2008)  have designed word based and sentence based association networks(  WAN and SAN for short,  respectively)  and proposed word and sentence weighting approaches based on how much co occurrence information they contain,  and applied to text summarization. 
To tackle this pressing text information overload problem,  document clustering(  Aliguliyev,  2009,  Aliguliyev,  2009,  Wang et al,  2008)  and text summarization(  Aliguliyev,  2006,  Aliguliyev,  2010,  Alguliev and Alyguliev,  2008,  Alguliev and Aliguliyev,  2009,  Alguliev et al,  2005)  together have been used as a solution. 
For this reason,  document clustering and text summarization can be used for important components of information retrieval systems(  Yoo,  Hu,  & Song,  2007). 
► We model unsupervised generic text summarization as an optimization problem. 
That is why document clustering enables us to group similar text information and then text summarization provides condensed text information for the similar text by extracting the most important text content from a similar document set or a document cluster. 
The proposed generic text summarization model is presented in Section 3. 
4.11-tSummarization of documents through a progressive technique for selection of sentences
4.12-tEvaluation of sentence scoring methods for extractive summarization of text
In (Ferreira et-al. 2013):
Extractive summarization Sentence scoring methods Summarization evaluation. 
In terms of extractive summarization,  sentence scoring is the technique most used for extractive text summarization. 
Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. 
In (Antiqueira et-al. 2009):
Automatic summarization of texts is now crucial for several information retrieval tasks owing to the huge amount of information available in digital media,  which has increased the demand for simple,  language independent extractive summarization strategies. 
We have described the use of complex networks concepts for extractive summarization,  as well as its evaluation through standard informativeness scores and significance tests. 
Although extractive summarization can produce texts that have cohesion and coherence problems,  many systems have been proven to yield summaries whose informative level is satisfactory. 
In (Riedhammer et-al. 2010):
We analyze and compare two di erent methods for unsupervised extractive spontaneous speech summarization in the meeting domain. 
Evaluation measuresforsummarization performancelikeROUGE(  Lin,  2004)  or Pyramid(  Nenkova and Passonneau,  2004)  and later developments like Basic Elements(  Hovy et al,  2006)  score summaries based on an overlap of ngrams(  ROUGE) ,  summary content units(  manually annotated parts in the target text Pyramid)  or dependency parsing relations(  Basic Elements)  Formally,  let ci denote the presence of concept i in the summary and sj denote the presence of sentence j in the summary. 
The selected meeting extracts can then either be juxtaposed to form a short text summarizing a meeting or used as a starting point to enhance browsing experience Extractive summarization algorithms often rely on the measurement of two important aspects. 
In (Sipos et-al. 2012):
In this paper,  we present a supervised learning approach to training sub modular scoring functions for extractive multi document summarization. 
The learning method applies to all sub modular summarization methods,  and we demonstrate its effectiveness for both pairwise as well as coverage based scoring functions on multiple data sets. 
Sentence extraction can also be implemented using other graph based scoring approaches(  Mihalcea,  2004)  such as HITS(  Klein berg,  1999)  and positional power functions. 
In (Ganesan et-al. 2010):
Going strictly by the definition of true abstraction(  Radev et al,  2002) ,  our problem formulation is still more extractive than abs tractive because the generated summary can only contain words that occur in the text to be summarized our problem definition may be regarded as a word level(  finer granularity)  extractive summarization. 
In this paper,  we described a novel summarization framework(  Opinosis)  that uses textual graphs to generate abs tractive summaries of highly redundant opinions. 
Graphs have been commonly used for extractive summarization(  e g,  LexRank(  Erk an and Radev,  2004)  and TextRank(  Mihalcea and Tarau,  2004) ) ,  but in these works the graph is often undirected with sentences as nodes and similarity as edges. 
In (Azmi AM, Al-Thanyyan S 2012):
A comprehensive evaluation in Uz da et al  (  2008)  has concluded that automatic text summarization methods which are based on RST are better than extractive summarizers and those with hybrid methods produce worse summaries. 
In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the final summary. 
Automatic text summarization is an essential tool in this era of information overloading. 
In (Ferreira et-al. 2014):
One solution to this problem is offered by using text summarization techniques. 
The following sections detail the sentence scoring methods and the sentence clustering algorithm used here. 
Text summarization,  the process of automatically creating a shorter version of one or more text documents,  is an important way of finding relevant information in large text libraries or in the Internet. 
In (Gupta and Lehal 2010):
Text Summarization methods can be classified into extractive and abs tractive summarization. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
In (Otterbacher et-al. 2009):
In the current paper,  we have also demonstrated the e ectiveness of our method as applied to two classical IR problems,  extractive text summarization and passage retrieval for question answering. 
The ranked sentences were added to a summary one by one until the summary exceeded 250 words which was the limit in theDUC evaluations. 
While evaluations of question answering systems are often based ona shorter list of ranked sentences,  we chose to generate longer lists for several reasons. 
In (Patel et-al. 2007):
This paper describes an efficient algorithm for language independent generic extractive summarization for single document. 
Various methods of scoring the relevance of sentences or passages and combining the scores are described in. 
Through evaluations performed on a single document summarization for English,  Hindi,  Gujarati and Urdu documents,  we show that the method performs equally well regardless of the language. 
In (Barrera and Verma 2012):
Our system outperforms MEAD and TextRank sentence extraction in all experiments of evaluation and is consistently higher than the baseline. 
For Data set A,  the top 15 selected sentences for each article version were used as models for the evaluations. 
These results have implications not only for extractive and abs tractive single document summarization,  but could also be leveraged in multi document summarization. 
In (Alguliev et-al. 2013):
There are several most widely used extractive summarization methods as follows. 
Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some textual units of the documents and extracting those with highest scores. 
The centroid based method,  MEAD,  is one of the popular extractive summarization methods(  Radev,  Jing,  Stys,  & Tam,  2004). 
In (Alguliev et-al. 2011):
In paper,  we propose an unsupervised text summarization model which generates a summary by extracting salient sentences in given document(  s). 
In particular,  we model text summarization as an integer linear programming problem. 
A transductive approach(  Amini & Usunier,  2009)  for extractive multi document summarization identifies topic themes within a document collection,  which help to identify two sets of relevant and irrelevant sentences to a question. 
In (Rush et-al. 2015):
Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. 
Summarization based on text extraction is inherently limited,  but generation style abs tractive methods have proven challenging to build. 
We run experiments both using the DUC2004 evaluation data set(  500 sentences,  4 references,  75 bytes)  with all systems and a randomly held out Gigaword test set(  2000 sentences,  1 reference). 
In (0035):
In this paper we address the automatic summarization task Recent research works on extractive summary generation employ some heuristics,  but few works indicate how to select the relevant features We will present a summarization procedure based on the application of trainable Machine Learning algorithms which employs a set of features extracted directly from the original text. 
Therefore maximum similarity corresponds to cos θ = 1,  whereas cos θ = 0 indicates total discrepancy between the text elements The evaluation of the quality of a generated summary is a key point in summarization research. 
We have chosen this research direction because it allows us to measure the results of a text summarization algorithm in an objective way,  similar to the standard evaluation of classification algorithms found in the ML literature. 
In (Nobata et-al. 2001):
Each evidence is integrated using parameters,  which are estimated using training data Suitable parameter sets can be selected at each section information and compression ratio In the following sections,  we explain methods used in our system,  then show and discuss the evaluation results on the TSC,  Text Summarization Challenge,  which was held by National Information Institute. 
We have developed a sentence extraction system,  which estimates the signi cance of sentences by integrating four scoring fours coring functions that use evidence such as sentence location,  sentence length,  TF/IDF values of words,  and similarity to the title. 
Table 3.1 shows the evaluation results of our system and baseline systems in the task A,  sentence extraction task Each row shows the section that articles appeared on,  and each column corresponds to the result at each compression ratio. 
In (Aliguliyev 2009):
The proposed approach is a continue sentence clustering based extractive summarization methods,  proposed in Alguliev Alguliev,  R. 
4.13-tExploring correlations among multiple terms through a graph-based summarizer, GRAPHSUM
In (Baralis et-al. 2013):
However,  since the high order correlations among multiple terms are disregarded during graph evaluation,  the summarization performance could be limited unless integrating ad hoc language dependent or semantics based analysis. 
Hence,  there is a need for novel graph based summarizers that also consider the correlations among multiple terms and the differences between positive and negative term correlations. 
In fact,  on the one hand,  since they do not consider the underlying correlations among multiple terms,  some relevant facets of the analyzed data could be disregarded. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
• using association rules in graph based summarization to represent correlations among multiple terms,. 
,  the summarizer presented in this paper relies on a general purpose,  graph based approach that discovers and exploits high order correlations among multiple document terms. 
This paper presents GraphSum,  a general purpose graph based summarizer that combines the association rules that were extracted from the analyzed documents in a graph based model in order to consider the correlations among multiple terms during the summarization process. 
In (Kulesza and Taskar 2012):
We compare the expressive power of DPPs and MRFs,  characterizing the tradeo s in terms of modeling power and computational e ciency. 
In the rst,  we subtract a multiple of one of the vectors in V from all of the other vectors so that they are zero in the ithcomponent,  leaving us with a set of vectors spanning the subspace of V orthogonal to ei. 
• An image search task,  where we model human judgments of diversity for image sets returned by Google Image Search(  Section 5.3)  ;• A multiple pose estimation task,  where we improve the detection of human poses in images from television shows by incorporating a bias toward non overlapping predictions(  Section 6.4)  ;• A news threading task,  where we automatically extract timelines of important news stories from a large corpus by balancing intratimeline coherence with intertimelinediversity(  Section 6.6.4). 
4.14-tIncorporating various levels of language analysis for tackling redundancy in text summarization
4.15-tEvolutionary optimization algorithm for summarizing multiple documents
In (Alguliev et-al. 2013):
DE is a simple yet efficient evolutionary algorithm,  which has been widely applied to solve continuous optimization problems(  Price,  Storn,  & Lampinen,  2005). 
In addition to single document summarization,  which has been first studied in this field for years,  researchers have started to work on multi document summarization whose goal is to generate a summary from multiple documents. 
To solve the optimization problem has been created an improved differential evolution algorithm. 
Therefore,  in our study the optimization problem(  8) , (  9) , (  10)  was solved using a DE algorithm. 
► We create a self adaptive differential evolution algorithm to solve the optimization problem. 
In this paper,  a self adaptive differential evolution(  DE)  algorithm is created to solve the optimization problem. 
Therefore,  this strategy is very convenient to handle the constraints for evolutionary algorithm by punishing the infeasible solution during the selection procedure to ensure the feasible ones are favored. 
In this section,  a self adaptive dynamic control mechanism for choosing the suitable value of CR during the evolutionary progress is presented. 
(  1)  content coverage,  summary should contain salient sentences that cover the main content of the documents(  2)  diversity,  summaries should not contain multiple sentences that convey(  carry)  the same information and(  3)  length,  summary should be bounded in length. 
4.16-tSummarization of multiple documents using a hybrid machine learning model
4.17-tImproving clustering at sentence-level with the help of ranking-based technique for theme-based summarization
In (Yang et-al. 2014):
In this paper,  we propose a ranking based clustering framework that utilizes ranking distribution of documents and terms to help generate high quality sentence clusters. 
Sentence clustering plays a pivotal role in theme based summarization,  which discovers topic themes defined as the clusters of highly related sentences in order to avoid redundancy and cover more diverse information. 
Ranking based clustering Sentence clustering Theme based summarization. 
Thus,  good sentence clusters are the guarantee of good summaries in theme based summarization. 
Recall that our goal is to obtain more accurate sentence clusters and generate good summaries in theme based summarization. 
Ranking based clustering framework shows the best performance,  it further presents ranking distribution of documents and terms can help generate more accurate sentence clusters. 
Based on it,  a ranking based sentence clustering framework is developed. 
In the framework,  conditional ranks of documents and terms help to get generative probability of each sentence,  so sentences can be mapped into a very low dimensional space defined by current clustering result. 
To help alleviate this problem,  we argue in this paper that a term can be deemed as an independent text object instead of a feature of a sentence. 
Table 1 shows current works in sentence similarity and also the technique used by each work. 
In (Harabagiu S, Lacatusu F 2005):
We first discuss a total of six different sentence extraction methods(  EM1– EM6)  which correspond to the four baseline topic representation techniques(  TR1–TR4) ,  introduced in Section 2,  plus the two topic theme based representations based on TR5 introduced in Section 3.16 For ease of exposition,  we will refer to the graph based theme representation as TH1 and the linked list based theme representation as TH2. 
Second,  we believe that our results represent a comprehensive and replicable study,  which demonstrates the effectiveness of a structured,  theme based approach to multi document summarization. 
We believe that these results prove the benefits of using the theme based topic representation for multi document summarization. 
4.18-tStatistical and linguistic based summarization system for multiple documents
In (Gupta and Lehal 2010):
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
The importance of sentences is decided based on statistical and linguistic features of sentences. 
Text summarization based on fuzzy logic system architecture. 
This approach is less expensive and more robust than a summarization technique based entirely on a single method. 
w is linear statistical model of system(  the weights w,  w w in the equation)  m is total number of sentences in the training corpus. 
Automatic text summarization based on fuzzy logic. 
It is an extraction based multi document summarization system. 
Then,  it enters all the rules needed for summarization,  in the knowledge base of system. 
In (Harabagiu S, Lacatusu F 2005):
In our work,  we have considered both, (  1)  component based evaluations,  which evaluated each phase in the creation of a multi document summary separately,  and(  2)  intrinsic evaluations,  which evaluate the quality of each individual multi document summary generated by a summarization system. 
Now more than ever,  consumers need access to robust multi document summarization(  MDS)  systems,  which can effectively condense information found in several documents into a short,  readable synopsis,  or summary. 
In this article,  we perform what we believe to be the first comprehensive evaluation of the impact that different topic representation techniques have on the performance of a multi document summarization system. 
In (Ferreira et-al. 2014):
The multi document summarization system proposed in this paper is based on statistical methods and linguistic treatment to increase information diversity of summaries also dealing with redundancy. 
The main contribution of the proposed algorithm is the creation of an unsupervised generic summarization system,  that makes linguistic treatment of the input text by performing co reference resolution and discourse analysis,  besides using statistical and semantic similarities,  as in all previous work in the literature. 
Such a purpose is achieved through a new sentence clustering algorithm based on a graph model that makes use of statistic similarities and linguistic treatment. 
4.19-tMulti-document summarization based information retrieval using event graphs
4.20-textractive summarization of single documents through genetic operators and guided local search
4.21-t Topic-aspect based summarization through selection of groups
In (Fang et-al. 2015):
Considering the summaries from multi documents of one topic can describe various aspects of one given topic,  this paper attempts to exploit appropriate priors to generate topic aspect oriented summarization(  abbreviated as TAOS). 
As a result,  controls the selection of groups of,  as well as the groups of features,  which represents the preferred aspects in our summarization task. 
Therefore,  in this paper,  we attempt to devise an approach to the generation of the topic aspect oriented summarization according to topic factors,  we call this TAOS(  Topic AspectOriented Summarization). 
With the construction of feature groups,  a latent variable is utilized to control the selection of them. 
Many extraction based summarization methods have been proposed in the past years. 
Experiment results on DUC2003 and DUC2004 data sets for text summarization and NUSWide data set for image summarization demonstrate that the introduction of group selection can indeed improve summary performance. 
For image summarization,  we choose an information theoretic measure which based on Jensen–Shannon Divergence. 
For text summarization task,  among the unsupervised methods,  the Document Summarization based on Data Reconstruction(  DSDR). 
A natural way to generate topic aspect oriented summarization is to assume that topic aspects can prefer some feature groups,  and the group sparsity can be introduced. 
abstract based and extraction based. 
Analogously,  the summary about one topic sky may prefer color based features while one summary about topic car is more likely to mention edge based features. 
Abstract based approaches can be considered as a synthesis of the original documents while the extraction based approaches focus on how to utilize the extracted elements(  always sentences or images)  to generate a concise description of original documents. 
4.22-tSummarization of multiple documents based on social Folksonomy by analyzing semantically
In (Heu et-al. 2015):
To overcome these drawbacks,  we propose a novel multi document summarization system called FoDoSu,  or Folksonomybased MultiDocument Summarization,  that employs the tag clusters used by Flickr,  a Folksonomy system,  for detecting key sentences from multiple documents. 
Many approaches to multi document summarization have used probability based methods and machine learning techniques to simultaneously summarize multiple documents sharing a common topic. 
In this paper,  we propose FoDoSu which is a novel multi document summarization system using tag clusters from a Folksonomy system to detect the key sentences in multiple documents. 
In this paper,  we propose a novel multi document summarization system,  called FoDoSu(  Folksonomybased MultiDocument Summarization) ,  that employs the tag clusters used in Folksonomy systems to detect key sentences in multiple documents. 
To overcome these drawbacks,  we propose a novel multi document summarization system FoDoSu that employs tag clusters of the Folksonomy system to detect the key sentences in multiple documents. 
More recently,  multi document summarization techniques have been researched to summarize multiple documents sharing a common topic at the same time. 
Finally,  we create a summary of multiple documents by analyzing the importance of each word and its semantic relatedness to others. 
For this purpose,  we exploit the tag clusters used by Flickr,  one of the most representative Folksonomy systems,  for summarizing multiple documents. 
Experimental results from the TAC 2008 and 2009 data sets demonstrate the improvement of our proposed framework over existing summarization systems. 
4.23-tOther text summarization approaches
4.23.1-tLearning-based approach for summarizing related sentences
4.23.2-tSemantic role labeling with minimal resources
In (Khan et-al. 2015):
In this framework,  contents of the source documents are represented by predicate argument structures by employing semantic role labeling. 
Our work,  in contrast,  aims to treat this limitation by using semantic role labeling(  SRL)  technique to build semantic representation from the document text automatically. 
First,  it employs semantic role labeling for semantic representation of text. 
To the best of our knowledge,  semantic role labeling(  SRL)  technique,  which exploits semantic role parser,  has not been employed for the semantic representation oftextin multi document abs tractive summarization. 
First,  it employs semantic role labeling to extract predicate argument structure(  semantic representation)  from the contents of input documents. 
introduced a work that combined semantic role labeling with general statistic method(  GSM)  to determine important sentences for single document extractive summary. 
4.23.3-tSummarizing single documents through nested tree structure
4.23.4-tTwo-level sparse representation model for summarization of multiple documents
In (Liu et-al. 2015):
Similar to(  Yang et al   2013) ,  the two level sparse representation model we introduced is NPhard,  we use simulated annealing algorithm to get the summarization To our knowledge,  this is the rst paper that utilizes cover. 
We design a two level sparse representation model to extract multi document summarization The original document set is sparsely represented by summary sentences. 
Based on the data reconstruction and sentence de noising assumption,  we present a twolevelsparse representation model to depict the process ofmultidocument summarization. 
In fact,  our model incorporate a two level sparse representation model. 
two level sparse representation model age,  sparsity and diversity together in multi document summarization. 
4.23.5-tSparse-coding based reader-aware summarization system for multiple documents
In (CR92):
We propose a new MDS paradigm called reader aware multi document summarization(  RAMDS)  Speci cally,  a set of reader comments associated with the news reports are also collected. 
Our framework can still be used for MDS task without reader comments by ignoring those components for comments Besides Random and Lead methods,  we compare our system with two other unsupervised sparse coding based methods,  namely DSDR He et al,  2012. 
(  3)  In our sparse coding model,  we weight the reconstruction error by a prior knowledge,  i e,  paragraph position,  which can improve the summarization performancesigni cantly. 
4.23.6-tSummarization of multiple documents through recursive neural networks based ranking approach
In (Cao et-al. 2015a):
We develop a Ranking framework upon Recursive Neural Networks(  R2N2)  to rank sentences for multi document summarization. 
This paper presents a Ranking framework upon Recursive Neural Networks(  R2N2)  for the sentence ranking task of multi document summarization. 
To this end,  we develop a ranking framework upon recursive neural networks(  R2N2)  to rank sentences for multi document summarization. 
In this parsing tree,  recursive neural networks measure the importance of all these non terminal nodes. 
In addition,  recursive neural networks are used to automatically learn ranking features over the tree,  with handcrafted feature vectors of words as inputs. 
This process is modeled by recursive neural networks(  RNN). 
4.23.7-tGraph-based extractive summarization by considering importance, non-redundancy and coherence
In (Parveen and Strube 2015):
We propose a graph based method for extractivesingledocument summarization which considers importance,  non redundancy and local coherence simultaneously. 
Therefore,  in our work,  computing importance,  non redundancy and coherence is tightly integrated and taken care of simultaneously Our graph based summarization technique has the further advantage of being completely unsupervised We apply our graph based summarization technique to scienti c articles from the journal PLOS Medicine,  a high impact open access journal from the medical domain. 
While previous work uses a measure of local coherence only to evaluate the local coherence of summaries,  we integrate it closely with determining importance and avoiding redundancy in an unsupervised graph based method for extractivesingledocument summarization. 
4.23.8-tSparse optimization based compressive document summarization
4.23.9-t Submodular mixtures based summarization of multi-document hierarchy of topics
In (Bairi et-al. 2015):
Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. 
Given a collection of articles spanning different topics,  but with similar titles,  automatically generate a disambiguation page for those titles using the Wikipedia category hierarchy as a topic DAG. 
Let G(  V,  E)  be the DAG structured topic hierarchy with V topics. 
Given a DAGstructured topic hierarchy and a subset of objects,  we investigate the problem of finding a subset of DAGstructured topics that are induced by that subset(  of objects). 
Our approach is based on sub modular maximization and mixture learning,  which has been successfully used in applications such as document summarization(  Lin,  2012)  and image summarization(  Tschiatschek et al,  2014) ,  but has never been applied to topic identification tasks or,  more generally,  DAG summarization. 
4.23.10-tDisaster summarization through prediction of salient updates
In (Kedzie et-al. 2015):
Our system predicts sentence salience in the context of a large scale event,  such as a disaster,  and integrates these predictions into a clustering based multi document summarization system. 
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection,  increasing the quality of the updates. 
While we evaluate these features in the domain of disasters,  this approach is generally applicable to many update summarization tasks. 
4.23.11-tSummarizing multiple documents through system combination
In (Goldstein et-al. 2000):
Another system(  Stein et al,  1999)  creates a multi document summary from multiple single document summaries,  an approach that can be suboptimal in some cases,  due to the fact that the process of generating the final multi document summary takes as input the individual summaries and not the complete documents. 
Conventional IR systems find and rank documents based on maximizing relevance to the user query(  Salton,  1970; van Rijsbergen,  1979; Buckley,  1985; Salton,  1989). 
Consider the situation where the user issues a search query,  for instance on a news topic,  and the retrieval system finds hundreds of closely ranked documents in response. 
4.23.12-tPhrase-based compressive cross-language summarization
4.23.13-t Re-evaluation of automatic summarization using BLEU and 192 variants of ROUGE
In (Graham 2015):
An analysis of evaluation of summarization metrics was provided with an evaluation of BLEU and 192 variants of ROUGE. 
In total,  therefore,  when employing ROUGE for the evaluation of summarization systems,  there are 192(  8 x 2 x 2 x 3 x 2)  possible system level variants to choose from. 
We subsequently replicate a recent large scale evaluation that relied on,  what we now know to be,  suboptimal ROUGE variants revealing distinct conclusions about the relative performance of stateoftheart summarization systems. 
Our evaluation reveals for the first time which metric variants significantly outperform others,  optimal metric variants distinct from current recommended best variants,  as well as machine translation metric BLEU to have performance on par with ROUGE for the purpose of evaluation of summarization systems. 
4.24-tPro and Cons
5-tComparison of recent automatic text summarization extractive approaches
6-tAbstractive approaches for text summarization
In (CR100):
In this section,  we explain previous work on recentabstractive summarization,  and we stress our novelty with respect to other similar approaches An approach for combining different fragments of information that have been extracted from one or more document sis documents is suggested in. 
In order to solve this limitation,  besides checking for the correctness of the sentences once they have been generated and filtering out those ones,  which do not satisfy the proposed constraints,  we would also need to apply some constraints based on the information sentences contain,  optimizing the set of generated sentences,  so that only the best ones with respect to their content are used With respect to the general results of the abstractiveapproaches,  since the length of the summaries is restricted to only 100 words,  when selecting the most important sentences before or after generating new sentences,  some of the oft he concepts may not be included. 
This paper focuses on abs tractive text summarization. 
Moreover,  in order to decide which of the new sentences should be included in the abs tractive summary,  an extractive text summarization approach is developed(  i e,  COMPENDIUM) ,  so that the most relevant abs tractive sentence scan sentences can be selected and extracted. 
On the Ont he one hand,  we try to elucidate the reasons why the Graph COMPENDIUM approach performs worse than theCOMPENDIUM Graph,  and on the other hand,  we want to analyze the reasons of the low overall performance of theabstractive approaches Regarding the first type of analysis carried out,  if we use the word graph based method for generating new sentences first,  and use all of them as input for COMPENDIUM,  thisTS tool can have difficulties in selecting important content This occurs because many of the sentences will start with the same words(  e g,  if we take the top 10 words with highest tfidf) ,  so once COMPENDIUM detects a specific fragment of information as relevant,  sentences containing the same portion of information that have not been detected as redundant will be also selected,  leading to summaries that have not much variation in content. 
In light of this,  Text Summarization(  TS)  is of great help since its main aim is to produce a condensed new text containing a significant portion of the information in the original text(  s)  The process of summarization can be divided into three stages. 
,  The aim of this paper is to conduct an analysis of the potentials and limitations of word graphs for generatingabstractive summaries. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
Knowledge poor approaches do not consider any knowledge pertaining to the domain to which text summarization is applied therefore,  knowledge poor approaches can be easily applied to any domain(  Abracos & Lopes,  1997; Gong & Liu,  2001; Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; May eng & Jang,  1999; Salton et al,  1997; Mitra,  Singh al,  & Buckley,  1997). 
In this paper,  we propose two text summarization approaches. 
outperforms keyword based text summarization approaches. 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
both with single documents and at the corpus level to investigate the competence of LSA in text summarization. 
Text search and text summarization are two essential technologies to address this problem. 
On the other hand,  text summarization can be roughly classified into two categories according to how much domain knowledge is involved(  Hahn & Mani,  2000). 
In (Genest PE, Lapalme G 2011):
Our framework differs from previous abs tractive summarization models in requiring a semantic analysis of the text. 
Most systems develop ped for the main international conference on text summarization,  the Text Analysis Conference(  TAC) (  Owczarzak and Dang,  2010) ,  predominantly use sentence extraction,  including all the top ranked systems,  which make only minor post editing of extracted sentences(  Conroy et al,  2010) (  Gil lick et al,  2009) (  Ge nest Gen est Gene st et al,  2008) (  Chen et al,  2008). 
This low linguistic score is understandable,  because this was our first try at text generation and abs tractive summarization,  whereas the other systems that year used sentence extraction,  with at most minor modifications made to the extracted sentences. 
The kind of texttotext generation involved in our work is related to approaches in paraphrasing(  Androutsopoulos and Malakasiotis,  2010). 
Our first attempt at full abs tractive summarization took place in the context of the TAC 2010 multi document news summarization task. 
Summarization approaches can generally be categorized as extractive or abs tractive(  Mani,  2001). 
This abstract representation relies on the concept of Information Items(  INIT) ,  which we define as the smallest element of coherent information in a text or a sentence. 
In (Glavaš G, Šnajder J 2014):
Nonetheless,  the majority of information retrieval and text summarization methods rely on shallow document representations that do not account for the semantics of events. 
Nevertheless,  studies on event based text summarization are rare(  Daniel,  Radev,  & Allison,  2003; Filatova & Hatzivassiloglou,  2004; Li,  Wu,  Lu,  Xu,  & Yuan,  2006). 
Furthermore,  the results suggest that similar event based approaches could be used to address closely related NLP tasks such as text simplification. 
Building on event graphs,  we propose novel models for event centered information retrieval and multi document text summarization. 
The proposed model is extractive(  summaries are built by extracting sentences from the original texts)  and non focused(  the summarization is not guided by a specific information need). 
Building on these insights,  we present a multi document summarization model that filters the text based on the relevance of event mentions. 
Despite the fact that(  multi)  document summarization approaches predominantly focus on summarizing news wire texts and that events are the primary concept of news(  as news describes real world events) ,  very few attempts have been made towards realizing event oriented text summarization. 
In (Baralis et-al. 2012):
In the context of multi document summarization,  the selection of the most relevant and not redundant sentences belonging to a collection of textual documents is definitely a challenging task. 
In last years,  the increasing availability of textual documents in the electronic form has prompted the need of efficient and effective data mining approaches suitable for textual data analysis. 
In the context of multi document summarization,  a summary is composed of the most representative sentences belonging to a document collection. 
Multi document summarization,  Text mining,  Frequent item set items et mining. 
Although its application to transactional data is well established,  to the best of our knowledge,  the usage of frequent item sets in textual document summarization has never been investigated so far. 
Experimental results,  reported in Section 3.2.3,  show that the proposed sentence selection algorithm is more effective and efficient than a branchandbound algorithm for text summarization purposes. 
To the best of our knowledge,  this is the first attempt to exploit frequent item sets in text summarization. 
Summarization is a challenging data mining task that focuses on constructing a succinct and informative description of a data collection. 
In (Zajic et-al. 2008):
Section 3 describes our general framework for text summarization and specific approaches we have developed for email thread summarization. 
The recent work of Carenini et al  (  2007)  examines extractive approaches to summarization on Enron data that leverage graphs defined by quoted texts. 
Finally,  evaluation issues in general present challenges for text summarization. 
We attempted to present our summarization systems with text as clean as possible. 
These two compression techniques represent different trade offs that we think are particularly salient for informal text. 
Furthermore,  this highly technical domain uses plenty of jargon that is not typically found in news wire text. 
Repetitions of text from earlier messages(  quoted text)  were also eliminated. 
For Trimmer,  proper compression depends on correct parse trees,  and parsers trained on news wire text(  like the one we use)  are likely to make many errors. 
We apply both methods to the problem of email thread summarization. 
We conducted a variety of experiments to explore the problem of email thread summarization. 
7-tMultilingual approaches for text summarization
In (Baralis et-al. 2013):
Graph based summarization entails extracting a worthwhile subset of sentences from a collection of textual documents by using a graph based model to represent the correlations between pairs of document terms. 
Multi document summarization Text mining Association rule mining Graph ranking. 
The multi document summarization task entails generating a summary of a collection of textual documents. 
As future work,  we plan to(  i)  adapt and evaluate the proposed summarizer into a multilingual contest(  e g,  the TAC’11 contest. 
To effectively address the summarization problem in a multilingual context in. 
in the context of document summarization. 
Nowadays Internet provides access to a huge number of electronic textual documents. 
To ease the knowledge discovery process,  a significant research effort has been devoted to studying and developing automated summarization tools,  which produce a succinct overview of the most relevant document content,  i e,  the summary. 
GraphSum performs better than many stateoftheart approaches,  including those that heavily rely on advanced semantics based models(  e g,  ontologies)  or complex linguistic processing steps. 
The raw textual content is commonly unsuitable for use in item set items et and association rule mining. 
)  to further improve the summarization performance. 
It discovers and exploits association rules to represent the correlations among multiple terms that have been neglected by previous approaches. 
with language dependent text processing. 
and in the multilingual TAC’11 contest. 
to tackle the summarization problem by combining complex network analysis. 
To validate the statistical significance of the GraphSum performance improvement against its competitors,  we used the paired ttest. 
Hence,  the proposed summarizer is potentially applicable to documents coming from rather different application contexts. 
) ,  the graph nodes could also represent a subset of terms with size higher than one(  e g,  Analysis,  Context). 
The contribution of the positive and negative term correlations will be differentiated during the summarization process,  as described in the following sections. 
In (Fung P, Ngai G 2006):
This article presents a multi document,  multilingual,  theme based summarization system based on modeling text cohesion(  story ow). 
We have presented a stochastic Hidden Markov Story Model for multilingual and multi document summarization. 
Conventional extractive summarization methods picks out salient sentences according to either their positions in the text and or closeness to the centroid sentence to be included in a summary. 
Other researchers have used Text Tiling in extraction based summarization as well. 
The per for mance of our summarizer is superior to conventional methods that do not incorporate text cohesion information. 
We argue that such inherent text cohesion exists and is(  1)  speci c to a particular story and(  2)  speci c to a particular language. 
Conventional extractive summarization systems which pick out salient sentences to include in a summary often disregard any ow or sequence that might exist between these sentences. 
We also propose a Na ve Bayes classi er for document summarization. 
We propose a system of segmenting multiple multilingual documents into different topics or stories and then summarizing these stories. 
can be attributed to the increasing availability of multilingual resources. 
In (Gupta V 2013):
This paper concentrates on hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. 
In addition to this,  it also suggests some new features for summarizing Hindi and Punjabi multilingual text. 
This paper concentrates on hybrid algorithm for multilingual extractive summarization of Hindi and Punjabi text. 
From results of weight learning in Table 1,  we can conclude that two most important features of hybrid algorithm for multilingual summarization of Hindi and Punjabi text are font feature and position feature. 
It is first time that this multilingual text summarizer has been proposed which supports both Hindi and Punjabi text. 
Nine features used in this algorithm for summarizing multilingual Hindi and Punjabi text are. 
It is first time that this hybrid algorithm for multilingual text summarization has been proposed which supports both Hindi and Punjabi text. 
Hybrid Algorithm for multilingual HindiPunjabi Text Summarization. 
In addition to this,  it also suggests some new features for summarizing Hindi and Punjabi multilingual text. 
Worthiness of lengthy documents can quickly and easily be judged using text summarization. 
In (Yeh et-al. 2005):
This paper proposes two approaches to address text summarization. 
Table 1 shows the outputs of our approaches in different phases of the text summarization process. 
According to the level of text processing,  Mani and May bury(  1999)  categorized text summarization approaches into surface,  entity,  and discourse levels. 
Knowledge poor approaches do not consider any knowledge pertaining to the domain to which text summarization is applied therefore,  knowledge poor approaches can be easily applied to any domain(  Abracos & Lopes,  1997; Gong & Liu,  2001; Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; May eng & Jang,  1999; Salton et al,  1997; Mitra,  Singh al,  & Buckley,  1997). 
In this paper,  we propose two text summarization approaches. 
outperforms keyword based text summarization approaches. 
Nowadays,  corpus based approaches play an important role in text summarization(  Hovy & Lin,  1997; Kupiec et al,  1995; Lin,  1999; Lin & Hovy,  1997; Teufel & Mo ens,  1997; Yeh et al,  2002). 
both with single documents and at the corpus level to investigate the competence of LSA in text summarization. 
Text search and text summarization are two essential technologies to address this problem. 
In (Gupta and Lehal 2010):
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
Text summarization. 
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
Extractive text summarization process. 
is a very important aspect for text summarization. 
Interest in automatic text summarization,  arose as early as the fifties. 
Automatic text summarization system. 
If one approaches the task of text abstracting from such a probabilistic modeling perspective,  it might well be possible that HMMs could be employed for this purpose,  as well. 
Text summarization using regression for estimating feature weights. 
8-tSummary evaluation
8.1-tInformativeness evaluation
In (Banerjee et-al. 2015):
In manual evaluation,  our approach also achieves promising results on informativeness and readability. 
Further,  manual evaluation by human judges shows that our technique produces summaries with acceptable linguistic quality and high informativeness. 
Evaluators were asked to rate the summaries based on informativeness(  the amount of information conveyed)  and linguistic quality(  readability of the summary). 
To select the best paths from the clusters,  we combine informativeness I(  p Cj i)  and linguistic quality LQ(  p Cj i)  in an optimization framework. 
,  we ask 10 evaluators to rate 10 sets of four summaries on two different factors – informativeness and linguistic quality. 
Other systems tackle redundancy as a final step however,  we integrate linguistic quality and informativeness to select the best sentences in the summary using our ILP based approach. 
Hence,  we introduce two factors – Informativeness(  I(  p Cj i) )  and Linguistic quality(  LQ(  p Cj i) ). 
By contrast,  our method selects sentences by jointly maximizing informativeness and readability and generates informative,  well formed and readable summaries. 
In principle,  we can use any existing method that computes the importance of a sentence to define Informativeness. 
Informativeness. 
In our model,  we use TextRank scores Mihalcea and Tarau,  2004. 
The informativeness score of a path(  I(  p Cj i) )  is obtained by adding the importance scores of the individual words in the path. 
In order to compute Linguistic quality,  we use a language model. 
Table 2 shows the results obtained by manual evaluation. 
In this work,  we aim at developing an abs tractive summarizer. 
Table 1 shows the following ROUGE scores for our evaluation. 
for automatic evaluation of summaries(  compared against human written model summaries)  as it has been proven effective in measuring qualities of summaries and correlates well to human judgments. 
We use ROUGE(  RecallOriented Understudy of Gi sting Evaluation)  Lin,  2004. 
First,  our proposed approach identifies the most important document in the multi document set. 
Second,  we generate Kshortest paths from the sentences in each cluster using a word graph structure. 
Experimental results on the DUC 2004 and 2005 multi document summarization data sets show that our proposed approach outperforms all the baselines and stateoftheart extractive summarizers as measured by the ROUGE scores. 
Our ILP model represents the shortest paths as binary variables and considers the length of the path,  information score and linguistic quality score in the objective function. 
Finally,  we select sentences from the set of shortest paths generated from all the clusters employing a novel integer linear programming(  ILP)  model with the objective of maximizing information content and readability of the final summary. 
Our method also outperforms a recent abs tractive summarization technique. 
However,  to the best of our knowledge,  none of the above methods explicitly model the role of linguistic quality and only aim at maximizing information content of the summaries. 
In this work,  we address readability by assigning a log probability score from a language model as an indicator of linguistic quality. 
More specifically,  we build a novel optimization model for summarization that jointly maximizes information content and readability. 
(  4)  As can be seen from Equation(  4) ,  we obtain the conditional probability of different sets of grams in the sentence. 
,  m,  XK i p Cj i ≤ 1(  6)  We introduce Equation(  7)  so that we can prevent similar information(  cosine similarity ≥ 0.5)  from being selected from different clusters. 
,  wq) , (  3)  where LL(  w,  w,  ...,  wq)  is defined as. 
LQ(  p Cj i)  = 1 1 − LL(  w,  w,. 
1,  K ⊂ Cj,  Cj 0 p Cj i + p Cj 0 i 0 ≤ 1 if sim(  p Cj i,  p Cj 0 i 0)  ≥ 0.5. 
The score LQ(  p Cj i)  assigned to each path is defined as follows. 
Suppose that a path contains a sequence of q words w,  w,  ...,  wq. 
,  wq)  = 1 L · log Yq t P(  wt wt wt). 
LL(  w,  w,. 
The LL(  w,  w,. 
In Figure 2,  this constraint ensures that only one of the several possible paths mentioned in the example is included in the final summary as they contain redundant information. 
j ∈ {1,. 
First,  we ensure that a maximum of one path is selected from each cluster using Equation(  6). 
j,  j ∈. 
We introduce several constraints to solve the problem. 
In addition,  T(  p Cj i) ,  the number of tokens in a path,  is also taken into consideration and the term 1 T(  p Cj i)  assigns more weight to shorter paths so that the system can favor shorter informative sentences. 
Each p Cj i represents a binary variable,  that can take 0 or 1,  depending on whether the path is selected in the final summary or not. 
The scores are combined and averaged by L,  the number of conditional probabilities computed. 
,  p Cm K)  = Xm j XK i 1 T(  p Cj i)  I(  p Cj i)  ·LQ(  p Cj i)  p Cj i(  5). 
F(  p C 1,. 
We maximize the following objective function. 
1,  m,  i,  i ∈. 
ILP Formulation. 
In our experiments,  we used a 3- gram model that is trained on the English Gigaword corpus. 
Therefore,  in Equation(  3) ,  we take the reciprocal of the logarithmic value with smoothing to compute LQ(  p Cj i). 
8.2-tQuality evaluation9-tEvaluation results
8.3-tAsiya, an evaluation toolkit
In (He et-al. 2012):
We use the ROUGE(  RecallOriented Understudy for Gi sting Evaluation)  toolkit(  Lin 2004)  which has been widely adopted by DUC for automatic summarization evaluation. 
We choose two automatic evaluation methods ROUGEN and ROUGEL in our experiment. 
ROUGE toolkit reports separate scores for 1,  2,  3 and gram,  and also for the longest common subsequence. 
Due to limited space,  more information can be referred to the toolkit package. 
Table 1 and Table 2 show the ROUGE evaluation results on DUC 2006 and DUC 2007 data sets respectively. 
In this study,  we use the standard summarization benchmark data sets DUC 2006 and DUC 2007 for the evaluation. 
Document summarization is of great value to many real world applications,  such as snippets generation for search results and news headlines generation. 
For the sake of efficient optimization,  following(  Yu et al   2008; Cai and He 2012) ,  we formulate the objective function of nonnegative DSDR as follows. 
where β =. 
β1,. 
,  n T is an auxiliary variable to control the candidate sentences selection. 
8.4-tText summarization evaluation programs
In (Neto et-al. 2000):
This paper describes a text mining tool that performs two tasks,  namely document clustering and text summarization. 
In order to make our experiments more relevant,  we have used the evaluation thee valuation framework of a large scale project for evaluating text summarization algorithms. 
Our text summarization algorithm is based on computing the value of aTFISF(  term frequency – inverse sentence frequency)  measure for each word,  which is an adaptation of the conventional TFIDF(  term frequency – inverse document frequency)  measure of information retrieval. 
theTIPSTER Text Summarization Evaluation Conference(  SUMMAC)  – hereafter referred to as the SUMMAC project for short Mani et al   98. 
“ Text summarization is still an emerging field,  and serious questions remain concerning the appropriate methods and types of evaluation. 
The source text has 75 sentences Clearly,  none of the two systems produced perfect summaries,  which is normal,  considering the high degree of difficulty associated with the text summarization task. 
In this section we describe the results of an experiment carr ied out to evaluate our text summarization algorithm. 
Probably the most important future research is to extend the functionality of the tool so that it can perform other text mining tasks,  mainly text categorization and the discovery of association rules between keywords of documents In addition,  we are currently developing other techniques for text summarization,  based on the use of genetic algorithms. 
However,  the textual,  unstructured nature of documents makes these two text mining tasks considerably more difficult than their data mining counterparts. 
Section 6 introduces the method developed for text summarization,  and section 7 presents the results of experiments evaluating the performance of the system in this task. 
document clustering and text summarization. 
This helps the user to quickly grasp the topics of the documents belonging to each cluster We have also developed a text summarization algorithm that capitalizes on the vectorialdocument representation used in our system. 
Another categorization can be made between objective or subjective measures of summary quality To evaluate our text summarization algorithm,  we have compared the summaries produced by our system against the summaries produced by algorithms performing the adhoc task of theSUMMAC project. 
Sentences with high values of TFISF are selected to producea summary of the source text. 
Here the topic is provided as an input to the summarization system,  and the evaluation seeks to determine whether the ... 
The concepts contained in a text are usually rather abstract and can hardly be modeled by using conventional knowledge representation structures Furthermore,  the occurrence of synonyms(  different words with the same meaning)  and homonyms(  words with the same spelling but with distinct meanings)  makes it difficult to detect valid relationships between different parts of the text In this paper we describe a system that performs two important text mining tasks. 
In (Mani and Maybury 1999):
This is a welcome volume for both researchers and teachers who are interested in extending the traditional boundaries of Information Retrieval to include related information access and analytic applications such as summarization,  extraction,  and question answering Text summarization,  which is de ned by the editors as the process of distilling the most important information from a source(  s)  to produce an abridged version for a particular user(  s)  and task(  s)  ” is a technology that has been worked on for more than forty years anda capability that users have long desired. 
Today s recognized information overload has exacerbated this need while advances in Natural Language Processing and statistical text processing appear ready to provide viable near term solutions The editors have compiled an excellent selection of papers on summarization,  with an even split between classical and contemporary papers thirteen of each,  with many of the contemporary contributions based on papers presented at the ACL/EACL Workshop on Intelligent Scalable Summarization,  held in Madrid,  Spain in The choice classical papers include the 1958 paper by Luhn on The AutomaticCreation of Literature Abstracts,  Edmund son s 1969 paper on New Methods in Automatic Extracting,  and the 1975 paper by Pollock and Zamora on Automatic AbstractingResearch at Chemical Abstracts readings which I always include in my course on Indexing and Abstracting. 
Classical Approaches CorpusBased Approaches Exploiting DiscourseStructure; KnowledgeRich Approaches Evaluation Methods,  and New SummarizationProblem Areas.Following the general introduction is an excellent piece by Karen Spar ck Jones that provides a framework for understanding the task of summarization and calls for a doable research strategy that should serve to provide useful technology in the near future using what is already known in NLP. 
The introductions are substantive pieces on their own and together with the piece by Spar ck Jones could well serve as a good summarization of the entire book Particularly welcome is the section on Evaluation Methods a topic that requires broader attention if the eld is to provide reliable advances. 
In (Gupta and Lehal 2010):
Text Summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
An abs tractive summarization method consists of understanding the original text and retelling it in fewer words. 
In this paper,  a Survey of Text Summarization Extractive techniques has been presented. 
The goal of automatic text summarization is condensing the source text into a shorter version preserving its information content and overall meaning. 
Text summarization using regression for estimating feature weights. 
Text Summarization methods can be classified into extractive and abs tractive summarization. 
This paper focuses on extractive text summarization methods. 
Interest in automatic text summarization,  arose as early as the fifties. 
Text summarization. 
Automatic text summarization system. 
,  in a text is one of good feature for text summarization. 
An approach to concept obtained text summarization. 
Automatic text summarization based on fuzzy logic. 
is a very important aspect for text summarization. 
is the multilingual summarization and evaluation method. 
These features are important as,  a number of methods of text summarization are using them. 
Text summarization based on fuzzy logic system architecture. 
Extractive text summarization process. 
is a good model to estimate the text feature weights. 
It is to be noted that this property applies only to data that has principal dimensions inherently however,  LSA would probably work since most of the text data has such principal dimensions owing to the variety of topics it addresses. 
9-tEvaluation results
10-Future directions in text summarization
REFERENCE
False Abuobieda et-al. 2012 Abuobieda A, Salim N, Albaham AT, Osman AH, Kumar YJ (2012) Text summarization features selection method using pseudo genetic-based model. In: International conference on information retrieval knowledge management, pp 193–197
0005 Aliguliyev 2009 Aliguliyev RM (2009) A new sentence similarity measure and sentence based extractive technique for automatic text summarization. Expert Syst Appl 36(4):7764–7772
0068 Alguliev et-al. 2013 Alguliev RM, Aliguliyev RM, Isazade NR (2013) Multiple documents summarization based on evolutionary optimization algorithm. Expert Syst Appl 40:1675–1689. doi:
0058 Alguliev et-al. 2011 Alguliev RM, Aliguliyev RM, Hajirahimova MS, Mehdiyev CA (2011) MCMR: maximum coverage and minimum redundant text summarization model. Expert Syst Appl 38:14514–14522. doi:
0038 Almeida and Martins 2013 Almeida M, Martins AF (2013) Fast and robust compressive summarization with dual decomposition and multi-task learning. In: ACL (1), pp 196–206
0074 Amigo et-al. 2005 Amigó E, Gonzalo J, Penas A, Verdejo F (2005) QARLA: a framework for the evaluation of text summarization systems. In: ACL ’05: proceedings of the 43rd annual meeting on association for computational linguistics, pp 280–289
0000 Antiqueira et-al. 2009 Antiqueira L, Oliveira ON, Costa F, Volpe G (2009) A complex network approach to text summarization. Inf Sci 179:584–599. doi:
0007 Azmi AM, Al-Thanyyan S 2012 Azmi AM, Al-Thanyyan S (2012) A text summarizer for Arabic. Comput Speech Lang 26:260–273. doi:
0088 Bairi et-al. 2015 Bairi RB, Iyer R, Ramakrishnan G, Bilmes J (2015) Summarization of multi-document topic hierarchies using submodular. In: Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing, pp 553–563
0062 Banerjee et-al. 2015 Banerjee S Mitra P, Sugiyama K (2015) Multi-document abstractive summarization using ILP based multi-sentence compression. In: Proceedings of the 24th international joint conference on artificial intelligence (IJCAI 2015), pp 1208–1214
0065 Baralis et-al. 2012 Baralis E, Cagliero L, Jabeen S, Fiori A (2012) Multi-document summarization exploiting frequent itemsets. In: Symposium on applied computing (SAC’12), pp 782–786
0044 Baralis et-al. 2013 Baralis E, Cagliero L, Mahoto N, Fiori A (2013) GRAPHSUM : discovering correlations among multiple terms for graph-based summarization. Inf Sci 249:96–109. doi:
0024 Barrera and Verma 2012 Barrera A, Verma R (2012) Combining syntax and semantics for automatic extractive single-document summarization. In: 13th international conference on computational linguistics and intelligent text processing. Springer, pp 366–377
0061 Barzilay and Lapata 2005 Barzilay R, Lapata M (2005) Modeling local coherance: an entity-based approach. In: Proceedings of the 43rd annual meeting of the association for computational linguistics (ACL ’05), pp 141–148
0008 Bing et-al. 2015 Bing L, Li P, Liao Y, Lam W, Guo W, Passonneau RJ (2015) Abstractive multi-document summarization via phrase selection and. arXiv preprint 
0052 Boudin F, Morin E 2013 Boudin F, Morin E (2013) Keyphrase extraction for N-best reranking in multi-sentence compression. In: North American Chapter of the Association for Computational Linguistics (NAACL)
0099 Brin and Page 1998 Brin S, Page L (1998) The anatomy of a large scale hypertextual web search engine. In: Proceedings of the 7th international conference on world wide web 7, pp 107–117
0076 Cao et-al. 2015a Cao Z, Wei F, Dong L, Li S, Zhou M (2015a) Ranking with recursive neural networks and its application to multi-document summarization. In: Twenty-ninth AAAI conference on artificial intelligence
0054 Cao et-al. 2015c Cao Z, Wei F, Li S, Li W, Zhou M, Wang H (2015c) Learning summary prior representation for extractive summarization. In: Proceedings of ACL: short papers, pp 829–833
0101 Carbonell and Goldstein 1998 Carbonell JG, Goldstein J (1998) The use of MMR, diversity-based re-ranking for re-ordering documents and producing summaries. In: Proceedings of the 21st annual international ACM SIGIR conference on research and development in information retrieval, pp 335–336
0090 Carenini et-al. 2007 Carenini G, Ng RT, Zhou X (2007) Summarizing email conversations with clue words. In: Proceedings of the 16th international conference on World Wide Web. ACM. pp 91–100
0091 Carenini et-al. 2008 Carenini G, Ng RT, Zhou X (2008) Summarizing emails with conversational cohesion and subjectivity. ACL 8:353–361
0020 Carlson et-al. 2003 Carlson L, Marcu D, Okurowski ME (2003) Building a discourse-tagged corpus in the framework of rhetorical structure theory. Springer, Netherlands, pp 85–112
0018 Chan 2006 Chan SWK (2006) Beyond keyword and cue-phrase matching: a sentence-based abstraction technique for information extraction. Decis Support Syst 42:759–777. doi:
0075 Dunlavy et-al. 2007 Dunlavy DM, O’Leary DP, Conroy JM, Schlesinger JD (2007) A system for querying, clustering and summarizing documents. Inf Process Manag 43:1588–1605
0102 Fang et-al. 2015 Fang H, Lu W, Wu F et al (2015) Topic aspect-oriented summarization via group selection. Neurocomputing 149:1613–1619. doi:
0042 Fattah and Ren 2009 Fattah MA, Ren F (2009) GA, MR, FFNN, PNN and GMM based models for automatic text summarization. Comput Speech Lang 23:126–144. doi:
0014 Ferreira et-al. 2013 Ferreira R, De Souza L, Dueire R et al (2013) Assessing sentence scoring techniques for extractive text summarization. Expert Syst Appl 40:5755–5764. doi:
0003 Ferreira et-al. 2014 Ferreira R, de Souza Cabral L, Freitas F et al (2014) A multi-document summarization system based on statistics and linguistic treatment. Expert Syst Appl 41:5780–5787. doi:
0021 Frank et-al. 2012 Frank JR, Kleiman-Weiner M, Roberts DA, Niu F, Zhang C, Re C, Soboroff I (2012) Building an entity-centric stream filtering test collection for TREC 2012. MASSACHUSETTS INST OF TECH CAMBRIDGE
0070 Fung P, Ngai G 2006 Fung P, Ngai G (2006) One story, one flow: hidden Markov Story Models for multilingual multidocument summarization. ACM Trans Speech Lang 3:1–16. doi:
0072 Ganesan et-al. 2010 Ganesan K, Zhai C, Han J (2010) Opinosis : a graph-based approach to abstractive summarization of highly redundant opinions. In: Proceedings of the 23rd international conference on computational linguistics, pp 340–348
0040 Genest PE, Lapalme G 2011 Genest PE, Lapalme G (2011) Framework for abstractive summarization using text-to-text generation. In: Proceedings of the workshop on monolingual text-to-text generation, Association for Computational Linguistics, pp 64–73
0089 Giannakopoulos et-al. 2008 Giannakopoulos G, Karkaletsis V, Vouros G, Stamatopoulos P (2008) Summarization system evaluation revisited: N-gram graphs. ACM Trans Speech Lang Process 5:1–39
0037 Glavaš G, Šnajder J 2014 Glavaš G, Šnajder J (2014) Event graphs for information retrieval and multi-document summarization. Expert Syst Appl 41:6904–6916. doi:
0064 Goldstein et-al. 2000 Goldstein J, Mittal V, Carbonelll J, Kantrowitz M (2000) Multi-document summarization by sentence extraction. In: NAACL-ANLP 2000 workshop on automatic summarization. pp 40–48
0077 Graham 2015 Graham Y (2015) Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 128–137
0022 Grosz et-al. 1995 Grosz BJ, Weinstein S, Joshi AK (1995) Centering: a framework for modeling the local coherence of discourse. Comput Linguist 21:203–225
0045 Gupta V 2013 Gupta V (2013) Hybrid algorithm for multilingual summarization of Hindi and Punjabi documents. In: Mining intelligence and knowledge exploration. Springer International Publishing, pp 717–727
0006 Gupta and Lehal 2010 Gupta V, Lehal GS (2010) A survey of text summarization extractive techniques. J Emerg Technol Web Intell 2:258–268. doi:
0092 Gupta et-al. 2011 Gupta P, Pendluri VS, Vats I (2011) Summarizing text by ranking texts units according to shallow linguistic features. In: 13th international conference on advanced communication technology. pp 1620–1625
0105 Hadi et-al. 2006 Hadi Y, Essannouni F, Thami ROH (2006) Unsupervised clustering by k-medoids for video summarization. In: ISCCSP’06 (the second international symposium on communications, control and signal processing)
0108 Harabagiu S, Lacatusu F 2005 Harabagiu S, Lacatusu F (2005) Topic themes for multi-document summarization. In: SIGIR’ 05: proceedings of the 28th annual international ACM SIGIR conference on research and development in information retrieval. pp 202–209
0031 He et-al. 2012 He Z, Chen C, Bu J, Wang C, Zhang L, Cai D, He X (2012) Document summarization based on data reconstruction. In: AAAI
0098 Hearst 1997 Hearst M (1997) TextTiling: segmenting text into multi-paragraph subtopic passages. Comput Linguist 23:33–64
0039 Heu et-al. 2015 Heu JU, Qasim I, Lee DH (2015) FoDoSu: multi-document summarization exploiting semantic analysis based on social Folksonomy. Inf Process Manag 51(1):212–225
0087 Hirao et-al. 2013 Hirao T, Yoshida Y, Nishino M, Yasuda N, Nagata M (2013) Single-document summarization as a tree knapsack problem. EMNLP 13:1515–1520
0048 Hong and Nenkova 2014 Hong K, Nenkova A (2014) Improving the estimation of word importance for news multi-document summarization. In: Proceedings of EACL
0095 Hong et-al. 2015 Hong K, Marcus M, Nenkova A (2015) System combination for multi-document summarization. In: Proceedings of the 2015 conference on empirical methods in natural language processing. pp 107–117
0015 Hovy et-al. 2006 Hovy E, Lin CY, Zhou L, Fukumoto J (2006) Automated summarization evaluation with basic elements. In: Proceedings of the 5th international conference on language resources and evaluation (LREC), pp 81–94
0060 Huang et-al. 2010 Huang L, He Y, Wei F, Li W (2010) Modeling document summarization as multi-objective optimization. In: Proceedings of the third international symposium on intelligent information technology and security informatics, pp 382–386
0069 Kabadjov et-al. 2010 Kabadjov M, Atkinson M, Steinberger J et al. (2010) NewsGist: a multilingual statistical news summarizer. Lecture notes in computer science (including including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics) 6323 LNAI, pp 591–594. doi:
0084 Kaljahi et-al. 2014 Kaljahi R, Foster J, Roturier J (2014) Semantic role labelling with minimal resources: experiments with french. In: Lexical and computational semantics (*SEM 2014), p 87
0049 Kallimani et-al. 2011 Kallimani JS, Srinivasa KG, Eswara Reddy B (2011) Information extraction by an abstractive text summarization for an Indian regional language. In: Natural language processing and knowledge engineering (NLP-KE), 2011 7th international conference on IEEE, pp 319–322
0073 Kedzie et-al. 2015 Kedzie C, McKeown K, Diaz F (2015) Predicting salient updates for disaster summarization. In: Proceedings of the 53rd annual meeting of the ACL and the 7th international conference on natural language processing. pp 1608–1617
0001 Khan et-al. 2015 Khan A, Salim N, Jaya Kumar Y (2015) A framework for multi-document abstractive summarization based on semantic role labelling. Appl Soft Comput 30:737–747. doi:
0016 Kim and Hovy 2005 Kim SM, Hovy E (2005) Automatic detection of opinion bearing words and sentences. In: Companion volume to the proceedings of the international joint conference on natural language processing (IJCNLP), pp 61–66
0056 Ko and Seo 2004 Ko Y, Seo J (2004) Learning with unlabeled data for text categorization using a bootstrapping and a feature projection technique. In: Proceedings of the 42nd annual meeting of the association for computational linguistics (ACL 2004). pp 255–262
0103 Ko et-al. 2003 Ko Y, Kim K, Seo J (2003) Topic keyword identification for text summarization using lexical clustering. IEICE Trans Inf Syst E86-D:1695–1701
0027 Kulesza and Taskar 2012 Kulesza A, Taskar B (2012) Determinantal point processes for machine learning. arXiv preprint 
0047 Kulkarni and Prasad 2010 Kulkarni UV, Prasad RS (2010) Implementation and evaluation of evolutionary connectionist approaches to automated text summarization. J Comput Sci 6:1366–1376
0055 Lee and Seung 1999 Lee DD, Seung HS (1999) Learning the parts of objects by non-negative matrix factorization. Nature 401(6755):788–791
0082 Leite and Rino 2006 Leite DS, Rino LHM (2006) Selecting a feature set to summarize texts in Brazilian Portuguese. Advances in artificial intelligence-IBERAMIA-SBIA 2006:462–471
0036 Li et-al. 2007 Li JW, Ng KW, Liu Y, Ong KL (2007) Enhancing the effectiveness of clustering with spectra analysis. IEEE Trans Knowl Data Eng 19:887–902
0033 Li et-al. 2013 Li C, Liu F, Weng F, Liu Y (2013) Document summarization via guided sentence compression. In: EMNLP, pp 490–500
0106 Li et-al. 2015a Li C, Liu Y, Zhao L (2015a) Using external resources and joint learning for bigram weighting in ilp-based multi-document summarization. In: Proceedings of NAACL-HLT, pp 778–787
0078 CR92 Li P, Bing L, Lam W, Li H, Liao Y (2015b) Reader-aware multi-document summarization via sparse coding. arXiv preprint 
0080 Lin 2004 Lin CY (2004) ROUGE: a package for automatic evaluation of summaries. In: Proceedings of ACL text summarization workshop, pp 74–81
0066 Lin and Bilmes 2010 Lin H, Bilmes J (2010) Multi-document summarization via budgeted maximization of submodular functions. In: Human language technologies: the 2010 annual conference of the North American chapter of the association for computational linguistics, Association for Computational Linguistics, pp 912–920
0010 Liu et-al. 2009 Liu X, Webster JJ, Kit C (2009) An extractive text summarizer based on significant words. In: Proceedings of the 22nd international conference on computer processing of oriental languages, language technology for the knowledge-based economy, Springer, pp 168–178
0063 Liu et-al. 2015 Liu H, Yu H, Deng ZH (2015) Multi-document summarization based on two-level sparse representation model. In: Twenty-ninth AAAI conference on artificial intelligence
0011 CR100 Lloret E, Palomar M (2011a) Analyzing the use of word graphs for abstractive text summarization. In: IMMM 2011, first international conference, pp 61–66
0100 Luhn 1958 Luhn H (1958) The automatic creation of literature abstracts. IBM J Res Dev 2:159–165
0009 Mani and Maybury 1999 Mani I, Maybury M (1999) Advances in automatic text summarization. MIT Press, Cambridge
0097 Mihalcea and Tarau 2004 Mihalcea R, Tarau P (2004) TextRank: bringing order into texts. In: Conference on empirical methods in natural language processing. pp 404–411
0083 Moawad IF, Aref M 2012 Moawad IF, Aref M (2012) Semantic graph reduction approach for abstractive Text Summarization. In: Proceedings of ICCES 2012, 2012 International Conference on Computer Engineering and Systems, pp 132–138. doi:
0013 Murdock 2006 Murdock VG (2006) Aspects of sentence retrieval. University of Massachusetts, Amherst
0029 Neto et-al. 2000 Neto JL, Santos AD, Kaestner CAA, Freitas AA (2000) Document clustering and text summarization. In: Proceedings of the fourth international conference practical applications of knowledge discovery and data mining (padd-2000), pp 41–55
0085 Nobata et-al. 2001 Nobata C, Satoshi S, Murata M, Uchimoto K, Utimaya M, Isahara H (2001) Sentence extraction system asssembling multiple evidence. In: Proceedings 2nd NTCIR workshop, pp 319–324
0019 Otterbacher et-al. 2009 Otterbacher J, Erkan G, Radev DR (2009) Biased LexRank: passage retrieval using random walks with question-based priors. Inf Process Manag 45(1):42–54
0012 Ouyang et-al. 2011 Ouyang Y, Li W, Li S, Lu Q (2011) Applying regression models to query-focused multi-document summarization. Inf Process Manag 47:227–237
0025 Owczarzak K 2009 Owczarzak K (2009) DEPEVAL summ: dependency-based evaluation for automatic summaries. In: Proceedings of the joint conference of the 47th annual meeting of the ACL and the 4th international joint conference on natural language processing of the AFNLP. pp 190–198
0071 Pang and Lee 2008 Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135
0043 Pardo et-al. 2003b Pardo TAS, Rino LHM, Nunes MGV (2003b) Gistsumm: a summarization tool based on a new extractive method. In: Proceedings of the sixth workshop on computational processing of written and spoken portuguese (propor), 2721 of LNAI, pp 210–218
0051 Parveen and Strube 2015 Parveen D, Strube M (2015) Integrating importance, non-redundancy and coherence in graph-based extractive summarization. In: Proceedings of the 24th international conference on artificial intelligence. AAAI Press. pp 1298–1304
0002 Patel et-al. 2007 Patel A, Siddiqui T, Tiwary US (2007) A language independent approach to multilingual text summarization. In: Large scale semantic access to content (text, image, video, and sound), pp 123–132
0109 Radev et-al. 2001 Radev DR, Fan W, Zhang Z, Arbor A (2001) WebInEssence: a personalized web-based multi-document summarization and recommendation system. In: NAACL 2001 workshop on automatic summarization, pp 79–88
0059 Radev et-al. 2004a Radev D, Allison T, Goldensohn B et al. (2004a) MEAD: a platform for multidocument multilingual text summarization. Proc Lr, 1–4
0023 CR132 Radev DR, Jing HY, Stys M, Tam D (2004b) Centroid-based summarization of multiple documents. Inf Process Manag 40:919–938
0057 Riedhammer et-al. 2010 Riedhammer K, Favre B, Hakkani-Tur D (2010) Long story short- global unsupervised models for keyphrase based meeting summarization. Speech Commun 52:801–815
0093 Rino and Modolo 2004 Rino LHM, Modolo M (2004) Supor: an environment for as of texts in brazilianportuguese. In: Espana for natural language processsing (EsTAL). pp 419–430
0004 Rush et-al. 2015 Rush AM, Chopra S, Weston J (2015) A neural attention model for abstractive sentence summarization. arXiv preprint 
0026 Sanderson M, Croft WB 1999 Sanderson M, Croft WB (1999) Deriving concept hierarchies from text. Proceedings of SIGIR 1999:206–213
0094 Sarkar 2010 Sarkar K (2010) Syntactic trimming of extracted sentences for improving extractive multi-document summarization. J Comput 2:177–184
0050 Shen et-al. 2011 Shen C, Li T, Ding CH (2011) Integrating clustering and multi-document summarization by bi-mixture probabilistic latent semantic analysis PLSA with sentence bases. In: AAAI
0032 Shen et-al. 2007 Shen D, Sun J-T, Li H et al. (2007) Document summarization using conditional random fields. In: Proceedings of 20th international joint conference on artificial intelligence. pp 2862–2867
0081 Simon et-al. 2007 Simon I, Snavely N, Seitz SM (2007) Scene summarization for online image collections. In: Computer vision, 2007. ICCV 2007. IEEE 11th international conference on. IEEE. pp 1–8
0053 Sipos et-al. 2012 Sipos R, Shivaswamy P, Joachims T (2012) Large-margin learning of submodular summarization models. In: Proceedings of the 13th conference of the European chapter of the association for computational linguistics, Association for Computational Linguistics, pp 224–233
0041 Song et-al. 2011 Song W, Choi LC, Park SC, Ding XF (2011) Fuzzy evolutionary optimization modeling and its applications to unsupervised categorization and extractive summarization. Expert Syst Appl 38:9112–9121
0028 Storn R, Price K 1997 Storn R, Price K (1997) Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. J Glob Optim 11(4):341–359
0110 Wang and Li 2012 Wang D, Li T (2012) Weighted consensus multi-document summarization. Inf Process Manag 48:513–523
0046 Yang et-al. 2013 Yang C, Shen J, Peng J, Fan J (2013) Image collection summarization via dictionary learning for sparse representation. Pattern Recognit 46(3):948–961
0034 Yang et-al. 2014 Yang L, Cai X, Zhang Y, Shi P (2014) Enhancing sentence-level clustering with ranking-based clustering framework for theme-based summarization. Inf Sci 260:37–50. doi:
0030 Ye et-al. 2007 Ye S, Chua TS, Kan MY, Qiu L (2007) Document concept lattice for text understanding and summarization. Inf Process Manag 43:1643–1662. doi:
0096 Yeh et-al. 2005 Yeh J-Y, Ke H-R, Yang W-P, Meng I-H (2005) Text summarization using a trainable summarizer and latent semantic analysis. Inf Process Manag 41:75–95. doi:
0086 Zajic et-al. 2008 Zajic DM, Dorr BJ, Lin J (2008) Single-document and multi-document summarization techniques for e-mail threads using sentence compression. Inf Process Manag 44:1600–1610
0107 Zhao et-al. 2009 Zhao L, Wu L, Huang X (2009) Using query expansion in graph-based approach for query-focused multi-document summarization. Inf Process Manag 45(1):35–41
